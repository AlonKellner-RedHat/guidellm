---
weight: -10
---

# Getting Started

Welcome to GuideLLM! This section will guide you through the process of installing the tool, setting up your benchmarking environment, running your first benchmark, and analyzing the results to optimize your LLM deployment for real-world inference workloads.

GuideLLM makes it simple to evaluate and optimize your large language model deployments, helping you find the perfect balance between performance, resource utilization, and cost-effectiveness.

## Quick Start Guides

Follow the guides below in sequence to get the most out of GuideLLM and optimize your LLM deployments for production use.

<div class="grid cards" markdown>

- :material-package-variant:{ .lg .middle } Installation

    ---

    Learn how to install GuideLLM using pip, from source, or with specific version requirements.

    [:octicons-arrow-right-24: Installation Guide](install.md)

- :material-server:{ .lg .middle } Start a Server

    ---

    Set up an OpenAI-compatible server using vLLM or other supported backends to benchmark your LLM deployments.

    [:octicons-arrow-right-24: Server Setup Guide](server.md)

- :material-speedometer:{ .lg .middle } Run Benchmarks

    ---

    Learn how to configure and run performance benchmarks against your LLM server under various load conditions.

    [:octicons-arrow-right-24: Benchmarking Guide](benchmark.md)

- :material-chart-bar:{ .lg .middle } Analyze Results

    ---

    Interpret benchmark results to understand throughput, latency, reliability, and optimize your deployments.

    [:octicons-arrow-right-24: Analysis Guide](analyze.md)

</div>
