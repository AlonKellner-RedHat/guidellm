{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Scale Efficiently: Evaluate and Optimize Your LLM Deployments for Real-World Inference  <p>GuideLLM is a platform for evaluating and optimizing the deployment of large language models (LLMs). By simulating real-world inference workloads, GuideLLM enables users to assess the performance, resource requirements, and cost implications of deploying LLMs on various hardware configurations. This approach ensures efficient, scalable, and cost-effective LLM inference serving while maintaining high service quality.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Performance Evaluation: Analyze LLM inference under different load scenarios to ensure your system meets your service level objectives (SLOs).</li> <li>Resource Optimization: Determine the most suitable hardware configurations for running your models effectively.</li> <li>Cost Estimation: Understand the financial impact of different deployment strategies and make informed decisions to minimize costs.</li> <li>Scalability Testing: Simulate scaling to handle large numbers of concurrent users without performance degradation.</li> </ul>"},{"location":"#key-sections","title":"Key Sections","text":"<ul> <li> <p> Getting Started</p> <p>Install GuideLLM, set up your first benchmark, and analyze the results to optimize your LLM deployment.</p> <p> Getting started</p> </li> <li> <p> Guides</p> <p>Detailed guides covering backends, datasets, metrics, and service level objectives for effective LLM benchmarking.</p> <p> Guides</p> </li> <li> <p> Examples</p> <p>Step-by-step examples demonstrating real-world benchmarking scenarios and optimization techniques.</p> <p> Examples</p> </li> <li> <p> API Reference</p> <p>Complete reference documentation for the GuideLLM API to integrate benchmarking into your workflow.</p> <p> API Reference</p> </li> </ul>"},{"location":"developer/","title":"Developer","text":"<p>Welcome to the Developer section of GuideLLM! This area provides essential resources for developers who want to contribute to or extend GuideLLM. Whether you're interested in fixing bugs, adding new features, improving documentation, or understanding the project's governance, you'll find comprehensive guides to help you get started.</p> <p>GuideLLM is an open-source project that values community contributions. We maintain high standards for code quality, documentation, and community interactions to ensure that GuideLLM remains a robust, reliable, and user-friendly tool for evaluating and optimizing LLM deployments.</p>"},{"location":"developer/#developer-resources","title":"Developer Resources","text":"<ul> <li> <p> Code of Conduct</p> <p>Our community guidelines ensure that participation in the GuideLLM project is a positive, inclusive, and respectful experience for everyone.</p> <p> Code of Conduct</p> </li> <li> <p> Contributing Guide</p> <p>Learn how to effectively contribute to GuideLLM, including reporting bugs, suggesting features, improving documentation, and submitting code.</p> <p> Contributing Guide</p> </li> <li> <p> Development Guide</p> <p>Detailed instructions for setting up your development environment, implementing changes, and adhering to the project's coding standards and best practices.</p> <p> Development Guide</p> </li> </ul>"},{"location":"developer/code-of-conduct/","title":"GuideLLM Code of Conduct","text":""},{"location":"developer/code-of-conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"developer/code-of-conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others\u2019 private information, such as a physical or email address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting</li> </ul>"},{"location":"developer/code-of-conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"developer/code-of-conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"developer/code-of-conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement through GitHub, Slack, or Email. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"developer/code-of-conduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"developer/code-of-conduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"developer/code-of-conduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"developer/code-of-conduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"developer/code-of-conduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"developer/code-of-conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the [Contributor Covenant][homepage], version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla\u2019s code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"developer/contributing/","title":"Contributing to GuideLLM","text":"<p>Thank you for considering contributing to GuideLLM! We welcome contributions from the community to help improve and grow this project. This document outlines the process and guidelines for contributing.</p>"},{"location":"developer/contributing/#how-can-you-contribute","title":"How Can You Contribute?","text":"<p>There are many ways to contribute to GuideLLM:</p> <ul> <li>Reporting Bugs: If you encounter a bug, please let us know by creating an issue.</li> <li>Suggesting Features: Have an idea for a new feature? Open an issue to discuss it.</li> <li>Improving Documentation: Help us improve our documentation by submitting pull requests.</li> <li>Writing Code: Contribute code to fix bugs, add features, or improve performance.</li> <li>Reviewing Pull Requests: Provide feedback on open pull requests to help maintain code quality.</li> </ul>"},{"location":"developer/contributing/#getting-started","title":"Getting Started","text":""},{"location":"developer/contributing/#prerequisites","title":"Prerequisites","text":"<p>Before contributing, ensure you have the following installed:</p> <ul> <li>Python 3.9 or higher</li> <li>pip (Python package manager)</li> <li>Tox</li> <li>Git</li> </ul>"},{"location":"developer/contributing/#setting-up-the-repository","title":"Setting Up the Repository","text":"<p>You can either clone the repository directly or fork it if you plan to contribute changes back:</p>"},{"location":"developer/contributing/#option-1-cloning-the-repository","title":"Option 1: Cloning the Repository","text":"<ol> <li>Clone the repository to your local machine:</li> </ol> <pre><code>git clone https://github.com/neuralmagic/guidellm.git\ncd guidellm\n</code></pre>"},{"location":"developer/contributing/#option-2-forking-the-repository","title":"Option 2: Forking the Repository","text":"<ol> <li> <p>Fork the repository by clicking the \"Fork\" button on the repository's GitHub page.</p> </li> <li> <p>Clone your forked repository to your local machine:</p> </li> </ol> <pre><code>git clone https://github.com/&lt;your-username&gt;/guidellm.git\ncd guidellm\n</code></pre> <p>For detailed instructions on setting up your development environment, please refer to the DEVELOPING.md file. It includes step-by-step guidance on:</p> <ul> <li>Installing dependencies</li> <li>Running tests</li> <li>Using Tox for various tasks</li> </ul>"},{"location":"developer/contributing/#code-style-and-guidelines","title":"Code Style and Guidelines","text":"<p>We follow strict coding standards to ensure code quality and maintainability. Please adhere to the following guidelines:</p> <ul> <li>Code Style: Use Black for code formatting and Ruff for linting.</li> <li>Type Checking: Use Mypy for type checking.</li> <li>Testing: Write unit tests for new features and bug fixes. Use pytest for testing.</li> <li>Documentation: Update documentation for any changes to the codebase.</li> </ul> <p>To check code quality locally, use the following Tox environment:</p> <pre><code>tox -e quality\n</code></pre> <p>To automatically fix style issues, use:</p> <pre><code>tox -e style\n</code></pre> <p>To run type checks, use:</p> <pre><code>tox -e types\n</code></pre>"},{"location":"developer/contributing/#submitting-changes","title":"Submitting Changes","text":"<ol> <li>Create a Branch: Create a new branch for your changes:</li> </ol> <pre><code>git checkout -b feature/your-feature-name\n</code></pre> <ol> <li> <p>Make Changes: Commit your changes with clear and descriptive commit messages.</p> </li> <li> <p>Run Tests and Quality Checks: Before submitting your changes, ensure all tests pass and code quality checks are satisfied:</p> </li> </ol> <pre><code>tox\n</code></pre> <ol> <li>Push Changes: Push your branch to your forked repository (if you forked):</li> </ol> <pre><code>git push origin feature/your-feature-name\n</code></pre> <ol> <li>Open a Pull Request: Go to the original repository and open a pull request. Provide a clear description of your changes and link any related issues.</li> </ol>"},{"location":"developer/contributing/#reporting-issues","title":"Reporting Issues","text":"<p>If you encounter a bug or have a feature request, please open an issue on GitHub. Include as much detail as possible, such as:</p> <ul> <li>Steps to reproduce the issue</li> <li>Expected and actual behavior</li> <li>Environment details (OS, Python version, etc.)</li> </ul>"},{"location":"developer/contributing/#community-standards","title":"Community Standards","text":"<p>We are committed to fostering a welcoming and inclusive community. Please read and adhere to our Code of Conduct.</p>"},{"location":"developer/contributing/#license","title":"License","text":"<p>By contributing to Speculators, you agree that your contributions will be licensed under the Apache License 2.0.</p>"},{"location":"developer/developing/","title":"Developing for Speculators","text":"<p>Thank you for your interest in contributing to Speculators! This document provides detailed instructions for setting up your development environment, implementing changes, and adhering to the project's best practices. Your contributions help us grow and improve this project.</p>"},{"location":"developer/developing/#setting-up-your-development-environment","title":"Setting Up Your Development Environment","text":""},{"location":"developer/developing/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following installed:</p> <ul> <li>Python 3.9 or higher</li> <li>pip (Python package manager)</li> <li>Tox</li> <li>Git</li> </ul>"},{"location":"developer/developing/#cloning-the-repository","title":"Cloning the Repository","text":"<ol> <li>Clone the repository to your local machine:</li> </ol> <pre><code>git clone https://github.com/neuralmagic/guidellm.git\ncd guidellm\n</code></pre> <ol> <li>(Optional) If you plan to contribute changes back, fork the repository and clone your fork instead:</li> </ol> <pre><code>git clone https://github.com/&lt;your-username&gt;/guidellm.git\ncd guidellm\n</code></pre>"},{"location":"developer/developing/#installing-dependencies","title":"Installing Dependencies","text":"<p>To install the required dependencies for the package and development, run:</p> <pre><code>pip install -e ./[dev]\n</code></pre> <p>The <code>-e</code> flag installs the package in editable mode, allowing you to make changes to the code without reinstalling it. The <code>[dev]</code> part installs additional dependencies needed for development, such as testing and linting tools.</p>"},{"location":"developer/developing/#implementing-changes","title":"Implementing Changes","text":""},{"location":"developer/developing/#writing-code","title":"Writing Code","text":"<ol> <li>Create a Branch: Create a new branch for your changes:</li> </ol> <pre><code>git checkout -b feature/your-feature-name\n</code></pre> <ol> <li> <p>Make Changes: Implement your changes in the appropriate files. Ensure that all public functions and classes have clear and concise docstrings.</p> </li> <li> <p>Update Documentation: Update or add documentation to reflect your changes. This includes updating README files, docstrings, and any relevant guides.</p> </li> </ol>"},{"location":"developer/developing/#running-quality-style-and-type-checks","title":"Running Quality, Style, and Type Checks","text":"<p>We use Tox to simplify running various tasks in isolated environments. Tox standardizes environments to ensure consistency across local development, CI/CD pipelines, and releases. This guarantees that the code behaves the same regardless of where it is executed.</p> <p>Additionally, to ensure consistency and quality of the codebase, we use ruff for linting and styling, isort for sorting imports, mypy for type checking, and mdformat for formatting Markdown files.</p>"},{"location":"developer/developing/#code-quality-and-style","title":"Code Quality and Style","text":"<p>To check code quality, including linting and formatting:</p> <pre><code>tox -e quality\n</code></pre> <p>To automatically fix style issues:</p> <pre><code>tox -e style\n</code></pre>"},{"location":"developer/developing/#type-checking","title":"Type Checking","text":"<p>To ensure type safety using Mypy:</p> <pre><code>tox -e types\n</code></pre>"},{"location":"developer/developing/#link-checking","title":"Link Checking","text":"<p>To ensure valid links added to the documentation / Markdown files:</p> <pre><code>tox -e links\n</code></pre>"},{"location":"developer/developing/#automating-quality-checks-with-pre-commit-hooks-optional","title":"Automating Quality Checks with Pre-Commit Hooks (Optional)","text":"<p>We use pre-commit to automate quality checks before commits. Pre-commit hooks run checks like linting, formatting, and type checking, ensuring that only high-quality code is committed.</p> <p>To install the pre-commit hooks, run:</p> <pre><code>pre-commit install\n</code></pre> <p>This will set up the hooks to run automatically before each commit. To manually run the hooks on all files, use:</p> <pre><code>pre-commit run --all-files\n</code></pre>"},{"location":"developer/developing/#running-tests","title":"Running Tests","text":"<p>For testing, we use pytest as our testing framework. We have different test suites for unit tests, integration tests, and end-to-end tests. To run the tests, you can use Tox, which will automatically create isolated environments for each test suite. Tox will also ensure that the tests are run in a consistent environment, regardless of where they are executed.</p>"},{"location":"developer/developing/#running-all-tests","title":"Running All Tests","text":"<p>To run all tests:</p> <pre><code>tox\n</code></pre>"},{"location":"developer/developing/#running-specific-tests","title":"Running Specific Tests","text":"<ul> <li>Unit tests (focused on individual components with mocking):</li> </ul> <pre><code>tox -e test-unit\n</code></pre> <ul> <li>Integration tests (focused on interactions between components ideally without mocking):</li> </ul> <pre><code>tox -e test-integration\n</code></pre> <ul> <li>End-to-end tests (focused on the entire system and user interfaces):</li> </ul> <pre><code>tox -e test-e2e\n</code></pre>"},{"location":"developer/developing/#running-tests-with-coverage","title":"Running Tests with Coverage","text":"<p>To ensure your changes are covered by tests, run:</p> <pre><code>tox -e test-unit -- --cov=speculators --cov-report=html\n</code></pre> <p>Review the coverage report to confirm that your new code is adequately tested.</p>"},{"location":"developer/developing/#opening-a-pull-request","title":"Opening a Pull Request","text":"<ol> <li>Push Changes: Push your branch to your forked repository (if you forked):</li> </ol> <pre><code>git push origin feature/your-feature-name\n</code></pre> <ol> <li>Open a Pull Request: Go to the original repository and open a pull request. Use the following template for your pull request description:</li> </ol> <pre><code># Title; ex: Add feature X to improve Y\n\n## Summary:\n\nShort paragraph detailing the pull request changes and reasoning in addition to any relevant context.\n\n## Details:\n\n- Detailed list of changes made in the pull request\n\n## Test Plan:\n\n- Detailed list of steps to test the changes made in the pull request\n\n## Related Issues\n\n- List of related issues or other pull requests; ex: \"Fixes #1234\"\n</code></pre> <ol> <li>Address Feedback: Respond to any feedback from reviewers and make necessary changes.</li> </ol>"},{"location":"developer/developing/#additional-resources","title":"Additional Resources","text":"<ul> <li>CONTRIBUTING.md: Guidelines for contributing to the project.</li> <li>CODE_OF_CONDUCT.md: Our expectations for community behavior.</li> <li>tox.ini: Configuration for Tox environments.</li> <li>.pre-commit-config.yaml: Configuration for pre-commit hooks.</li> </ul>"},{"location":"examples/","title":"Examples","text":"<p>Welcome to the GuideLLM examples section! This area is designed to showcase practical applications of GuideLLM for evaluating and optimizing LLM deployments in various real-world scenarios. Our goal is to provide you with concrete examples that demonstrate how to use GuideLLM effectively in your own workflows.</p>"},{"location":"examples/#call-for-contributions","title":"Call for Contributions","text":"<p>Currently, we do not have any specific examples available, but we welcome contributions from the community! If you have examples of how you've used GuideLLM to solve real-world problems or optimize your LLM deployments, we'd love to feature them here.</p> <p>To contribute an example:</p> <ol> <li>Fork the GuideLLM repository</li> <li>Create your example in the <code>docs/examples/</code> directory following our contribution guidelines</li> <li>Submit a pull request with your contribution</li> </ol> <p>Your examples will help others leverage GuideLLM more effectively and contribute to the growing knowledge base around LLM deployment optimization.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Welcome to GuideLLM! This section will guide you through the process of installing the tool, setting up your benchmarking environment, running your first benchmark, and analyzing the results to optimize your LLM deployment for real-world inference workloads.</p> <p>GuideLLM makes it simple to evaluate and optimize your large language model deployments, helping you find the perfect balance between performance, resource utilization, and cost-effectiveness.</p>"},{"location":"getting-started/#quick-start-guides","title":"Quick Start Guides","text":"<p>Follow the guides below in sequence to get the most out of GuideLLM and optimize your LLM deployments for production use.</p> <ul> <li> <p> Installation</p> <p>Learn how to install GuideLLM using pip, from source, or with specific version requirements.</p> <p> Installation Guide</p> </li> <li> <p> Start a Server</p> <p>Set up an OpenAI-compatible server using vLLM or other supported backends to benchmark your LLM deployments.</p> <p> Server Setup Guide</p> </li> <li> <p> Run Benchmarks</p> <p>Learn how to configure and run performance benchmarks against your LLM server under various load conditions.</p> <p> Benchmarking Guide</p> </li> <li> <p> Analyze Results</p> <p>Interpret benchmark results to understand throughput, latency, reliability, and optimize your deployments.</p> <p> Analysis Guide</p> </li> </ul>"},{"location":"getting-started/analyze/","title":"Analyze Results","text":"<p>After running a benchmark, GuideLLM provides comprehensive results that help you understand your LLM deployment's performance. This guide explains how to interpret both console output and file-based results.</p>"},{"location":"getting-started/analyze/#understanding-console-output","title":"Understanding Console Output","text":"<p>Upon benchmark completion, GuideLLM automatically displays results in the console, divided into three main sections:</p>"},{"location":"getting-started/analyze/#1-benchmarks-metadata","title":"1. Benchmarks Metadata","text":"<p>This section provides a high-level summary of the benchmark run, including:</p> <ul> <li>Server configuration: Target URL, model name, and backend details</li> <li>Data configuration: Data source, token counts, and dataset properties</li> <li>Profile arguments: Rate type, maximum duration, request limits, etc.</li> <li>Extras: Any additional metadata provided via the <code>--output-extras</code> argument</li> </ul> <p>Example:</p> <pre><code>Benchmarks Metadata\n------------------\nArgs:        {\"backend_type\": \"openai\", \"target\": \"http://localhost:8000\", \"model\": \"Meta-Llama-3.1-8B-Instruct-quantized\", ...}\nWorker:      {\"type_\": \"generative\", \"backend_type\": \"openai\", \"backend_args\": {\"timeout\": 120.0, ...}, ...}\nRequest Loader: {\"type_\": \"generative\", \"data_args\": {\"prompt_tokens\": 256, \"output_tokens\": 128, ...}, ...}\nExtras:      {}\n</code></pre>"},{"location":"getting-started/analyze/#2-benchmarks-info","title":"2. Benchmarks Info","text":"<p>This section summarizes the key information about each benchmark run, presented as a table with columns:</p> <ul> <li>Type: The benchmark type (e.g., synchronous, constant, poisson, etc.)</li> <li>Start/End Time: When the benchmark started and ended</li> <li>Duration: Total duration of the benchmark in seconds</li> <li>Requests: Count of successful, incomplete, and errored requests</li> <li>Token Stats: Average token counts and totals for prompts and outputs</li> </ul> <p>This section helps you understand what was executed and provides a quick overview of the results.</p>"},{"location":"getting-started/analyze/#3-benchmarks-stats","title":"3. Benchmarks Stats","text":"<p>This is the most critical section for performance analysis. It displays detailed statistics for each metric:</p> <ul> <li> <p>Throughput Metrics:</p> </li> <li> <p>Requests per second (RPS)</p> </li> <li>Request concurrency</li> <li>Output tokens per second</li> <li> <p>Total tokens per second</p> </li> <li> <p>Latency Metrics:</p> </li> <li> <p>Request latency (mean, median, p99)</p> </li> <li>Time to first token (TTFT) (mean, median, p99)</li> <li>Inter-token latency (ITL) (mean, median, p99)</li> <li>Time per output token (mean, median, p99)</li> </ul> <p>The p99 (99<sup>th</sup> percentile) values are particularly important for SLO analysis, as they represent the worst-case performance for 99% of requests.</p>"},{"location":"getting-started/analyze/#analyzing-the-results-file","title":"Analyzing the Results File","text":"<p>For deeper analysis, GuideLLM saves detailed results to a file (default: <code>benchmarks.json</code>). This file contains all metrics with more comprehensive statistics and individual request data.</p>"},{"location":"getting-started/analyze/#file-formats","title":"File Formats","text":"<p>GuideLLM supports multiple output formats:</p> <ul> <li>JSON: Complete benchmark data in JSON format (default)</li> <li>YAML: Complete benchmark data in human-readable YAML format</li> <li>CSV: Summary of key metrics in CSV format</li> </ul> <p>To specify the format, use the <code>--output-path</code> argument with the appropriate extension:</p> <pre><code>guidellm benchmark --target \"http://localhost:8000\" --output-path results.yaml\n</code></pre>"},{"location":"getting-started/analyze/#programmatic-analysis","title":"Programmatic Analysis","text":"<p>For custom analysis, you can reload the results into Python:</p> <pre><code>from guidellm.benchmark import GenerativeBenchmarksReport\n\n# Load results from file\nreport = GenerativeBenchmarksReport.load_file(\"benchmarks.json\")\n\n# Access individual benchmarks\nfor benchmark in report.benchmarks:\n    # Print basic info\n    print(f\"Benchmark: {benchmark.id_}\")\n    print(f\"Type: {benchmark.type_}\")\n\n    # Access metrics\n    print(f\"Avg RPS: {benchmark.metrics.requests_per_second.successful.mean}\")\n    print(f\"p99 latency: {benchmark.metrics.request_latency.successful.percentiles.p99}\")\n    print(f\"TTFT (p99): {benchmark.metrics.time_to_first_token_ms.successful.percentiles.p99}\")\n</code></pre>"},{"location":"getting-started/analyze/#key-performance-indicators","title":"Key Performance Indicators","text":"<p>When analyzing your results, focus on these key indicators:</p>"},{"location":"getting-started/analyze/#1-throughput-and-capacity","title":"1. Throughput and Capacity","text":"<ul> <li>Maximum RPS: What's the highest request rate your server can handle?</li> <li>Concurrency: How many concurrent requests can your server process?</li> <li>Token Throughput: How many tokens per second can your server generate?</li> </ul>"},{"location":"getting-started/analyze/#2-latency-and-responsiveness","title":"2. Latency and Responsiveness","text":"<ul> <li>Time to First Token (TTFT): How quickly does the model start generating output?</li> <li>Inter-Token Latency (ITL): How smoothly does the model generate subsequent tokens?</li> <li>Total Request Latency: How long do complete requests take end-to-end?</li> </ul>"},{"location":"getting-started/analyze/#3-reliability-and-error-rates","title":"3. Reliability and Error Rates","text":"<ul> <li>Success Rate: What percentage of requests completes successfully?</li> <li>Error Distribution: What types of errors occur and at what rates?</li> </ul>"},{"location":"getting-started/analyze/#additional-analysis-techniques","title":"Additional Analysis Techniques","text":""},{"location":"getting-started/analyze/#comparing-different-models-or-hardware","title":"Comparing Different Models or Hardware","text":"<p>Run benchmarks with different models or hardware configurations, then compare:</p> <pre><code>guidellm benchmark --target \"http://server1:8000\" --output-path model1.json\nguidellm benchmark --target \"http://server2:8000\" --output-path model2.json\n</code></pre>"},{"location":"getting-started/analyze/#cost-optimization","title":"Cost Optimization","text":"<p>Calculate cost-effectiveness by analyzing:</p> <ul> <li>Tokens per second per dollar of hardware cost</li> <li>Maximum throughput for different hardware configurations</li> <li>Optimal batch size vs. latency tradeoffs</li> </ul>"},{"location":"getting-started/analyze/#determining-scaling-requirements","title":"Determining Scaling Requirements","text":"<p>Use your benchmark results to plan:</p> <ul> <li>How many servers you need to handle your expected load</li> <li>When to automatically scale up or down based on demand</li> <li>What hardware provides the best price/performance for your workload</li> </ul>"},{"location":"getting-started/benchmark/","title":"Run a Benchmark","text":"<p>After installing GuideLLM and starting a server, you're ready to run benchmarks to evaluate your LLM deployment's performance.</p> <p>Running a GuideLLM benchmark is straightforward. The basic command structure is:</p> <pre><code>guidellm benchmark --target &lt;server-url&gt; [options]\n</code></pre>"},{"location":"getting-started/benchmark/#basic-example","title":"Basic Example","text":"<p>To run a benchmark against your local vLLM server with default settings:</p> <pre><code>guidellm benchmark \\\n  --target \"http://localhost:8000\" \\\n  --data \"prompt_tokens=256,output_tokens=128\"\n</code></pre> <p>This command:</p> <ul> <li>Connects to your vLLM server running at <code>http://localhost:8000</code></li> <li>Uses synthetic data with 256 prompt tokens and 128 output tokens per request</li> <li>Automatically determines the available model on the server</li> <li>Runs a \"sweep\" benchmark (default) to find optimal performance points</li> </ul> <p>During the benchmark, you'll see a progress display similar to this:</p> <p></p>"},{"location":"getting-started/benchmark/#understanding-benchmark-options","title":"Understanding Benchmark Options","text":"<p>GuideLLM offers a wide range of configuration options to customize your benchmarks. Here are the most important parameters you should know:</p>"},{"location":"getting-started/benchmark/#key-parameters","title":"Key Parameters","text":"Parameter Description Example <code>--target</code> URL of the OpenAI-compatible server <code>--target \"http://localhost:8000\"</code> <code>--model</code> Model name to benchmark (optional) <code>--model \"Meta-Llama-3.1-8B-Instruct\"</code> <code>--data</code> Data configuration for benchmarking <code>--data \"prompt_tokens=256,output_tokens=128\"</code> <code>--rate-type</code> Type of benchmark to run <code>--rate-type sweep</code> <code>--rate</code> Request rate or number of benchmarks for sweep <code>--rate 10</code> <code>--max-seconds</code> Duration for each benchmark in seconds <code>--max-seconds 30</code> <code>--output-path</code> Output file path and format <code>--output-path results.json</code>"},{"location":"getting-started/benchmark/#benchmark-types-rate-type","title":"Benchmark Types (<code>--rate-type</code>)","text":"<p>GuideLLM supports several benchmark types:</p> <ul> <li><code>synchronous</code>: Runs requests one at a time (sequential)</li> <li><code>throughput</code>: Tests maximum throughput by running requests in parallel</li> <li><code>concurrent</code>: Runs a fixed number of parallel request streams</li> <li><code>constant</code>: Sends requests at a fixed rate per second</li> <li><code>poisson</code>: Sends requests following a Poisson distribution</li> <li><code>sweep</code>: Automatically determines optimal performance points (default)</li> </ul>"},{"location":"getting-started/benchmark/#data-options","title":"Data Options","text":"<p>For synthetic data, you can customize:</p> <ul> <li><code>prompt_tokens</code>: Average number of tokens for prompts</li> <li><code>output_tokens</code>: Average number of tokens for outputs</li> <li><code>samples</code>: Number of samples to generate (default: 1000)</li> </ul> <p>For a complete list of options, run:</p> <pre><code>guidellm benchmark --help\n</code></pre>"},{"location":"getting-started/benchmark/#working-with-real-data","title":"Working with Real Data","text":"<p>While synthetic data is convenient for quick tests, you can benchmark with real-world data:</p> <pre><code>guidellm benchmark \\\n  --target \"http://localhost:8000\" \\\n  --data \"/path/to/your/dataset.json\" \\\n  --rate-type constant \\\n  --rate 5\n</code></pre> <p>You can also use datasets from HuggingFace or customize synthetic data generation with additional parameters such as standard deviation, minimum, and maximum values.</p> <p>By default, complete results are saved to <code>benchmarks.json</code> in your current directory. Use the <code>--output-path</code> parameter to specify a different location or format.</p> <p>Learn more about dataset options in the Datasets documentation.</p>"},{"location":"getting-started/install/","title":"Install","text":"<p>GuideLLM can be installed using several methods depending on your requirements. Below are the detailed instructions for each installation pathway.</p>"},{"location":"getting-started/install/#prerequisites","title":"Prerequisites","text":"<p>Before installing GuideLLM, ensure you have the following prerequisites:</p> <ul> <li> <p>Operating System: Linux or MacOS</p> </li> <li> <p>Python Version: 3.9 \u2013 3.13</p> </li> <li> <p>Pip Version: Ensure you have the latest version of pip installed. You can upgrade pip using the following command:</p> </li> </ul> <pre><code>python -m pip install --upgrade pip\n</code></pre>"},{"location":"getting-started/install/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/install/#1-install-the-latest-release-from-pypi","title":"1. Install the Latest Release from PyPI","text":"<p>The simplest way to install GuideLLM is via pip from the Python Package Index (PyPI):</p> <pre><code>pip install guidellm\n</code></pre> <p>This will install the latest stable release of GuideLLM.</p>"},{"location":"getting-started/install/#2-install-a-specific-version-from-pypi","title":"2. Install a Specific Version from PyPI","text":"<p>If you need a specific version of GuideLLM, you can specify the version number during installation. For example, to install version <code>0.2.0</code>:</p> <pre><code>pip install guidellm==0.2.0\n</code></pre>"},{"location":"getting-started/install/#3-install-from-source-on-the-main-branch","title":"3. Install from Source on the Main Branch","text":"<p>To install the latest development version of GuideLLM from the main branch, use the following command:</p> <pre><code>pip install git+https://github.com/neuralmagic/guidellm.git\n</code></pre> <p>This will clone the repository and install GuideLLM directly from the main branch.</p>"},{"location":"getting-started/install/#4-install-from-a-specific-branch","title":"4. Install from a Specific Branch","text":"<p>If you want to install GuideLLM from a specific branch (e.g., <code>feature-branch</code>), use the following command:</p> <pre><code>pip install git+https://github.com/neuralmagic/guidellm.git@feature-branch\n</code></pre> <p>Replace <code>feature-branch</code> with the name of the branch you want to install.</p>"},{"location":"getting-started/install/#5-install-from-a-local-clone","title":"5. Install from a Local Clone","text":"<p>If you have cloned the GuideLLM repository locally and want to install it, navigate to the repository directory and run:</p> <pre><code>pip install .\n</code></pre> <p>Alternatively, for development purposes, you can install it in editable mode:</p> <pre><code>pip install -e .\n</code></pre> <p>This allows you to make changes to the source code and have them reflected immediately without reinstalling.</p>"},{"location":"getting-started/install/#verifying-the-installation","title":"Verifying the Installation","text":"<p>After installation, you can verify that GuideLLM is installed correctly by running:</p> <pre><code>guidellm --help\n</code></pre> <p>This should display the installed version of GuideLLM.</p>"},{"location":"getting-started/install/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues during installation, ensure that your Python and pip versions meet the prerequisites. For further assistance, please refer to the GitHub Issues page or consult the Documentation.</p>"},{"location":"getting-started/server/","title":"Start a Server","text":""},{"location":"getting-started/server/#start-a-server_1","title":"Start a Server","text":"<p>Before running GuideLLM benchmarks, you need an OpenAI-compatible server to test against. This guide will help you set up a server quickly.</p>"},{"location":"getting-started/server/#recommended-option-vllm","title":"Recommended Option: vLLM","text":"<p>vLLM\u00a0is the recommended backend for running GuideLLM benchmarks due to its performance and compatibility.</p>"},{"location":"getting-started/server/#installing-vllm","title":"Installing vLLM","text":"<pre><code>pip\u00a0install\u00a0vllm\n</code></pre>"},{"location":"getting-started/server/#starting-a-vllm-server","title":"Starting a vLLM Server","text":"<p>Run the following command to start a vLLM server with a quantized Llama 3.1 8B model:</p> <pre><code>vllm\u00a0serve\u00a0\"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16\"\n</code></pre> <p>This will start an OpenAI-compatible server at\u00a0<code>http://localhost:8000</code>.</p> <p>For more configuration options, refer to the\u00a0vLLM documentation.</p>"},{"location":"getting-started/server/#alternative-servers","title":"Alternative Servers","text":"<p>GuideLLM supports any OpenAI-compatible server, such as TGI, SG Lang, and more. For detailed information on all supported backends, see the\u00a0Backends documentation.</p>"},{"location":"getting-started/server/#verifying-your-server","title":"Verifying Your Server","text":"<p>Once your server is running, you can verify it's working and accessible from your benchmarking server with a simple curl command (if it is running on another machine, replace\u00a0<code>localhost</code> with the server's IP address):</p> <pre><code>curl\u00a0http://localhost:8000/v1/models\n</code></pre> <p>You should see a response listing the available model on your server.</p>"},{"location":"guides/","title":"Guides","text":"<p>Welcome to the GuideLLM guides section! Here you'll find comprehensive documentation covering key components and concepts of the GuideLLM platform. These guides will help you understand the inner workings of GuideLLM, how to configure its various components, and how to interpret benchmark results to optimize your LLM deployments.</p> <p>Whether you're interested in understanding the system architecture, exploring supported backends, configuring datasets, analyzing metrics, or setting service level objectives, these guides provide the detailed information you need to make informed decisions about your LLM deployments.</p>"},{"location":"guides/#key-guides","title":"Key Guides","text":"<ul> <li> <p> Architecture</p> <p>Understanding the modular design of GuideLLM and how core components interact to evaluate LLM deployments.</p> <p> Architecture Overview</p> </li> <li> <p> Backends</p> <p>Learn about supported LLM backends and how to set up OpenAI-compatible servers for benchmarking.</p> <p> Backend Guide</p> </li> <li> <p> Datasets</p> <p>Configure and use different data sources for benchmarking, including synthetic data, Hugging Face datasets, and file-based options.</p> <p> Dataset Guide</p> </li> <li> <p> Metrics</p> <p>Explore the comprehensive metrics provided by GuideLLM to evaluate performance, including latency, throughput, and token-level analysis.</p> <p> Metrics Guide</p> </li> <li> <p> Outputs</p> <p>Learn about supported output formats and how to customize result reporting for your benchmarks.</p> <p> Output Guide</p> </li> <li> <p> Service Level Objectives</p> <p>Define and implement SLOs and SLAs for your LLM deployments to ensure reliability and performance.</p> <p> SLO Guide</p> </li> </ul>"},{"location":"guides/architecture/","title":"Architecture","text":"<p>GuideLLM is designed to evaluate and optimize large language model (LLM) deployments by simulating real-world inference workloads. The architecture is modular, enabling flexibility and scalability. Below is an overview of the core components and their interactions.</p> <pre><code>+------------------+       +------------------+       +------------------+\n|   DatasetCreator | ---&gt;  |   RequestLoader  | ---&gt;  |     Scheduler    |\n+------------------+       +------------------+       +------------------+\n                                                    /         |          \\\n                                                   /          |           \\\n                                                  /           |            \\\n                                                 v            v             v\n                                       +------------------+ +------------------+\n                                       | RequestsWorker   | | RequestsWorker   |\n                                       +------------------+ +------------------+\n                                                 |                     |\n                                                 v                     v\n                                       +------------------+ +------------------+\n                                       |     Backend      | |     Backend      |\n                                       +------------------+ +------------------+\n                                                 |                     |\n                                                 v                     v\n                                       +---------------------------------------+\n                                       |         BenchmarkAggregator           |\n                                       +---------------------------------------+\n                                                 |\n                                                 v\n                                       +------------------+\n                                       |    Benchmarker   |\n                                       +------------------+\n</code></pre>"},{"location":"guides/architecture/#core-components","title":"Core Components","text":""},{"location":"guides/architecture/#1-backend","title":"1. Backend","text":"<p>The <code>Backend</code> is an abstract interface for interacting with generative AI backends. It is responsible for processing requests and generating results. GuideLLM supports OpenAI-compatible HTTP servers, such as vLLM, as backends.</p> <ul> <li>Responsibilities:</li> <li>Accept requests from the <code>RequestsWorker</code>.</li> <li>Generate responses for text or chat completions.</li> <li>Validate backend readiness and available models.</li> </ul>"},{"location":"guides/architecture/#2-requestloader","title":"2. RequestLoader","text":"<p>The <code>RequestLoader</code> handles sourcing data from an iterable and generating requests for the backend. It ensures that data is properly formatted and ready for processing.</p> <ul> <li>Responsibilities:</li> <li>Load data from datasets or synthetic sources.</li> <li>Generate requests in a format compatible with the backend.</li> </ul>"},{"location":"guides/architecture/#3-datasetcreator","title":"3. DatasetCreator","text":"<p>The <code>DatasetCreator</code> is responsible for loading data sources and converting them into Hugging Face (HF) dataset items. These items can then be streamed by the <code>RequestLoader</code>.</p> <ul> <li>Responsibilities:</li> <li>Load datasets from local files, Hugging Face datasets, or synthetic data.</li> <li>Convert data into a format compatible with the <code>RequestLoader</code>.</li> </ul>"},{"location":"guides/architecture/#4-scheduler","title":"4. Scheduler","text":"<p>The <code>Scheduler</code> manages the scheduling of requests to the backend. It uses multiprocessing and multithreading with asyncio to minimize overheads and maximize throughput.</p> <ul> <li>Responsibilities:</li> <li>Schedule requests to the backend.</li> <li>Manage queues for requests and results.</li> <li>Ensure efficient utilization of resources.</li> </ul>"},{"location":"guides/architecture/#5-requestsworker","title":"5. RequestsWorker","text":"<p>The <code>RequestsWorker</code> is a worker process that pulls requests from a queue, processes them using the backend, and sends the results back to the scheduler.</p> <ul> <li>Responsibilities:</li> <li>Process requests from the scheduler.</li> <li>Interact with the backend to generate results.</li> <li>Return results to the scheduler.</li> </ul>"},{"location":"guides/architecture/#6-benchmarker","title":"6. Benchmarker","text":"<p>The <code>Benchmarker</code> wraps around multiple invocations of the <code>Scheduler</code>, one for each benchmark. It aggregates results using a <code>BenchmarkAggregator</code> and compiles them into a <code>Benchmark</code> once complete.</p> <ul> <li>Responsibilities:</li> <li>Manage multiple benchmarks.</li> <li>Aggregate results from the scheduler.</li> <li>Compile results into a final benchmark report.</li> </ul>"},{"location":"guides/architecture/#7-benchmarkaggregator","title":"7. BenchmarkAggregator","text":"<p>The <code>BenchmarkAggregator</code> is responsible for storing and compiling results from the benchmarks.</p> <ul> <li>Responsibilities:</li> <li>Aggregate results from multiple benchmarks.</li> <li>Compile results into a <code>Benchmark</code> object.</li> </ul>"},{"location":"guides/architecture/#component-interactions","title":"Component Interactions","text":"<p>The following diagram illustrates the relationships between the core components:</p>"},{"location":"guides/backends/","title":"Backends","text":"<p>GuideLLM is designed to work with OpenAI-compatible HTTP servers, enabling seamless integration with a variety of generative AI backends. This compatibility ensures that users can evaluate and optimize their large language model (LLM) deployments efficiently. While the current focus is on OpenAI-compatible servers, we welcome contributions to expand support for other backends, including additional server implementations and Python interfaces.</p>"},{"location":"guides/backends/#supported-backends","title":"Supported Backends","text":""},{"location":"guides/backends/#openai-compatible-http-servers","title":"OpenAI-Compatible HTTP Servers","text":"<p>GuideLLM supports OpenAI-compatible HTTP servers, which provide a standardized API for interacting with LLMs. This includes popular implementations such as vLLM and Text Generation Inference (TGI). These servers allow GuideLLM to perform evaluations, benchmarks, and optimizations with minimal setup.</p>"},{"location":"guides/backends/#examples-for-spinning-up-compatible-servers","title":"Examples for Spinning Up Compatible Servers","text":""},{"location":"guides/backends/#1-vllm","title":"1. vLLM","text":"<p>vLLM is a high-performance OpenAI-compatible server designed for efficient LLM inference. It supports a variety of models and provides a simple interface for deployment.</p> <p>First ensure you have vLLM installed (<code>pip install vllm</code>), and then run the following command to start a vLLM server with a Llama 3.1 8B quantized model:</p> <pre><code>vllm serve \"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16\"\n</code></pre> <p>For more information on starting a vLLM server, see the vLLM Documentation.</p>"},{"location":"guides/backends/#2-text-generation-inference-tgi","title":"2. Text Generation Inference (TGI)","text":"<p>Text Generation Inference (TGI) is another OpenAI-compatible server that supports a wide range of models, including those hosted on Hugging Face. TGI is optimized for high-throughput and low-latency inference.</p> <p>To start a TGI server with a Llama 3.1 8B model using Docker, run the following command:</p> <pre><code>docker run --gpus 1 -ti --shm-size 1g --ipc=host --rm -p 8080:80 \\\n  -e MODEL_ID=meta-llama/Meta-Llama-3.1-8B-Instruct \\\n  -e NUM_SHARD=1 \\\n  -e MAX_INPUT_TOKENS=4096 \\\n  -e MAX_TOTAL_TOKENS=6000 \\\n  -e HF_TOKEN=$(cat ~/.cache/huggingface/token) \\\n  ghcr.io/huggingface/text-generation-inference:2.2.0\n</code></pre> <p>For more information on starting a TGI server, see the TGI Documentation.</p>"},{"location":"guides/backends/#expanding-backend-support","title":"Expanding Backend Support","text":"<p>GuideLLM is an open platform, and we encourage contributions to extend its backend support. Whether it's adding new server implementations, integrating with Python-based backends, or enhancing existing capabilities, your contributions are welcome. For more details on how to contribute, see the CONTRIBUTING.md file.</p>"},{"location":"guides/datasets/","title":"Datasets","text":"<p>GuideLLM supports various dataset configurations to enable benchmarking and evaluation of large language models (LLMs). This document provides a comprehensive guide to configuring datasets for different use cases, along with detailed examples and rationale for choosing specific pathways.</p>"},{"location":"guides/datasets/#data-arguments-overview","title":"Data Arguments Overview","text":"<p>The following arguments can be used to configure datasets and their processing:</p> <ul> <li><code>--data</code>: Specifies the dataset source. This can be a file path, Hugging Face dataset ID, synthetic data configuration, or in-memory data.</li> <li><code>--data-args</code>: A JSON string or dictionary argument that allows you to control how datasets are parsed and prepared. This includes specific aliases for GuideLLM flows, such as:</li> <li><code>prompt_column</code>: Specifies the column name for the prompt. By default, GuideLLM will try the most common column names (e.g., <code>prompt</code>, <code>text</code>, <code>input</code>).</li> <li><code>prompt_tokens_count_column</code>: Specifies the column name for the prompt token count. These are used to set the request prompt token count for counting metrics. By default, GuideLLM assumes no token count is provided.</li> <li><code>output_tokens_count_column</code>: Specifies the column name for the output token count. These are used to set the request output token count for the request and counting metrics. By default, GuideLLM assumes no token count is provided.</li> <li><code>split</code>: Specifies the dataset split to use (e.g., <code>train</code>, <code>val</code>, <code>test</code>). By default, GuideLLM will try the most common split names (e.g., <code>train</code>, <code>validation</code>, <code>test</code>) if the dataset has splits, otherwise it will use the entire dataset.</li> <li>Any remaining arguments are passed directly into the dataset constructor as kwargs.</li> <li><code>--data-sampler</code>: Specifies the sampling strategy for datasets. By default, no sampling is applied. When set to <code>random</code>, it enables random shuffling of the dataset, which can be useful for creating diverse batches during benchmarking.</li> <li><code>--processor</code>: Specifies the processor or tokenizer to use. This is only required for synthetic data generation or when local calculations are specified through configuration settings. By default, the processor is set to the <code>--model</code> argument. If <code>--model</code> is not supplied, it defaults to the model retrieved from the backend.</li> <li><code>--processor-args</code>: A JSON string containing any arguments to pass to the processor or tokenizer constructor. These arguments are passed as a dictionary of kwargs.</li> </ul>"},{"location":"guides/datasets/#example-usage","title":"Example Usage","text":"<pre><code>guidellm benchmark \\\n    --target \"http://localhost:8000\" \\\n    --rate-type \"throughput\" \\\n    --max-requests 1000 \\\n    --data \"path/to/dataset|dataset_id\" \\\n    --data-args '{\"prompt_column\": \"prompt\", \"split\": \"train\"}' \\\n    --processor \"path/to/processor\" \\\n    --processor-args '{\"arg1\": \"value1\"}' \\\n    --data-sampler \"random\"\n</code></pre>"},{"location":"guides/datasets/#dataset-types","title":"Dataset Types","text":"<p>GuideLLM supports several types of datasets, each with its own advantages and use cases. Below are the main dataset types supported by GuideLLM, including synthetic data, Hugging Face datasets, file-based datasets, and in-memory datasets.</p>"},{"location":"guides/datasets/#synthetic-data","title":"Synthetic Data","text":"<p>Synthetic datasets allow you to generate data on the fly with customizable parameters. This is useful for controlled experiments, stress testing, and simulating specific scenarios. For example, you might want to evaluate how a model handles long prompts or generates outputs with specific characteristics.</p>"},{"location":"guides/datasets/#example-commands","title":"Example Commands","text":"<pre><code>guidellm benchmark \\\n    --target \"http://localhost:8000\" \\\n    --rate-type \"throughput\" \\\n    --max-requests 1000 \\\n    --data \"prompt_tokens=256,output_tokens=128\"\n</code></pre> <p>Or using a JSON string:</p> <pre><code>guidellm benchmark \\\n    --target \"http://localhost:8000\" \\\n    --rate-type \"throughput\" \\\n    --max-requests 1000 \\\n    --data '{\"prompt_tokens\": 256, \"output_tokens\": 128}'\n</code></pre>"},{"location":"guides/datasets/#configuration-options","title":"Configuration Options","text":"<ul> <li><code>prompt_tokens</code>: Average number of tokens in prompts. If nothing else is specified, all requests will have this number of tokens.</li> <li><code>prompt_tokens_stdev</code>: Standard deviation for prompt tokens. If not supplied and min/max are not specified, no deviation is applied. If not supplied and min/max are specified, a uniform distribution is used.</li> <li><code>prompt_tokens_min</code>: Minimum number of tokens in prompts. If unset and <code>prompt_tokens_stdev</code> is set, the minimum is 1.</li> <li><code>prompt_tokens_max</code>: Maximum number of tokens in prompts. If unset and <code>prompt_tokens_stdev</code> is set, the maximum is 5 times the standard deviation.</li> <li><code>output_tokens</code>: Average number of tokens in outputs. If nothing else is specified, all requests will have this number of tokens.</li> <li><code>output_tokens_stdev</code>: Standard deviation for output tokens. If not supplied and min/max are not specified, no deviation is applied. If not supplied and min/max are specified, a uniform distribution is used.</li> <li><code>output_tokens_min</code>: Minimum number of tokens in outputs. If unset and <code>output_tokens_stdev</code> is set, the minimum is 1.</li> <li><code>output_tokens_max</code>: Maximum number of tokens in outputs. If unset and <code>output_tokens_stdev</code> is set, the maximum is 5 times the standard deviation.</li> <li><code>samples</code>: Number of samples to generate (default: 1000). More samples will increase the time taken to generate the dataset before benchmarking, but will also decrease the likelihood of caching requests.</li> <li><code>source</code>: Source text for generation (default: <code>data:prideandprejudice.txt.gz</code>). This can be any text file, URL containing a text file, or a compressed text file. The text is used to sample from at a word and punctuation granularity and then combined into a single string of the desired lengths.</li> </ul>"},{"location":"guides/datasets/#notes","title":"Notes","text":"<ul> <li>A processor/tokenizer is required. By default, the model passed in or retrieved from the server is used. If unavailable, use the <code>--processor</code> argument to specify a directory or Hugging Face model ID containing the processor/tokenizer files.</li> </ul>"},{"location":"guides/datasets/#hugging-face-datasets","title":"Hugging Face Datasets","text":"<p>GuideLLM supports datasets from the Hugging Face Hub or local directories that follow the <code>datasets</code> library format. This allows you to easily leverage a wide range of datasets for benchmarking and evaluation with real-world data.</p>"},{"location":"guides/datasets/#example-commands_1","title":"Example Commands","text":"<pre><code>guidellm benchmark \\\n    --target \"http://localhost:8000\" \\\n    --rate-type \"throughput\" \\\n    --max-requests 1000 \\\n    --data \"garage-bAInd/Open-Platypus\"\n</code></pre> <p>Or using a local dataset:</p> <pre><code>guidellm benchmark \\\n    --target \"http://localhost:8000\" \\\n    --rate-type \"throughput\" \\\n    --max-requests 1000 \\\n    --data \"path/to/dataset\"\n</code></pre>"},{"location":"guides/datasets/#notes_1","title":"Notes","text":"<ul> <li>Hugging Face datasets can be specified by ID, a local directory, or a path to a local Python file.</li> <li>A supported Hugging Face datasets format is defined as one that can be loaded using the <code>datasets</code> library with the <code>load_dataset</code> function and therefore it is representable as a <code>Dataset</code>, <code>DatasetDict</code>, <code>IterableDataset</code>, or <code>IterableDatasetDict</code>. More information on the supported data types and additional args for the underlying use of <code>load_dataset</code> can be found in the Hugging Face datasets documentation.</li> <li>A processor/tokenizer is only required if <code>GUIDELLM__PREFERRED_PROMPT_TOKENS_SOURCE=\"local\"</code> or <code>GUIDELLM__PREFERRED_OUTPUT_TOKENS_SOURCE=\"local\"</code> is set in the environment. In this case, the processor/tokenizer must be specified using the <code>--processor</code> argument. If not set, the processor/tokenizer will be set to the model passed in or retrieved from the server.</li> </ul>"},{"location":"guides/datasets/#file-based-datasets","title":"File-Based Datasets","text":"<p>GuideLLM supports various file formats for datasets, including text, CSV, JSON, and more. These datasets can be used for benchmarking and evaluation, allowing you to work with structured data in a familiar format that matches your use case.</p>"},{"location":"guides/datasets/#supported-formats-with-examples","title":"Supported Formats with Examples","text":"<ul> <li>Text files (<code>.txt</code>, <code>.text</code>): Where each line is a separate prompt to use.   <pre><code>Hello, how are you?\nWhat is your name?\n</code></pre></li> <li>CSV files (<code>.csv</code>): Where each row is a separate dataset entry and the first row contains the column names. The columns should include <code>prompt</code> or other common names for the prompt which will be used as the prompt column. Additional columns can be included based on the previously mentioned aliases for the <code>--data-args</code> argument.   <pre><code>prompt,output_tokens_count,additional_column,additional_column2\nHello, how are you?,5,foo,bar\nWhat is your name?,3,baz,qux\n</code></pre></li> <li>JSON Lines files (<code>.jsonl</code>): Where each line is a separate JSON object. The objects should include <code>prompt</code> or other common names for the prompt which will be used as the prompt column. Additional fields can be included based on the previously mentioned aliases for the <code>--data-args</code> argument.   <pre><code>{\"prompt\": \"Hello, how are you?\", \"output_tokens_count\": 5, \"additional_column\": \"foo\", \"additional_column2\": \"bar\"}\n{\"prompt\": \"What is your name?\", \"output_tokens_count\": 3, \"additional_column\": \"baz\", \"additional_column2\": \"qux\"}\n</code></pre></li> <li>JSON files (<code>.json</code>): Where the entire dataset is represented as a JSON array of objects nested under a specific key. To surface the correct key to use, a <code>--data-args</code> argument must be passed in of <code>\"field\": \"NAME\"</code> for where the array exists. The objects should include <code>prompt</code> or other common names for the prompt which will be used as the prompt column. Additional fields can be included based on the previously mentioned aliases for the <code>--data-args</code> argument.   <pre><code>{\n  \"version\": \"1.0\",\n  \"data\": [\n    {\"prompt\": \"Hello, how are you?\", \"output_tokens_count\": 5, \"additional_column\": \"foo\", \"additional_column2\": \"bar\"},\n    {\"prompt\": \"What is your name?\", \"output_tokens_count\": 3, \"additional_column\": \"baz\", \"additional_column2\": \"qux\"}\n  ]\n}\n</code></pre></li> <li>Parquet files (<code>.parquet</code>) Example: A binary columnar storage format for efficient data processing. For more information on the supported formats, see the Hugging Face dataset documentation linked in the Notes section.</li> <li>Arrow files (<code>.arrow</code>) Example: A cross-language development platform for in-memory data. For more information on the supported formats, see the Hugging Face dataset documentation linked in the Notes section.</li> <li>HDF5 files (<code>.hdf5</code>) Example: A hierarchical data format for storing large amounts of data. For more information on the supported formats, see the Hugging Face dataset documentation linked in the Notes section.</li> </ul>"},{"location":"guides/datasets/#example-commands_2","title":"Example Commands","text":"<pre><code>guidellm benchmark \\\n    --target \"http://localhost:8000\" \\\n    --rate-type \"throughput\" \\\n    --max-requests 1000 \\\n    --data \"path/to/dataset.ext\" \\\n    --data-args '{\"prompt_column\": \"prompt\", \"split\": \"train\"}'\n</code></pre> <p>Where <code>.ext</code> can be any of the supported file formats listed above.</p>"},{"location":"guides/datasets/#notes_2","title":"Notes","text":"<ul> <li>Ensure the file format matches the expected structure for the dataset and is listed as a supported format.</li> <li>The <code>--data-args</code> argument can be used to specify additional parameters for parsing the dataset, such as the prompt column name or the split to use.</li> <li>A processor/tokenizer is only required if <code>GUIDELLM__PREFERRED_PROMPT_TOKENS_SOURCE=\"local\"</code> or <code>GUIDELLM__PREFERRED_OUTPUT_TOKENS_SOURCE=\"local\"</code> is set in the environment. In this case, the processor/tokenizer must be specified using the <code>--processor</code> argument. If not set, the processor/tokenizer will be set to the model passed in or retrieved from the server.</li> <li>More information on the supported formats and additional args for the underlying use of <code>load_dataset</code> can be found in the Hugging Face datasets documentation.</li> </ul>"},{"location":"guides/datasets/#in-memory-datasets","title":"In-Memory Datasets","text":"<p>In-memory datasets allow you to directly pass data as Python objects, making them ideal for quick prototyping and testing without the need to save data to disk.</p>"},{"location":"guides/datasets/#supported-formats-with-examples_1","title":"Supported Formats with Examples","text":"<ul> <li>Dictionary of columns and values: Where each key is a column name and the values are lists of data points. The keys should include <code>prompt</code> or other common names for the prompt which will be used as the prompt column. Additional columns can be included based on the previously mentioned aliases for the <code>--data-args</code> argument.   <pre><code>{\n    \"column1\": [\"value1\", \"value2\"],\n    \"column2\": [\"value3\", \"value4\"]\n}\n</code></pre></li> <li>List of dictionaries: Where each dictionary represents a single data point with key-value pairs. The dictionaries should include <code>prompt</code> or other common names for the prompt which will be used as the prompt column. Additional fields can be included based on the previously mentioned aliases for the <code>--data-args</code> argument.   <pre><code>[\n    {\"column1\": \"value1\", \"column2\": \"value3\"},\n    {\"column1\": \"value2\", \"column2\": \"value4\"}\n]\n</code></pre></li> <li>List of items: Where each item is a single data point. The items should include <code>prompt</code> or other common names for the prompt which will be used as the prompt column. Additional fields can be included based on the previously mentioned aliases for the <code>--data-args</code> argument.   <pre><code>[\n    \"value1\",\n    \"value2\",\n    \"value3\"\n]\n</code></pre></li> </ul>"},{"location":"guides/datasets/#example-usage_1","title":"Example Usage","text":"<pre><code>from guidellm.benchmark import benchmark_generative_text\n\ndata = [\n    {\"prompt\": \"Hello\", \"output\": \"Hi\"},\n    {\"prompt\": \"How are you?\", \"output\": \"I'm fine.\"}\n]\n\nbenchmark_generative_text(data=data, ...)\n</code></pre>"},{"location":"guides/datasets/#notes_3","title":"Notes","text":"<ul> <li>Ensure that the data format is consistent and adheres to one of the supported structures.</li> <li>For dictionaries, all columns must have the same number of samples.</li> <li>For lists of dictionaries, all items must have the same keys.</li> <li>For lists of items, all elements must be of the same type.</li> <li>A processor/tokenizer is only required if <code>GUIDELLM__PREFERRED_PROMPT_TOKENS_SOURCE=\"local\"</code> or <code>GUIDELLM__PREFERRED_OUTPUT_TOKENS_SOURCE=\"local\"</code> is set in the environment. In this case, the processor/tokenizer must be specified using the <code>--processor</code> argument. If not set, the processor/tokenizer will be set to the model passed in or retrieved from the server.</li> </ul>"},{"location":"guides/metrics/","title":"Metrics","text":"<p>GuideLLM provides a comprehensive set of metrics to evaluate and optimize the performance of large language model (LLM) deployments. These metrics are designed to help users understand the behavior of their models under various conditions, identify bottlenecks, and make informed decisions about scaling and resource allocation. Below, we outline the key metrics measured by GuideLLM, their definitions, use cases, and how they can be interpreted.</p>"},{"location":"guides/metrics/#request-status-metrics","title":"Request Status Metrics","text":""},{"location":"guides/metrics/#successful-incomplete-and-error-requests","title":"Successful, Incomplete, and Error Requests","text":"<ul> <li>Successful Requests: The number of requests that were completed successfully without any errors.</li> <li>Incomplete Requests: The number of requests that were started but not completed, often due to timeouts or interruptions.</li> <li>Error Requests: The number of requests that failed due to errors, such as invalid inputs or server issues.</li> </ul> <p>These metrics provide a breakdown of the overall request statuses, helping users identify the reliability and stability of their LLM deployment.</p>"},{"location":"guides/metrics/#requests-made","title":"Requests Made","text":"<ul> <li>Definition: The total number of requests made during a benchmark run, broken down by status (successful, incomplete, error).</li> <li>Use Case: Helps gauge the workload handled by the system and identify the proportion of requests that were successful versus those that failed or were incomplete.</li> </ul>"},{"location":"guides/metrics/#token-metrics","title":"Token Metrics","text":""},{"location":"guides/metrics/#prompt-tokens-and-counts","title":"Prompt Tokens and Counts","text":"<ul> <li>Definition: The number of tokens in the input prompts sent to the LLM.</li> <li>Use Case: Useful for understanding the complexity of the input data and its impact on model performance.</li> </ul>"},{"location":"guides/metrics/#output-tokens-and-counts","title":"Output Tokens and Counts","text":"<ul> <li>Definition: The number of tokens generated by the LLM in response to the input prompts.</li> <li>Use Case: Helps evaluate the model's output length and its correlation with latency and resource usage.</li> </ul>"},{"location":"guides/metrics/#performance-metrics","title":"Performance Metrics","text":""},{"location":"guides/metrics/#request-rate-requests-per-second","title":"Request Rate (Requests Per Second)","text":"<ul> <li>Definition: The number of requests processed per second.</li> <li>Use Case: Indicates the throughput of the system and its ability to handle concurrent workloads.</li> </ul>"},{"location":"guides/metrics/#request-concurrency","title":"Request Concurrency","text":"<ul> <li>Definition: The number of requests being processed simultaneously.</li> <li>Use Case: Helps evaluate the system's capacity to handle parallel workloads.</li> </ul>"},{"location":"guides/metrics/#output-tokens-per-second","title":"Output Tokens Per Second","text":"<ul> <li>Definition: The average number of output tokens generated per second as a throughput metric across all requests.</li> <li>Use Case: Provides insights into the server's performance and efficiency in generating output tokens.</li> </ul>"},{"location":"guides/metrics/#total-tokens-per-second","title":"Total Tokens Per Second","text":"<ul> <li>Definition: The combined rate of prompt and output tokens processed per second as a throughput metric across all requests.</li> <li>Use Case: Provides insights into the server's overall performance and efficiency in processing both prompt and output tokens.</li> </ul>"},{"location":"guides/metrics/#request-latency","title":"Request Latency","text":"<ul> <li>Definition: The time taken to process a single request, from start to finish.</li> <li>Use Case: A critical metric for evaluating the responsiveness of the system.</li> </ul>"},{"location":"guides/metrics/#time-to-first-token-ttft","title":"Time to First Token (TTFT)","text":"<ul> <li>Definition: The time taken to generate the first token of the output.</li> <li>Use Case: Indicates the initial response time of the model, which is crucial for user-facing applications.</li> </ul>"},{"location":"guides/metrics/#inter-token-latency-itl","title":"Inter-Token Latency (ITL)","text":"<ul> <li>Definition: The average time between generating consecutive tokens in the output, excluding the first token.</li> <li>Use Case: Helps assess the smoothness and speed of token generation.</li> </ul>"},{"location":"guides/metrics/#time-per-output-token","title":"Time Per Output Token","text":"<ul> <li>Definition: The average time taken to generate each output token, including the first token.</li> <li>Use Case: Provides a detailed view of the model's token generation efficiency.</li> </ul>"},{"location":"guides/metrics/#statistical-summaries","title":"Statistical Summaries","text":"<p>GuideLLM provides detailed statistical summaries for each of the above metrics using the <code>StatusDistributionSummary</code> and <code>DistributionSummary</code> models. These summaries include the following statistics:</p>"},{"location":"guides/metrics/#summary-statistics","title":"Summary Statistics","text":"<ul> <li>Mean: The average value of the metric.</li> <li>Median: The middle value of the metric when sorted.</li> <li>Mode: The most frequently occurring value of the metric.</li> <li>Variance: The measure of how much the values of the metric vary.</li> <li>Standard Deviation (Std Dev): The square root of the variance, indicating the spread of the values.</li> <li>Min: The minimum value of the metric.</li> <li>Max: The maximum value of the metric.</li> <li>Count: The total number of data points for the metric.</li> <li>Total Sum: The sum of all values for the metric.</li> </ul>"},{"location":"guides/metrics/#percentiles","title":"Percentiles","text":"<p>GuideLLM calculates a comprehensive set of percentiles for each metric, including:</p> <ul> <li>0.1th Percentile (p001): The value below which 0.1% of the data falls.</li> <li>1<sup>st</sup> Percentile (p01): The value below which 1% of the data falls.</li> <li>5<sup>th</sup> Percentile (p05): The value below which 5% of the data falls.</li> <li>10<sup>th</sup> Percentile (p10): The value below which 10% of the data falls.</li> <li>25<sup>th</sup> Percentile (p25): The value below which 25% of the data falls.</li> <li>75<sup>th</sup> Percentile (p75): The value below which 75% of the data falls.</li> <li>90<sup>th</sup> Percentile (p90): The value below which 90% of the data falls.</li> <li>95<sup>th</sup> Percentile (p95): The value below which 95% of the data falls.</li> <li>99<sup>th</sup> Percentile (p99): The value below which 99% of the data falls.</li> <li>99.9<sup>th</sup> Percentile (p999): The value below which 99.9% of the data falls.</li> </ul>"},{"location":"guides/metrics/#use-cases-for-statistical-summaries","title":"Use Cases for Statistical Summaries","text":"<ul> <li>Mean and Median: Provide a central tendency of the metric values.</li> <li>Variance and Std Dev: Indicate the variability and consistency of the metric.</li> <li>Min and Max: Highlight the range of the metric values.</li> <li>Percentiles: Offer a detailed view of the distribution, helping identify outliers and performance at different levels of service.</li> </ul> <p>By combining these metrics and statistical summaries, GuideLLM enables users to gain a deep understanding of their LLM deployments, optimize performance, and ensure scalability and cost-effectiveness.</p>"},{"location":"guides/outputs/","title":"Output Types","text":"<p>GuideLLM provides flexible options for outputting benchmark results, catering to both console-based summaries and file-based detailed reports. This document outlines the supported output types, their configurations, and how to utilize them effectively.</p> <p>For all of the output formats, <code>--output-extras</code> can be used to include additional information. This could include tags, metadata, hardware details, and other relevant information that can be useful for analysis. This must be supplied as a JSON encoded string. For example:</p> <pre><code>guidellm benchmark \\\n  --target \"http://localhost:8000\" \\\n  --rate-type sweep \\\n  --max-seconds 30 \\\n  --data \"prompt_tokens=256,output_tokens=128\" \\\n  --output-extras '{\"tag\": \"my_tag\", \"metadata\": {\"key\": \"value\"}}'\n</code></pre>"},{"location":"guides/outputs/#console-output","title":"Console Output","text":"<p>By default, GuideLLM displays benchmark results and progress directly in the console. The console progress and outputs are divided into multiple sections:</p> <ol> <li>Initial Setup Progress: Displays the progress of the initial setup, including server connection and data preparation.</li> <li>Benchmark Progress: Shows the progress of the benchmark runs, including the number of requests completed and the current rate.</li> <li>Final Results: Summarizes the benchmark results, including average latency, throughput, and other key metrics.</li> <li>Benchmarks Metadata: Summarizes the benchmark run, including server details, data configurations, and profile arguments.</li> <li>Benchmarks Info: Provides a high-level overview of each benchmark, including request statuses, token counts, and durations.</li> <li>Benchmarks Stats: Displays detailed statistics for each benchmark, such as request rates, concurrency, latency, and token-level metrics.</li> </ol>"},{"location":"guides/outputs/#disabling-console-output","title":"Disabling Console Output","text":"<p>To disable the progress outputs to the console, use the <code>disable-progress</code> flag when running the <code>guidellm benchmark</code> command. For example:</p> <pre><code>guidellm benchmark \\\n  --target \"http://localhost:8000\" \\\n  --rate-type sweep \\\n  --max-seconds 30 \\\n  --data \"prompt_tokens=256,output_tokens=128\" \\\n  --disable-progress\n</code></pre> <p>To disable console output, use the <code>--disable-console-outputs</code> flag when running the <code>guidellm benchmark</code> command. For example:</p> <pre><code>guidellm benchmark \\\n  --target \"http://localhost:8000\" \\\n  --rate-type sweep \\\n  --max-seconds 30 \\\n  --data \"prompt_tokens=256,output_tokens=128\" \\\n  --disable-console-outputs\n</code></pre>"},{"location":"guides/outputs/#enabling-extra-information","title":"Enabling Extra Information","text":"<p>GuideLLM includes the option to display extra information during the benchmark runs to monitor the overheads and performance of the system. This can be enabled by using the <code>--display-scheduler-stats</code> flag when running the <code>guidellm benchmark</code> command. For example:</p> <pre><code>guidellm benchmark \\\n  --target \"http://localhost:8000\" \\\n  --rate-type sweep \\\n  --max-seconds 30 \\\n  --data \"prompt_tokens=256,output_tokens=128\" \\\n  --display-scheduler-stats\n</code></pre> <p>The above command will display an additional row for each benchmark within the progress output, showing the scheduler overheads and other relevant information.</p>"},{"location":"guides/outputs/#file-based-outputs","title":"File-Based Outputs","text":"<p>GuideLLM supports saving benchmark results to files in various formats, including JSON, YAML, and CSV. These files can be used for further analysis, reporting, or reloading into Python for detailed exploration.</p>"},{"location":"guides/outputs/#supported-file-formats","title":"Supported File Formats","text":"<ol> <li>JSON: Contains all benchmark results, including full statistics and request data. This format is ideal for reloading into Python for in-depth analysis.</li> <li>YAML: Similar to JSON, YAML files include all benchmark results and are human-readable.</li> <li>CSV: Provides a summary of the benchmark data, focusing on key metrics and statistics. Note that CSV does not include detailed request-level data.</li> </ol>"},{"location":"guides/outputs/#configuring-file-outputs","title":"Configuring File Outputs","text":"<ul> <li>Output Path: Use the <code>--output-path</code> argument to specify the file path or directory for saving the results. If a directory is provided, the results will be saved as <code>benchmarks.json</code> by default. The file type is determined by the file extension (e.g., <code>.json</code>, <code>.yaml</code>, <code>.csv</code>).</li> <li>Sampling: To limit the size of the output files, you can configure sampling options for the dataset using the <code>--output-sampling</code> argument.</li> </ul> <p>Example command to save results in YAML format:</p> <pre><code>guidellm benchmark \\\n  --target \"http://localhost:8000\" \\\n  --rate-type sweep \\\n  --max-seconds 30 \\\n  --data \"prompt_tokens=256,output_tokens=128\" \\\n  --output-path \"results/benchmarks.csv\" \\\n  --output-sampling 20\n</code></pre>"},{"location":"guides/outputs/#reloading-results","title":"Reloading Results","text":"<p>JSON and YAML files can be reloaded into Python for further analysis using the <code>GenerativeBenchmarksReport</code> class. Below is a sample code snippet for reloading results:</p> <pre><code>from guidellm.benchmark import GenerativeBenchmarksReport\n\nreport = GenerativeBenchmarksReport.load_file(\n    path=\"benchmarks.json\",\n)\nbenchmarks = report.benchmarks\n\nfor benchmark in benchmarks:\n    print(benchmark.id_)\n</code></pre> <p>For more details on the <code>GenerativeBenchmarksReport</code> class and its methods, refer to the source code.</p>"},{"location":"guides/service_level_objectives/","title":"Service Level Objectives","text":"<p>Service Level Objectives (SLOs) and Service Level Agreements (SLAs) are critical for ensuring the quality and reliability of large language model (LLM) deployments. They define measurable performance and reliability targets that a system must meet to satisfy user expectations and business requirements. Below, we outline the key concepts, tradeoffs, and examples of SLOs/SLAs for various LLM use cases.</p>"},{"location":"guides/service_level_objectives/#definitions","title":"Definitions","text":""},{"location":"guides/service_level_objectives/#service-level-objectives-slos","title":"Service Level Objectives (SLOs)","text":"<p>SLOs are internal performance and reliability targets that guide the operation and optimization of a system. They are typically defined as measurable metrics, such as latency, throughput, or error rates, and serve as benchmarks for evaluating system performance.</p>"},{"location":"guides/service_level_objectives/#service-level-agreements-slas","title":"Service Level Agreements (SLAs)","text":"<p>SLAs are formal agreements between a service provider and its users or customers. They specify the performance and reliability guarantees that the provider commits to delivering. SLAs often include penalties or compensations if the agreed-upon targets are not met.</p>"},{"location":"guides/service_level_objectives/#tradeoffs-between-latency-and-throughput","title":"Tradeoffs Between Latency and Throughput","text":"<p>When setting SLOs and SLAs for LLM deployments, it is essential to balance the tradeoffs between latency, throughput, and cost efficiency:</p> <ul> <li>Latency: The time taken to process individual requests, including metrics like Time to First Token (TTFT) and Inter-Token Latency (ITL). Low latency is critical for user-facing applications where responsiveness is key.</li> <li>Throughput: The number of requests processed per second. High throughput is essential for handling large-scale workloads efficiently.</li> <li>Cost Efficiency: The cost per request, which depends on the system's resource utilization and throughput. Optimizing for cost efficiency often involves increasing throughput, which may come at the expense of higher latency for individual requests.</li> </ul> <p>For example, a chat application may prioritize low latency to ensure a smooth user experience, while a batch processing system for content generation may prioritize high throughput to minimize costs.</p>"},{"location":"guides/service_level_objectives/#examples-of-slosslas-for-common-llm-use-cases","title":"Examples of SLOs/SLAs for Common LLM Use Cases","text":""},{"location":"guides/service_level_objectives/#real-time-application-facing-usage","title":"Real-Time, Application-Facing Usage","text":"<p>This category includes use cases where low latency is critical for external-facing applications. These systems must ensure quick responses to maintain user satisfaction and meet stringent performance requirements.</p>"},{"location":"guides/service_level_objectives/#1-chat-applications","title":"1. Chat Applications","text":"<p>Enterprise Use Case: A customer support chatbot for an e-commerce platform, where quick responses are critical to maintaining user satisfaction and resolving issues in real time.</p> <ul> <li>SLOs:</li> <li>TTFT: \u2264 200ms for 99% of requests</li> <li>ITL: \u2264 50ms for 99% of requests</li> </ul>"},{"location":"guides/service_level_objectives/#2-retrieval-augmented-generation-rag","title":"2. Retrieval-Augmented Generation (RAG)","text":"<p>Enterprise Use Case: A legal document search tool that retrieves and summarizes relevant case law in real time for lawyers during court proceedings.</p> <ul> <li>SLOs:</li> <li>Request Latency: \u2264 3s for 99% of requests</li> <li>TTFT: \u2264 300ms for 99% of requests (if iterative outputs are shown)</li> <li>ITL: \u2264 100ms for 99% of requests (if iterative outputs are shown)</li> </ul>"},{"location":"guides/service_level_objectives/#3-instruction-following-agentic-ai","title":"3. Instruction Following / Agentic AI","text":"<p>Enterprise Use Case: A virtual assistant for scheduling meetings and managing tasks, where quick responses are essential for user productivity.</p> <ul> <li>SLOs:</li> <li>Request Latency: \u2264 5s for 99% of requests</li> </ul>"},{"location":"guides/service_level_objectives/#real-time-internal-usage","title":"Real-Time, Internal Usage","text":"<p>This category includes use cases where low latency is important but less stringent compared to external-facing applications. These systems are often used by internal teams within enterprises, but if provided as a service, they may require external-facing guarantees.</p>"},{"location":"guides/service_level_objectives/#4-content-generation","title":"4. Content Generation","text":"<p>Enterprise Use Case: An internal marketing tool for generating ad copy and social media posts, where slightly higher latencies are acceptable compared to external-facing applications.</p> <ul> <li>SLOs:</li> <li>TTFT: \u2264 600ms for 99% of requests</li> <li>ITL: \u2264 200ms for 99% of requests</li> </ul>"},{"location":"guides/service_level_objectives/#5-code-generation","title":"5. Code Generation","text":"<p>Enterprise Use Case: A developer productivity tool for generating boilerplate code and API integrations, used internally by engineering teams.</p> <ul> <li>SLOs:</li> <li>TTFT: \u2264 500ms for 99% of requests</li> <li>ITL: \u2264 150ms for 99% of requests</li> </ul>"},{"location":"guides/service_level_objectives/#6-code-completion","title":"6. Code Completion","text":"<p>Enterprise Use Case: An integrated development environment (IDE) plugin for auto-completing code snippets, improving developer efficiency.</p> <ul> <li>SLOs:</li> <li>Request Latency: \u2264 2s for 99% of requests</li> </ul>"},{"location":"guides/service_level_objectives/#offline-batch-use-cases","title":"Offline, Batch Use Cases","text":"<p>This category includes use cases where maximizing throughput is the primary concern. These systems process large volumes of data in batches, often during off-peak hours, to optimize resource utilization and cost efficiency.</p>"},{"location":"guides/service_level_objectives/#7-summarization","title":"7. Summarization","text":"<p>Enterprise Use Case: A tool for summarizing customer reviews to extract insights for product improvement, processed in large batches overnight.</p> <ul> <li>SLOs:</li> <li>Maximize Throughput: \u2265 100 requests per second</li> </ul>"},{"location":"guides/service_level_objectives/#8-analysis","title":"8. Analysis","text":"<p>Enterprise Use Case: A data analysis pipeline for generating actionable insights from sales data, used to inform quarterly business strategies.</p> <ul> <li>SLOs:</li> <li>Maximize Throughput: \u2265 150 requests per second</li> </ul>"},{"location":"guides/service_level_objectives/#conclusion","title":"Conclusion","text":"<p>Setting appropriate SLOs and SLAs is essential for optimizing LLM deployments to meet user expectations and business requirements. By balancing latency, throughput, and cost efficiency, organizations can ensure high-quality service while minimizing operational costs. The examples provided above serve as a starting point for defining SLOs and SLAs tailored to specific use cases.</p>"},{"location":"reference/guidellm/","title":"guidellm","text":"<p>Guidellm is a package that provides an easy and intuitive interface for evaluating and benchmarking large language models (LLMs).</p>"},{"location":"reference/guidellm/#guidellm.DatasetSettings","title":"<code>DatasetSettings</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Dataset settings for the application</p> Source code in <code>src/guidellm/config.py</code> <pre><code>class DatasetSettings(BaseModel):\n    \"\"\"\n    Dataset settings for the application\n    \"\"\"\n\n    preferred_data_columns: list[str] = Field(\n        default_factory=lambda: [\n            \"prompt\",\n            \"instruction\",\n            \"input\",\n            \"inputs\",\n            \"question\",\n            \"context\",\n            \"text\",\n            \"content\",\n            \"body\",\n            \"data\",\n        ]\n    )\n    preferred_data_splits: list[str] = Field(\n        default_factory=lambda: [\"test\", \"tst\", \"validation\", \"val\", \"train\"]\n    )\n</code></pre>"},{"location":"reference/guidellm/#guidellm.Environment","title":"<code>Environment</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for the supported environments</p> Source code in <code>src/guidellm/config.py</code> <pre><code>class Environment(str, Enum):\n    \"\"\"\n    Enum for the supported environments\n    \"\"\"\n\n    LOCAL = \"local\"\n    DEV = \"dev\"\n    STAGING = \"staging\"\n    PROD = \"prod\"\n</code></pre>"},{"location":"reference/guidellm/#guidellm.LoggingSettings","title":"<code>LoggingSettings</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Logging settings for the application</p> Source code in <code>src/guidellm/config.py</code> <pre><code>class LoggingSettings(BaseModel):\n    \"\"\"\n    Logging settings for the application\n    \"\"\"\n\n    disabled: bool = False\n    clear_loggers: bool = True\n    console_log_level: str = \"WARNING\"\n    log_file: Optional[str] = None\n    log_file_level: Optional[str] = None\n</code></pre>"},{"location":"reference/guidellm/#guidellm.OpenAISettings","title":"<code>OpenAISettings</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>OpenAI settings for the application to connect to the API for OpenAI server based pathways</p> Source code in <code>src/guidellm/config.py</code> <pre><code>class OpenAISettings(BaseModel):\n    \"\"\"\n    OpenAI settings for the application to connect to the API\n    for OpenAI server based pathways\n    \"\"\"\n\n    api_key: Optional[str] = None\n    bearer_token: Optional[str] = None\n    organization: Optional[str] = None\n    project: Optional[str] = None\n    base_url: str = \"http://localhost:8000\"\n    max_output_tokens: int = 16384\n</code></pre>"},{"location":"reference/guidellm/#guidellm.Settings","title":"<code>Settings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>All the settings are powered by pydantic_settings and could be populated from the .env file.</p> <p>The format to populate the settings is next</p> <pre><code>export GUIDELLM__LOGGING__DISABLED=true\nexport GUIDELLM__OPENAI__API_KEY=******\n</code></pre> Source code in <code>src/guidellm/config.py</code> <pre><code>class Settings(BaseSettings):\n    \"\"\"\n    All the settings are powered by pydantic_settings and could be\n    populated from the .env file.\n\n    The format to populate the settings is next\n\n    ```sh\n    export GUIDELLM__LOGGING__DISABLED=true\n    export GUIDELLM__OPENAI__API_KEY=******\n    ```\n    \"\"\"\n\n    model_config = SettingsConfigDict(\n        env_prefix=\"GUIDELLM__\",\n        env_nested_delimiter=\"__\",\n        extra=\"ignore\",\n        validate_default=True,\n        env_file=\".env\",\n    )\n\n    # general settings\n    env: Environment = Environment.PROD\n    default_async_loop_sleep: float = 10e-5\n    logging: LoggingSettings = LoggingSettings()\n    default_sweep_number: int = 10\n\n    # HTTP settings\n    request_follow_redirects: bool = True\n    request_timeout: int = 60 * 5  # 5 minutes\n    request_http2: bool = True\n\n    # Scheduler settings\n    max_concurrency: int = 512\n    max_worker_processes: int = 10\n    max_add_requests_per_loop: int = 20\n\n    # Data settings\n    dataset: DatasetSettings = DatasetSettings()\n\n    # Request/stats settings\n    preferred_prompt_tokens_source: Optional[\n        Literal[\"request\", \"response\", \"local\"]\n    ] = \"response\"\n    preferred_output_tokens_source: Optional[\n        Literal[\"request\", \"response\", \"local\"]\n    ] = \"response\"\n    preferred_backend: Literal[\"openai\"] = \"openai\"\n    preferred_route: Literal[\"text_completions\", \"chat_completions\"] = (\n        \"text_completions\"\n    )\n    openai: OpenAISettings = OpenAISettings()\n\n    # Output settings\n    table_border_char: str = \"=\"\n    table_headers_border_char: str = \"-\"\n    table_column_separator_char: str = \"|\"\n\n    @model_validator(mode=\"after\")\n    @classmethod\n    def set_default_source(cls, values):\n        return values\n\n    def generate_env_file(self) -&gt; str:\n        \"\"\"\n        Generate the .env file from the current settings\n        \"\"\"\n        return Settings._recursive_generate_env(\n            self,\n            self.model_config[\"env_prefix\"],  # type: ignore  # noqa: PGH003\n            self.model_config[\"env_nested_delimiter\"],  # type: ignore  # noqa: PGH003\n        )\n\n    @staticmethod\n    def _recursive_generate_env(model: BaseModel, prefix: str, delimiter: str) -&gt; str:\n        env_file = \"\"\n        add_models = []\n        for key, value in model.model_dump().items():\n            if isinstance(value, BaseModel):\n                # add nested properties to be processed after the current level\n                add_models.append((key, value))\n                continue\n\n            dict_values = (\n                {\n                    f\"{prefix}{key.upper()}{delimiter}{sub_key.upper()}\": sub_value\n                    for sub_key, sub_value in value.items()\n                }\n                if isinstance(value, dict)\n                else {f\"{prefix}{key.upper()}\": value}\n            )\n\n            for tag, sub_value in dict_values.items():\n                if isinstance(sub_value, Sequence) and not isinstance(sub_value, str):\n                    value_str = \",\".join(f'\"{item}\"' for item in sub_value)\n                    env_file += f\"{tag}=[{value_str}]\\n\"\n                elif isinstance(sub_value, dict):\n                    value_str = json.dumps(sub_value)\n                    env_file += f\"{tag}={value_str}\\n\"\n                elif not sub_value:\n                    env_file += f\"{tag}=\\n\"\n                else:\n                    env_file += f'{tag}=\"{sub_value}\"\\n'\n\n        for key, value in add_models:\n            env_file += Settings._recursive_generate_env(\n                value, f\"{prefix}{key.upper()}{delimiter}\", delimiter\n            )\n        return env_file\n</code></pre>"},{"location":"reference/guidellm/#guidellm.Settings.generate_env_file","title":"<code>generate_env_file()</code>","text":"<p>Generate the .env file from the current settings</p> Source code in <code>src/guidellm/config.py</code> <pre><code>def generate_env_file(self) -&gt; str:\n    \"\"\"\n    Generate the .env file from the current settings\n    \"\"\"\n    return Settings._recursive_generate_env(\n        self,\n        self.model_config[\"env_prefix\"],  # type: ignore  # noqa: PGH003\n        self.model_config[\"env_nested_delimiter\"],  # type: ignore  # noqa: PGH003\n    )\n</code></pre>"},{"location":"reference/guidellm/#guidellm.configure_logger","title":"<code>configure_logger(config=settings.logging)</code>","text":"<p>Configure the metrics for LLM Compressor. This function sets up the console and file logging as per the specified or default parameters.</p> <p>Note: Environment variables take precedence over the function parameters.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>LoggingSettings</code> <p>The configuration for the logger to use.</p> <code>logging</code> Source code in <code>src/guidellm/logger.py</code> <pre><code>def configure_logger(config: LoggingSettings = settings.logging):\n    \"\"\"\n    Configure the metrics for LLM Compressor.\n    This function sets up the console and file logging\n    as per the specified or default parameters.\n\n    Note: Environment variables take precedence over the function parameters.\n\n    :param config: The configuration for the logger to use.\n    :type config: LoggerConfig\n    \"\"\"\n\n    if config.disabled:\n        logger.disable(\"guidellm\")\n        return\n\n    logger.enable(\"guidellm\")\n\n    if config.clear_loggers:\n        logger.remove()\n\n    # log as a human readable string with the time, function, level, and message\n    logger.add(\n        sys.stdout,\n        level=config.console_log_level.upper(),\n        format=\"{time} | {function} | {level} - {message}\",\n    )\n\n    if config.log_file or config.log_file_level:\n        log_file = config.log_file or \"guidellm.log\"\n        log_file_level = config.log_file_level or \"INFO\"\n        # log as json to the file for easier parsing\n        logger.add(log_file, level=log_file_level.upper(), serialize=True)\n</code></pre>"},{"location":"reference/guidellm/#guidellm.print_config","title":"<code>print_config()</code>","text":"<p>Print the current configuration settings</p> Source code in <code>src/guidellm/config.py</code> <pre><code>def print_config():\n    \"\"\"\n    Print the current configuration settings\n    \"\"\"\n    print(f\"Settings: \\n{settings.generate_env_file()}\")  # noqa: T201\n</code></pre>"},{"location":"reference/guidellm/#guidellm.reload_settings","title":"<code>reload_settings()</code>","text":"<p>Reload the settings from the environment variables</p> Source code in <code>src/guidellm/config.py</code> <pre><code>def reload_settings():\n    \"\"\"\n    Reload the settings from the environment variables\n    \"\"\"\n    new_settings = Settings()\n    settings.__dict__.update(new_settings.__dict__)\n</code></pre>"},{"location":"reference/guidellm/config/","title":"guidellm.config","text":""},{"location":"reference/guidellm/config/#guidellm.config.DatasetSettings","title":"<code>DatasetSettings</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Dataset settings for the application</p> Source code in <code>src/guidellm/config.py</code> <pre><code>class DatasetSettings(BaseModel):\n    \"\"\"\n    Dataset settings for the application\n    \"\"\"\n\n    preferred_data_columns: list[str] = Field(\n        default_factory=lambda: [\n            \"prompt\",\n            \"instruction\",\n            \"input\",\n            \"inputs\",\n            \"question\",\n            \"context\",\n            \"text\",\n            \"content\",\n            \"body\",\n            \"data\",\n        ]\n    )\n    preferred_data_splits: list[str] = Field(\n        default_factory=lambda: [\"test\", \"tst\", \"validation\", \"val\", \"train\"]\n    )\n</code></pre>"},{"location":"reference/guidellm/config/#guidellm.config.Environment","title":"<code>Environment</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for the supported environments</p> Source code in <code>src/guidellm/config.py</code> <pre><code>class Environment(str, Enum):\n    \"\"\"\n    Enum for the supported environments\n    \"\"\"\n\n    LOCAL = \"local\"\n    DEV = \"dev\"\n    STAGING = \"staging\"\n    PROD = \"prod\"\n</code></pre>"},{"location":"reference/guidellm/config/#guidellm.config.LoggingSettings","title":"<code>LoggingSettings</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Logging settings for the application</p> Source code in <code>src/guidellm/config.py</code> <pre><code>class LoggingSettings(BaseModel):\n    \"\"\"\n    Logging settings for the application\n    \"\"\"\n\n    disabled: bool = False\n    clear_loggers: bool = True\n    console_log_level: str = \"WARNING\"\n    log_file: Optional[str] = None\n    log_file_level: Optional[str] = None\n</code></pre>"},{"location":"reference/guidellm/config/#guidellm.config.OpenAISettings","title":"<code>OpenAISettings</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>OpenAI settings for the application to connect to the API for OpenAI server based pathways</p> Source code in <code>src/guidellm/config.py</code> <pre><code>class OpenAISettings(BaseModel):\n    \"\"\"\n    OpenAI settings for the application to connect to the API\n    for OpenAI server based pathways\n    \"\"\"\n\n    api_key: Optional[str] = None\n    bearer_token: Optional[str] = None\n    organization: Optional[str] = None\n    project: Optional[str] = None\n    base_url: str = \"http://localhost:8000\"\n    max_output_tokens: int = 16384\n</code></pre>"},{"location":"reference/guidellm/config/#guidellm.config.Settings","title":"<code>Settings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>All the settings are powered by pydantic_settings and could be populated from the .env file.</p> <p>The format to populate the settings is next</p> <pre><code>export GUIDELLM__LOGGING__DISABLED=true\nexport GUIDELLM__OPENAI__API_KEY=******\n</code></pre> Source code in <code>src/guidellm/config.py</code> <pre><code>class Settings(BaseSettings):\n    \"\"\"\n    All the settings are powered by pydantic_settings and could be\n    populated from the .env file.\n\n    The format to populate the settings is next\n\n    ```sh\n    export GUIDELLM__LOGGING__DISABLED=true\n    export GUIDELLM__OPENAI__API_KEY=******\n    ```\n    \"\"\"\n\n    model_config = SettingsConfigDict(\n        env_prefix=\"GUIDELLM__\",\n        env_nested_delimiter=\"__\",\n        extra=\"ignore\",\n        validate_default=True,\n        env_file=\".env\",\n    )\n\n    # general settings\n    env: Environment = Environment.PROD\n    default_async_loop_sleep: float = 10e-5\n    logging: LoggingSettings = LoggingSettings()\n    default_sweep_number: int = 10\n\n    # HTTP settings\n    request_follow_redirects: bool = True\n    request_timeout: int = 60 * 5  # 5 minutes\n    request_http2: bool = True\n\n    # Scheduler settings\n    max_concurrency: int = 512\n    max_worker_processes: int = 10\n    max_add_requests_per_loop: int = 20\n\n    # Data settings\n    dataset: DatasetSettings = DatasetSettings()\n\n    # Request/stats settings\n    preferred_prompt_tokens_source: Optional[\n        Literal[\"request\", \"response\", \"local\"]\n    ] = \"response\"\n    preferred_output_tokens_source: Optional[\n        Literal[\"request\", \"response\", \"local\"]\n    ] = \"response\"\n    preferred_backend: Literal[\"openai\"] = \"openai\"\n    preferred_route: Literal[\"text_completions\", \"chat_completions\"] = (\n        \"text_completions\"\n    )\n    openai: OpenAISettings = OpenAISettings()\n\n    # Output settings\n    table_border_char: str = \"=\"\n    table_headers_border_char: str = \"-\"\n    table_column_separator_char: str = \"|\"\n\n    @model_validator(mode=\"after\")\n    @classmethod\n    def set_default_source(cls, values):\n        return values\n\n    def generate_env_file(self) -&gt; str:\n        \"\"\"\n        Generate the .env file from the current settings\n        \"\"\"\n        return Settings._recursive_generate_env(\n            self,\n            self.model_config[\"env_prefix\"],  # type: ignore  # noqa: PGH003\n            self.model_config[\"env_nested_delimiter\"],  # type: ignore  # noqa: PGH003\n        )\n\n    @staticmethod\n    def _recursive_generate_env(model: BaseModel, prefix: str, delimiter: str) -&gt; str:\n        env_file = \"\"\n        add_models = []\n        for key, value in model.model_dump().items():\n            if isinstance(value, BaseModel):\n                # add nested properties to be processed after the current level\n                add_models.append((key, value))\n                continue\n\n            dict_values = (\n                {\n                    f\"{prefix}{key.upper()}{delimiter}{sub_key.upper()}\": sub_value\n                    for sub_key, sub_value in value.items()\n                }\n                if isinstance(value, dict)\n                else {f\"{prefix}{key.upper()}\": value}\n            )\n\n            for tag, sub_value in dict_values.items():\n                if isinstance(sub_value, Sequence) and not isinstance(sub_value, str):\n                    value_str = \",\".join(f'\"{item}\"' for item in sub_value)\n                    env_file += f\"{tag}=[{value_str}]\\n\"\n                elif isinstance(sub_value, dict):\n                    value_str = json.dumps(sub_value)\n                    env_file += f\"{tag}={value_str}\\n\"\n                elif not sub_value:\n                    env_file += f\"{tag}=\\n\"\n                else:\n                    env_file += f'{tag}=\"{sub_value}\"\\n'\n\n        for key, value in add_models:\n            env_file += Settings._recursive_generate_env(\n                value, f\"{prefix}{key.upper()}{delimiter}\", delimiter\n            )\n        return env_file\n</code></pre>"},{"location":"reference/guidellm/config/#guidellm.config.Settings.generate_env_file","title":"<code>generate_env_file()</code>","text":"<p>Generate the .env file from the current settings</p> Source code in <code>src/guidellm/config.py</code> <pre><code>def generate_env_file(self) -&gt; str:\n    \"\"\"\n    Generate the .env file from the current settings\n    \"\"\"\n    return Settings._recursive_generate_env(\n        self,\n        self.model_config[\"env_prefix\"],  # type: ignore  # noqa: PGH003\n        self.model_config[\"env_nested_delimiter\"],  # type: ignore  # noqa: PGH003\n    )\n</code></pre>"},{"location":"reference/guidellm/config/#guidellm.config.print_config","title":"<code>print_config()</code>","text":"<p>Print the current configuration settings</p> Source code in <code>src/guidellm/config.py</code> <pre><code>def print_config():\n    \"\"\"\n    Print the current configuration settings\n    \"\"\"\n    print(f\"Settings: \\n{settings.generate_env_file()}\")  # noqa: T201\n</code></pre>"},{"location":"reference/guidellm/config/#guidellm.config.reload_settings","title":"<code>reload_settings()</code>","text":"<p>Reload the settings from the environment variables</p> Source code in <code>src/guidellm/config.py</code> <pre><code>def reload_settings():\n    \"\"\"\n    Reload the settings from the environment variables\n    \"\"\"\n    new_settings = Settings()\n    settings.__dict__.update(new_settings.__dict__)\n</code></pre>"},{"location":"reference/guidellm/logger/","title":"guidellm.logger","text":"<p>Logger configuration for GuideLLM.</p> <p>This module provides a flexible logging configuration using the loguru library. It supports console and file logging with options to configure via environment variables or direct function calls.</p> <p>Environment Variables:     - GUIDELLM__LOGGING__DISABLED: Disable logging (default: false).     - GUIDELLM__LOGGING__CLEAR_LOGGERS: Clear existing loggers         from loguru (default: true).     - GUIDELLM__LOGGING__LOG_LEVEL: Log level for console logging         (default: none, options: DEBUG, INFO, WARNING, ERROR, CRITICAL).     - GUIDELLM__LOGGING__FILE: Path to the log file for file logging         (default: guidellm.log if log file level set else none)     - GUIDELLM__LOGGING__FILE_LEVEL: Log level for file logging         (default: INFO if log file set else none).</p> <p>Usage:     from guidellm import logger, configure_logger, LoggerConfig</p> <pre><code># Configure metrics with default settings\nconfigure_logger(\n    config=LoggingConfig\n        disabled=False,\n        clear_loggers=True,\n        console_log_level=\"DEBUG\",\n        log_file=None,\n        log_file_level=None,\n    )\n)\n\nlogger.debug(\"This is a debug message\")\nlogger.info(\"This is an info message\")\n</code></pre>"},{"location":"reference/guidellm/logger/#guidellm.logger.configure_logger","title":"<code>configure_logger(config=settings.logging)</code>","text":"<p>Configure the metrics for LLM Compressor. This function sets up the console and file logging as per the specified or default parameters.</p> <p>Note: Environment variables take precedence over the function parameters.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>LoggingSettings</code> <p>The configuration for the logger to use.</p> <code>logging</code> Source code in <code>src/guidellm/logger.py</code> <pre><code>def configure_logger(config: LoggingSettings = settings.logging):\n    \"\"\"\n    Configure the metrics for LLM Compressor.\n    This function sets up the console and file logging\n    as per the specified or default parameters.\n\n    Note: Environment variables take precedence over the function parameters.\n\n    :param config: The configuration for the logger to use.\n    :type config: LoggerConfig\n    \"\"\"\n\n    if config.disabled:\n        logger.disable(\"guidellm\")\n        return\n\n    logger.enable(\"guidellm\")\n\n    if config.clear_loggers:\n        logger.remove()\n\n    # log as a human readable string with the time, function, level, and message\n    logger.add(\n        sys.stdout,\n        level=config.console_log_level.upper(),\n        format=\"{time} | {function} | {level} - {message}\",\n    )\n\n    if config.log_file or config.log_file_level:\n        log_file = config.log_file or \"guidellm.log\"\n        log_file_level = config.log_file_level or \"INFO\"\n        # log as json to the file for easier parsing\n        logger.add(log_file, level=log_file_level.upper(), serialize=True)\n</code></pre>"},{"location":"reference/guidellm/backend/","title":"guidellm.backend","text":""},{"location":"reference/guidellm/backend/#guidellm.backend.Backend","title":"<code>Backend</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for generative AI backends.</p> <p>This class provides a common interface for creating and interacting with different generative AI backends. Subclasses should implement the abstract methods to define specific backend behavior.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <code>BackendType</code> <p>The type of the backend.</p> required <p>Attributes:</p> Name Type Description <code>_registry</code> <code>dict[BackendType, type[Backend]]</code> <p>A registration dictionary that maps BackendType to backend classes.</p> Source code in <code>src/guidellm/backend/backend.py</code> <pre><code>class Backend(ABC):\n    \"\"\"\n    Abstract base class for generative AI backends.\n\n    This class provides a common interface for creating and interacting with different\n    generative AI backends. Subclasses should implement the abstract methods to\n    define specific backend behavior.\n\n    :cvar _registry: A registration dictionary that maps BackendType to backend classes.\n    :param type_: The type of the backend.\n    \"\"\"\n\n    _registry: dict[BackendType, \"type[Backend]\"] = {}\n\n    @classmethod\n    def register(cls, backend_type: BackendType):\n        \"\"\"\n        A decorator to register a backend class in the backend registry.\n\n        :param backend_type: The type of backend to register.\n        :type backend_type: BackendType\n        :return: The decorated backend class.\n        :rtype: Type[Backend]\n        \"\"\"\n        if backend_type in cls._registry:\n            raise ValueError(f\"Backend type already registered: {backend_type}\")\n\n        if not issubclass(cls, Backend):\n            raise TypeError(\"Only subclasses of Backend can be registered\")\n\n        def inner_wrapper(wrapped_class: type[\"Backend\"]):\n            cls._registry[backend_type] = wrapped_class\n            logger.info(\"Registered backend type: {}\", backend_type)\n            return wrapped_class\n\n        return inner_wrapper\n\n    @classmethod\n    def create(cls, type_: BackendType, **kwargs) -&gt; \"Backend\":\n        \"\"\"\n        Factory method to create a backend instance based on the backend type.\n\n        :param type_: The type of backend to create.\n        :type type_: BackendType\n        :param kwargs: Additional arguments for backend initialization.\n        :return: An instance of a subclass of Backend.\n        :rtype: Backend\n        :raises ValueError: If the backend type is not registered.\n        \"\"\"\n\n        logger.info(\"Creating backend of type {}\", type_)\n\n        if type_ not in cls._registry:\n            err = ValueError(f\"Unsupported backend type: {type_}\")\n            logger.error(\"{}\", err)\n            raise err\n\n        return Backend._registry[type_](**kwargs)\n\n    def __init__(self, type_: BackendType):\n        self._type = type_\n\n    @property\n    def type_(self) -&gt; BackendType:\n        \"\"\"\n        :return: The type of the backend.\n        \"\"\"\n        return self._type\n\n    @property\n    @abstractmethod\n    def target(self) -&gt; str:\n        \"\"\"\n        :return: The target location for the backend.\n        \"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def model(self) -&gt; Optional[str]:\n        \"\"\"\n        :return: The model used for the backend requests.\n        \"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def info(self) -&gt; dict[str, Any]:\n        \"\"\"\n        :return: The information about the backend.\n        \"\"\"\n        ...\n\n    async def validate(self):\n        \"\"\"\n        Handle final setup and validate the backend is ready for use.\n        If not successful, raises the appropriate exception.\n        \"\"\"\n        logger.info(\"{} validating backend {}\", self.__class__.__name__, self.type_)\n        await self.check_setup()\n        models = await self.available_models()\n        if not models:\n            raise ValueError(\"No models available for the backend\")\n\n        async for _ in self.text_completions(\n            prompt=\"Test connection\", output_token_count=1\n        ):  # type: ignore[attr-defined]\n            pass\n\n    @abstractmethod\n    async def check_setup(self):\n        \"\"\"\n        Check the setup for the backend.\n        If unsuccessful, raises the appropriate exception.\n\n        :raises ValueError: If the setup check fails.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    async def prepare_multiprocessing(self):\n        \"\"\"\n        Prepare the backend for use in a multiprocessing environment.\n        This is useful for backends that have instance state that can not\n        be shared across processes and should be cleared out and re-initialized\n        for each new process.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    async def available_models(self) -&gt; list[str]:\n        \"\"\"\n        Get the list of available models for the backend.\n\n        :return: The list of available models.\n        :rtype: List[str]\n        \"\"\"\n        ...\n\n    @abstractmethod\n    async def text_completions(\n        self,\n        prompt: Union[str, list[str]],\n        request_id: Optional[str] = None,\n        prompt_token_count: Optional[int] = None,\n        output_token_count: Optional[int] = None,\n        **kwargs,\n    ) -&gt; AsyncGenerator[Union[StreamingTextResponse, ResponseSummary], None]:\n        \"\"\"\n        Generate text only completions for the given prompt.\n        Does not support multiple modalities, complicated chat interfaces,\n        or chat templates. Specifically, it requests with only the prompt.\n\n        :param prompt: The prompt (or list of prompts) to generate a completion for.\n            If a list is supplied, these are concatenated and run through the model\n            for a single prompt.\n        :param request_id: The unique identifier for the request, if any.\n            Added to logging statements and the response for tracking purposes.\n        :param prompt_token_count: The number of tokens measured in the prompt, if any.\n            Returned in the response stats for later analysis, if applicable.\n        :param output_token_count: If supplied, the number of tokens to enforce\n            generation of for the output for this request.\n        :param kwargs: Additional keyword arguments to pass with the request.\n        :return: An async generator that yields a StreamingTextResponse for start,\n            a StreamingTextResponse for each received iteration,\n            and a ResponseSummary for the final response.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    async def chat_completions(\n        self,\n        content: Union[\n            str,\n            list[Union[str, dict[str, Union[str, dict[str, str]]], Path, Image.Image]],\n            Any,\n        ],\n        request_id: Optional[str] = None,\n        prompt_token_count: Optional[int] = None,\n        output_token_count: Optional[int] = None,\n        raw_content: bool = False,\n        **kwargs,\n    ) -&gt; AsyncGenerator[Union[StreamingTextResponse, ResponseSummary], None]:\n        \"\"\"\n        Generate chat completions for the given content.\n        Supports multiple modalities, complicated chat interfaces, and chat templates.\n        Specifically, it requests with the content, which can be any combination of\n        text, images, and audio provided the target model supports it,\n        and returns the output text. Additionally, any chat templates\n        for the model are applied within the backend.\n\n        :param content: The content (or list of content) to generate a completion for.\n            This supports any combination of text, images, and audio (model dependent).\n            Supported text only request examples:\n                content=\"Sample prompt\", content=[\"Sample prompt\", \"Second prompt\"],\n                content=[{\"type\": \"text\", \"value\": \"Sample prompt\"}.\n            Supported text and image request examples:\n                content=[\"Describe the image\", PIL.Image.open(\"image.jpg\")],\n                content=[\"Describe the image\", Path(\"image.jpg\")],\n                content=[\"Describe the image\", {\"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}].\n            Supported text and audio request examples:\n                content=[\"Transcribe the audio\", Path(\"audio.wav\")],\n                content=[\"Transcribe the audio\", {\"type\": \"input_audio\",\n                \"input_audio\": {\"data\": f\"{base64_bytes}\", \"format\": \"wav}].\n            Additionally, if raw_content=True then the content is passed directly to the\n            backend without any processing.\n        :param request_id: The unique identifier for the request, if any.\n            Added to logging statements and the response for tracking purposes.\n        :param prompt_token_count: The number of tokens measured in the prompt, if any.\n            Returned in the response stats for later analysis, if applicable.\n        :param output_token_count: If supplied, the number of tokens to enforce\n            generation of for the output for this request.\n        :param kwargs: Additional keyword arguments to pass with the request.\n        :return: An async generator that yields a StreamingTextResponse for start,\n            a StreamingTextResponse for each received iteration,\n            and a ResponseSummary for the final response.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/guidellm/backend/#guidellm.backend.Backend.info","title":"<code>info</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The information about the backend.</p>"},{"location":"reference/guidellm/backend/#guidellm.backend.Backend.model","title":"<code>model</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns:</p> Type Description <code>Optional[str]</code> <p>The model used for the backend requests.</p>"},{"location":"reference/guidellm/backend/#guidellm.backend.Backend.target","title":"<code>target</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns:</p> Type Description <code>str</code> <p>The target location for the backend.</p>"},{"location":"reference/guidellm/backend/#guidellm.backend.Backend.type_","title":"<code>type_</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>BackendType</code> <p>The type of the backend.</p>"},{"location":"reference/guidellm/backend/#guidellm.backend.Backend.available_models","title":"<code>available_models()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Get the list of available models for the backend.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>The list of available models.</p> Source code in <code>src/guidellm/backend/backend.py</code> <pre><code>@abstractmethod\nasync def available_models(self) -&gt; list[str]:\n    \"\"\"\n    Get the list of available models for the backend.\n\n    :return: The list of available models.\n    :rtype: List[str]\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/guidellm/backend/#guidellm.backend.Backend.chat_completions","title":"<code>chat_completions(content, request_id=None, prompt_token_count=None, output_token_count=None, raw_content=False, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Generate chat completions for the given content. Supports multiple modalities, complicated chat interfaces, and chat templates. Specifically, it requests with the content, which can be any combination of text, images, and audio provided the target model supports it, and returns the output text. Additionally, any chat templates for the model are applied within the backend.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>Union[str, list[Union[str, dict[str, Union[str, dict[str, str]]], Path, Image]], Any]</code> <p>The content (or list of content) to generate a completion for. This supports any combination of text, images, and audio (model dependent). Supported text only request examples: content=\"Sample prompt\", content=[\"Sample prompt\", \"Second prompt\"], content=[{\"type\": \"text\", \"value\": \"Sample prompt\"}. Supported text and image request examples: content=[\"Describe the image\", PIL.Image.open(\"image.jpg\")], content=[\"Describe the image\", Path(\"image.jpg\")], content=[\"Describe the image\", {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}]. Supported text and audio request examples: content=[\"Transcribe the audio\", Path(\"audio.wav\")], content=[\"Transcribe the audio\", {\"type\": \"input_audio\", \"input_audio\": {\"data\": f\"{base64_bytes}\", \"format\": \"wav}]. Additionally, if raw_content=True then the content is passed directly to the backend without any processing.</p> required <code>request_id</code> <code>Optional[str]</code> <p>The unique identifier for the request, if any. Added to logging statements and the response for tracking purposes.</p> <code>None</code> <code>prompt_token_count</code> <code>Optional[int]</code> <p>The number of tokens measured in the prompt, if any. Returned in the response stats for later analysis, if applicable.</p> <code>None</code> <code>output_token_count</code> <code>Optional[int]</code> <p>If supplied, the number of tokens to enforce generation of for the output for this request.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass with the request.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncGenerator[Union[StreamingTextResponse, ResponseSummary], None]</code> <p>An async generator that yields a StreamingTextResponse for start, a StreamingTextResponse for each received iteration, and a ResponseSummary for the final response.</p> Source code in <code>src/guidellm/backend/backend.py</code> <pre><code>@abstractmethod\nasync def chat_completions(\n    self,\n    content: Union[\n        str,\n        list[Union[str, dict[str, Union[str, dict[str, str]]], Path, Image.Image]],\n        Any,\n    ],\n    request_id: Optional[str] = None,\n    prompt_token_count: Optional[int] = None,\n    output_token_count: Optional[int] = None,\n    raw_content: bool = False,\n    **kwargs,\n) -&gt; AsyncGenerator[Union[StreamingTextResponse, ResponseSummary], None]:\n    \"\"\"\n    Generate chat completions for the given content.\n    Supports multiple modalities, complicated chat interfaces, and chat templates.\n    Specifically, it requests with the content, which can be any combination of\n    text, images, and audio provided the target model supports it,\n    and returns the output text. Additionally, any chat templates\n    for the model are applied within the backend.\n\n    :param content: The content (or list of content) to generate a completion for.\n        This supports any combination of text, images, and audio (model dependent).\n        Supported text only request examples:\n            content=\"Sample prompt\", content=[\"Sample prompt\", \"Second prompt\"],\n            content=[{\"type\": \"text\", \"value\": \"Sample prompt\"}.\n        Supported text and image request examples:\n            content=[\"Describe the image\", PIL.Image.open(\"image.jpg\")],\n            content=[\"Describe the image\", Path(\"image.jpg\")],\n            content=[\"Describe the image\", {\"type\": \"image_url\",\n                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}].\n        Supported text and audio request examples:\n            content=[\"Transcribe the audio\", Path(\"audio.wav\")],\n            content=[\"Transcribe the audio\", {\"type\": \"input_audio\",\n            \"input_audio\": {\"data\": f\"{base64_bytes}\", \"format\": \"wav}].\n        Additionally, if raw_content=True then the content is passed directly to the\n        backend without any processing.\n    :param request_id: The unique identifier for the request, if any.\n        Added to logging statements and the response for tracking purposes.\n    :param prompt_token_count: The number of tokens measured in the prompt, if any.\n        Returned in the response stats for later analysis, if applicable.\n    :param output_token_count: If supplied, the number of tokens to enforce\n        generation of for the output for this request.\n    :param kwargs: Additional keyword arguments to pass with the request.\n    :return: An async generator that yields a StreamingTextResponse for start,\n        a StreamingTextResponse for each received iteration,\n        and a ResponseSummary for the final response.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/guidellm/backend/#guidellm.backend.Backend.check_setup","title":"<code>check_setup()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Check the setup for the backend. If unsuccessful, raises the appropriate exception.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the setup check fails.</p> Source code in <code>src/guidellm/backend/backend.py</code> <pre><code>@abstractmethod\nasync def check_setup(self):\n    \"\"\"\n    Check the setup for the backend.\n    If unsuccessful, raises the appropriate exception.\n\n    :raises ValueError: If the setup check fails.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/guidellm/backend/#guidellm.backend.Backend.create","title":"<code>create(type_, **kwargs)</code>  <code>classmethod</code>","text":"<p>Factory method to create a backend instance based on the backend type.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <code>BackendType</code> <p>The type of backend to create.</p> required <code>kwargs</code> <p>Additional arguments for backend initialization.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Backend</code> <p>An instance of a subclass of Backend.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the backend type is not registered.</p> Source code in <code>src/guidellm/backend/backend.py</code> <pre><code>@classmethod\ndef create(cls, type_: BackendType, **kwargs) -&gt; \"Backend\":\n    \"\"\"\n    Factory method to create a backend instance based on the backend type.\n\n    :param type_: The type of backend to create.\n    :type type_: BackendType\n    :param kwargs: Additional arguments for backend initialization.\n    :return: An instance of a subclass of Backend.\n    :rtype: Backend\n    :raises ValueError: If the backend type is not registered.\n    \"\"\"\n\n    logger.info(\"Creating backend of type {}\", type_)\n\n    if type_ not in cls._registry:\n        err = ValueError(f\"Unsupported backend type: {type_}\")\n        logger.error(\"{}\", err)\n        raise err\n\n    return Backend._registry[type_](**kwargs)\n</code></pre>"},{"location":"reference/guidellm/backend/#guidellm.backend.Backend.prepare_multiprocessing","title":"<code>prepare_multiprocessing()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Prepare the backend for use in a multiprocessing environment. This is useful for backends that have instance state that can not be shared across processes and should be cleared out and re-initialized for each new process.</p> Source code in <code>src/guidellm/backend/backend.py</code> <pre><code>@abstractmethod\nasync def prepare_multiprocessing(self):\n    \"\"\"\n    Prepare the backend for use in a multiprocessing environment.\n    This is useful for backends that have instance state that can not\n    be shared across processes and should be cleared out and re-initialized\n    for each new process.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/guidellm/backend/#guidellm.backend.Backend.register","title":"<code>register(backend_type)</code>  <code>classmethod</code>","text":"<p>A decorator to register a backend class in the backend registry.</p> <p>Parameters:</p> Name Type Description Default <code>backend_type</code> <code>BackendType</code> <p>The type of backend to register.</p> required <p>Returns:</p> Type Description <code>Type[Backend]</code> <p>The decorated backend class.</p> Source code in <code>src/guidellm/backend/backend.py</code> <pre><code>@classmethod\ndef register(cls, backend_type: BackendType):\n    \"\"\"\n    A decorator to register a backend class in the backend registry.\n\n    :param backend_type: The type of backend to register.\n    :type backend_type: BackendType\n    :return: The decorated backend class.\n    :rtype: Type[Backend]\n    \"\"\"\n    if backend_type in cls._registry:\n        raise ValueError(f\"Backend type already registered: {backend_type}\")\n\n    if not issubclass(cls, Backend):\n        raise TypeError(\"Only subclasses of Backend can be registered\")\n\n    def inner_wrapper(wrapped_class: type[\"Backend\"]):\n        cls._registry[backend_type] = wrapped_class\n        logger.info(\"Registered backend type: {}\", backend_type)\n        return wrapped_class\n\n    return inner_wrapper\n</code></pre>"},{"location":"reference/guidellm/backend/#guidellm.backend.Backend.text_completions","title":"<code>text_completions(prompt, request_id=None, prompt_token_count=None, output_token_count=None, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Generate text only completions for the given prompt. Does not support multiple modalities, complicated chat interfaces, or chat templates. Specifically, it requests with only the prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Union[str, list[str]]</code> <p>The prompt (or list of prompts) to generate a completion for. If a list is supplied, these are concatenated and run through the model for a single prompt.</p> required <code>request_id</code> <code>Optional[str]</code> <p>The unique identifier for the request, if any. Added to logging statements and the response for tracking purposes.</p> <code>None</code> <code>prompt_token_count</code> <code>Optional[int]</code> <p>The number of tokens measured in the prompt, if any. Returned in the response stats for later analysis, if applicable.</p> <code>None</code> <code>output_token_count</code> <code>Optional[int]</code> <p>If supplied, the number of tokens to enforce generation of for the output for this request.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass with the request.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncGenerator[Union[StreamingTextResponse, ResponseSummary], None]</code> <p>An async generator that yields a StreamingTextResponse for start, a StreamingTextResponse for each received iteration, and a ResponseSummary for the final response.</p> Source code in <code>src/guidellm/backend/backend.py</code> <pre><code>@abstractmethod\nasync def text_completions(\n    self,\n    prompt: Union[str, list[str]],\n    request_id: Optional[str] = None,\n    prompt_token_count: Optional[int] = None,\n    output_token_count: Optional[int] = None,\n    **kwargs,\n) -&gt; AsyncGenerator[Union[StreamingTextResponse, ResponseSummary], None]:\n    \"\"\"\n    Generate text only completions for the given prompt.\n    Does not support multiple modalities, complicated chat interfaces,\n    or chat templates. Specifically, it requests with only the prompt.\n\n    :param prompt: The prompt (or list of prompts) to generate a completion for.\n        If a list is supplied, these are concatenated and run through the model\n        for a single prompt.\n    :param request_id: The unique identifier for the request, if any.\n        Added to logging statements and the response for tracking purposes.\n    :param prompt_token_count: The number of tokens measured in the prompt, if any.\n        Returned in the response stats for later analysis, if applicable.\n    :param output_token_count: If supplied, the number of tokens to enforce\n        generation of for the output for this request.\n    :param kwargs: Additional keyword arguments to pass with the request.\n    :return: An async generator that yields a StreamingTextResponse for start,\n        a StreamingTextResponse for each received iteration,\n        and a ResponseSummary for the final response.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/guidellm/backend/#guidellm.backend.Backend.validate","title":"<code>validate()</code>  <code>async</code>","text":"<p>Handle final setup and validate the backend is ready for use. If not successful, raises the appropriate exception.</p> Source code in <code>src/guidellm/backend/backend.py</code> <pre><code>async def validate(self):\n    \"\"\"\n    Handle final setup and validate the backend is ready for use.\n    If not successful, raises the appropriate exception.\n    \"\"\"\n    logger.info(\"{} validating backend {}\", self.__class__.__name__, self.type_)\n    await self.check_setup()\n    models = await self.available_models()\n    if not models:\n        raise ValueError(\"No models available for the backend\")\n\n    async for _ in self.text_completions(\n        prompt=\"Test connection\", output_token_count=1\n    ):  # type: ignore[attr-defined]\n        pass\n</code></pre>"},{"location":"reference/guidellm/backend/#guidellm.backend.OpenAIHTTPBackend","title":"<code>OpenAIHTTPBackend</code>","text":"<p>               Bases: <code>Backend</code></p> <p>A HTTP-based backend implementation for requests to an OpenAI compatible server. For example, a vLLM server instance or requests to OpenAI's API.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Optional[str]</code> <p>The target URL string for the OpenAI server. ex: http://0.0.0.0:8000</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>The model to use for all requests on the target server. If none is provided, the first available model will be used.</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>The API key to use for requests to the OpenAI server. If provided, adds an Authorization header with the value \"Authorization: Bearer {api_key}\". If not provided, no Authorization header is added.</p> <code>None</code> <code>organization</code> <code>Optional[str]</code> <p>The organization to use for requests to the OpenAI server. For example, if set to \"org_123\", adds an OpenAI-Organization header with the value \"OpenAI-Organization: org_123\". If not provided, no OpenAI-Organization header is added.</p> <code>None</code> <code>project</code> <code>Optional[str]</code> <p>The project to use for requests to the OpenAI server. For example, if set to \"project_123\", adds an OpenAI-Project header with the value \"OpenAI-Project: project_123\". If not provided, no OpenAI-Project header is added.</p> <code>None</code> <code>timeout</code> <code>Optional[float]</code> <p>The timeout to use for requests to the OpenAI server. If not provided, the default timeout provided from settings is used.</p> <code>None</code> <code>http2</code> <code>Optional[bool]</code> <p>If True, uses HTTP/2 for requests to the OpenAI server. Defaults to True.</p> <code>True</code> <code>follow_redirects</code> <code>Optional[bool]</code> <p>If True, the HTTP client will follow redirect responses. If not provided, the default value from settings is used.</p> <code>None</code> <code>max_output_tokens</code> <code>Optional[int]</code> <p>The maximum number of tokens to request for completions. If not provided, the default maximum tokens provided from settings is used.</p> <code>None</code> <code>extra_query</code> <code>Optional[dict]</code> <p>Query parameters to include in requests to the OpenAI server. If \"chat_completions\", \"models\", or \"text_completions\" are included as keys, the values of these keys will be used as the parameters for the respective endpoint. If not provided, no extra query parameters are added.</p> <code>None</code> Source code in <code>src/guidellm/backend/openai.py</code> <pre><code>@Backend.register(\"openai_http\")\nclass OpenAIHTTPBackend(Backend):\n    \"\"\"\n    A HTTP-based backend implementation for requests to an OpenAI compatible server.\n    For example, a vLLM server instance or requests to OpenAI's API.\n\n    :param target: The target URL string for the OpenAI server. ex: http://0.0.0.0:8000\n    :param model: The model to use for all requests on the target server.\n        If none is provided, the first available model will be used.\n    :param api_key: The API key to use for requests to the OpenAI server.\n        If provided, adds an Authorization header with the value\n        \"Authorization: Bearer {api_key}\".\n        If not provided, no Authorization header is added.\n    :param organization: The organization to use for requests to the OpenAI server.\n        For example, if set to \"org_123\", adds an OpenAI-Organization header with the\n        value \"OpenAI-Organization: org_123\".\n        If not provided, no OpenAI-Organization header is added.\n    :param project: The project to use for requests to the OpenAI server.\n        For example, if set to \"project_123\", adds an OpenAI-Project header with the\n        value \"OpenAI-Project: project_123\".\n        If not provided, no OpenAI-Project header is added.\n    :param timeout: The timeout to use for requests to the OpenAI server.\n        If not provided, the default timeout provided from settings is used.\n    :param http2: If True, uses HTTP/2 for requests to the OpenAI server.\n        Defaults to True.\n    :param follow_redirects: If True, the HTTP client will follow redirect responses.\n        If not provided, the default value from settings is used.\n    :param max_output_tokens: The maximum number of tokens to request for completions.\n        If not provided, the default maximum tokens provided from settings is used.\n    :param extra_query: Query parameters to include in requests to the OpenAI server.\n        If \"chat_completions\", \"models\", or \"text_completions\" are included as keys,\n        the values of these keys will be used as the parameters for the respective\n        endpoint.\n        If not provided, no extra query parameters are added.\n    \"\"\"\n\n    def __init__(\n        self,\n        target: Optional[str] = None,\n        model: Optional[str] = None,\n        api_key: Optional[str] = None,\n        organization: Optional[str] = None,\n        project: Optional[str] = None,\n        timeout: Optional[float] = None,\n        http2: Optional[bool] = True,\n        follow_redirects: Optional[bool] = None,\n        max_output_tokens: Optional[int] = None,\n        extra_query: Optional[dict] = None,\n        extra_body: Optional[dict] = None,\n    ):\n        super().__init__(type_=\"openai_http\")\n        self._target = target or settings.openai.base_url\n\n        if not self._target:\n            raise ValueError(\"Target URL must be provided for OpenAI HTTP backend.\")\n\n        if self._target.endswith(\"/v1\") or self._target.endswith(\"/v1/\"):\n            # backwards compatability, strip v1 off\n            self._target = self._target[:-3]\n\n        if self._target.endswith(\"/\"):\n            self._target = self._target[:-1]\n\n        self._model = model\n\n        api_key = api_key or settings.openai.api_key\n        self.authorization = (\n            f\"Bearer {api_key}\" if api_key else settings.openai.bearer_token\n        )\n\n        self.organization = organization or settings.openai.organization\n        self.project = project or settings.openai.project\n        self.timeout = timeout if timeout is not None else settings.request_timeout\n        self.http2 = http2 if http2 is not None else settings.request_http2\n        self.follow_redirects = (\n            follow_redirects\n            if follow_redirects is not None\n            else settings.request_follow_redirects\n        )\n        self.max_output_tokens = (\n            max_output_tokens\n            if max_output_tokens is not None\n            else settings.openai.max_output_tokens\n        )\n        self.extra_query = extra_query\n        self.extra_body = extra_body\n        self._async_client: Optional[httpx.AsyncClient] = None\n\n    @property\n    def target(self) -&gt; str:\n        \"\"\"\n        :return: The target URL string for the OpenAI server.\n        \"\"\"\n        return self._target\n\n    @property\n    def model(self) -&gt; Optional[str]:\n        \"\"\"\n        :return: The model to use for all requests on the target server.\n            If validate hasn't been called yet and no model was passed in,\n            this will be None until validate is called to set the default.\n        \"\"\"\n        return self._model\n\n    @property\n    def info(self) -&gt; dict[str, Any]:\n        \"\"\"\n        :return: The information about the backend.\n        \"\"\"\n        return {\n            \"max_output_tokens\": self.max_output_tokens,\n            \"timeout\": self.timeout,\n            \"http2\": self.http2,\n            \"follow_redirects\": self.follow_redirects,\n            \"authorization\": bool(self.authorization),\n            \"organization\": self.organization,\n            \"project\": self.project,\n            \"text_completions_path\": TEXT_COMPLETIONS_PATH,\n            \"chat_completions_path\": CHAT_COMPLETIONS_PATH,\n        }\n\n    async def check_setup(self):\n        \"\"\"\n        Check if the backend is setup correctly and can be used for requests.\n        Specifically, if a model is not provided, it grabs the first available model.\n        If no models are available, raises a ValueError.\n        If a model is provided and not available, raises a ValueError.\n\n        :raises ValueError: If no models or the provided model is not available.\n        \"\"\"\n        models = await self.available_models()\n        if not models:\n            raise ValueError(f\"No models available for target: {self.target}\")\n\n        if not self.model:\n            self._model = models[0]\n        elif self.model not in models:\n            raise ValueError(\n                f\"Model {self.model} not found in available models:\"\n                f\"{models} for target: {self.target}\"\n            )\n\n    async def prepare_multiprocessing(self):\n        \"\"\"\n        Prepare the backend for use in a multiprocessing environment.\n        Clears out the sync and async clients to ensure they are re-initialized\n        for each process.\n        \"\"\"\n        if self._async_client is not None:\n            await self._async_client.aclose()\n            self._async_client = None\n\n    async def available_models(self) -&gt; list[str]:\n        \"\"\"\n        Get the available models for the target server using the OpenAI models endpoint:\n        /v1/models\n        \"\"\"\n        target = f\"{self.target}/v1/models\"\n        headers = self._headers()\n        params = self._params(MODELS)\n        response = await self._get_async_client().get(\n            target, headers=headers, params=params\n        )\n        response.raise_for_status()\n\n        models = []\n\n        for item in response.json()[\"data\"]:\n            models.append(item[\"id\"])\n\n        return models\n\n    async def text_completions(  # type: ignore[override]\n        self,\n        prompt: Union[str, list[str]],\n        request_id: Optional[str] = None,\n        prompt_token_count: Optional[int] = None,\n        output_token_count: Optional[int] = None,\n        **kwargs,\n    ) -&gt; AsyncGenerator[Union[StreamingTextResponse, ResponseSummary], None]:\n        \"\"\"\n        Generate text completions for the given prompt using the OpenAI\n        completions endpoint: /v1/completions.\n\n        :param prompt: The prompt (or list of prompts) to generate a completion for.\n            If a list is supplied, these are concatenated and run through the model\n            for a single prompt.\n        :param request_id: The unique identifier for the request, if any.\n            Added to logging statements and the response for tracking purposes.\n        :param prompt_token_count: The number of tokens measured in the prompt, if any.\n            Returned in the response stats for later analysis, if applicable.\n        :param output_token_count: If supplied, the number of tokens to enforce\n            generation of for the output for this request.\n        :param kwargs: Additional keyword arguments to pass with the request.\n        :return: An async generator that yields a StreamingTextResponse for start,\n            a StreamingTextResponse for each received iteration,\n            and a ResponseSummary for the final response.\n        \"\"\"\n        logger.debug(\"{} invocation with args: {}\", self.__class__.__name__, locals())\n\n        if isinstance(prompt, list):\n            raise ValueError(\n                \"List prompts (batching) is currently not supported for \"\n                f\"text_completions OpenAI pathways. Received: {prompt}\"\n            )\n\n        headers = self._headers()\n        params = self._params(TEXT_COMPLETIONS)\n        body = self._body(TEXT_COMPLETIONS)\n        payload = self._completions_payload(\n            body=body,\n            orig_kwargs=kwargs,\n            max_output_tokens=output_token_count,\n            prompt=prompt,\n        )\n\n        try:\n            async for resp in self._iterative_completions_request(\n                type_=\"text_completions\",\n                request_id=request_id,\n                request_prompt_tokens=prompt_token_count,\n                request_output_tokens=output_token_count,\n                headers=headers,\n                params=params,\n                payload=payload,\n            ):\n                yield resp\n        except Exception as ex:\n            logger.error(\n                \"{} request with headers: {} and params: {} and payload: {} failed: {}\",\n                self.__class__.__name__,\n                headers,\n                params,\n                payload,\n                ex,\n            )\n            raise ex\n\n    async def chat_completions(  # type: ignore[override]\n        self,\n        content: Union[\n            str,\n            list[Union[str, dict[str, Union[str, dict[str, str]]], Path, Image.Image]],\n            Any,\n        ],\n        request_id: Optional[str] = None,\n        prompt_token_count: Optional[int] = None,\n        output_token_count: Optional[int] = None,\n        raw_content: bool = False,\n        **kwargs,\n    ) -&gt; AsyncGenerator[Union[StreamingTextResponse, ResponseSummary], None]:\n        \"\"\"\n        Generate chat completions for the given content using the OpenAI\n        chat completions endpoint: /v1/chat/completions.\n\n        :param content: The content (or list of content) to generate a completion for.\n            This supports any combination of text, images, and audio (model dependent).\n            Supported text only request examples:\n                content=\"Sample prompt\", content=[\"Sample prompt\", \"Second prompt\"],\n                content=[{\"type\": \"text\", \"value\": \"Sample prompt\"}.\n            Supported text and image request examples:\n                content=[\"Describe the image\", PIL.Image.open(\"image.jpg\")],\n                content=[\"Describe the image\", Path(\"image.jpg\")],\n                content=[\"Describe the image\", {\"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}].\n            Supported text and audio request examples:\n                content=[\"Transcribe the audio\", Path(\"audio.wav\")],\n                content=[\"Transcribe the audio\", {\"type\": \"input_audio\",\n                \"input_audio\": {\"data\": f\"{base64_bytes}\", \"format\": \"wav}].\n            Additionally, if raw_content=True then the content is passed directly to the\n            backend without any processing.\n        :param request_id: The unique identifier for the request, if any.\n            Added to logging statements and the response for tracking purposes.\n        :param prompt_token_count: The number of tokens measured in the prompt, if any.\n            Returned in the response stats for later analysis, if applicable.\n        :param output_token_count: If supplied, the number of tokens to enforce\n            generation of for the output for this request.\n        :param kwargs: Additional keyword arguments to pass with the request.\n        :return: An async generator that yields a StreamingTextResponse for start,\n            a StreamingTextResponse for each received iteration,\n            and a ResponseSummary for the final response.\n        \"\"\"\n        logger.debug(\"{} invocation with args: {}\", self.__class__.__name__, locals())\n        headers = self._headers()\n        params = self._params(CHAT_COMPLETIONS)\n        body = self._body(CHAT_COMPLETIONS)\n        messages = (\n            content if raw_content else self._create_chat_messages(content=content)\n        )\n        payload = self._completions_payload(\n            body=body,\n            orig_kwargs=kwargs,\n            max_output_tokens=output_token_count,\n            messages=messages,\n        )\n\n        try:\n            async for resp in self._iterative_completions_request(\n                type_=\"chat_completions\",\n                request_id=request_id,\n                request_prompt_tokens=prompt_token_count,\n                request_output_tokens=output_token_count,\n                headers=headers,\n                params=params,\n                payload=payload,\n            ):\n                yield resp\n        except Exception as ex:\n            logger.error(\n                \"{} request with headers: {} and params: {} and payload: {} failed: {}\",\n                self.__class__.__name__,\n                headers,\n                params,\n                payload,\n                ex,\n            )\n            raise ex\n\n    def _get_async_client(self) -&gt; httpx.AsyncClient:\n        \"\"\"\n        Get the async HTTP client for making requests.\n        If the client has not been created yet, it will create one.\n\n        :return: The async HTTP client.\n        \"\"\"\n        if self._async_client is None:\n            client = httpx.AsyncClient(\n                http2=self.http2,\n                timeout=self.timeout,\n                follow_redirects=self.follow_redirects,\n            )\n            self._async_client = client\n        else:\n            client = self._async_client\n\n        return client\n\n    def _headers(self) -&gt; dict[str, str]:\n        headers = {\n            \"Content-Type\": \"application/json\",\n        }\n\n        if self.authorization:\n            headers[\"Authorization\"] = self.authorization\n\n        if self.organization:\n            headers[\"OpenAI-Organization\"] = self.organization\n\n        if self.project:\n            headers[\"OpenAI-Project\"] = self.project\n\n        return headers\n\n    def _params(self, endpoint_type: EndpointType) -&gt; dict[str, str]:\n        if self.extra_query is None:\n            return {}\n\n        if (\n            CHAT_COMPLETIONS in self.extra_query\n            or MODELS in self.extra_query\n            or TEXT_COMPLETIONS in self.extra_query\n        ):\n            return self.extra_query.get(endpoint_type, {})\n\n        return self.extra_query\n\n    def _body(self, endpoint_type: EndpointType) -&gt; dict[str, str]:\n        if self.extra_body is None:\n            return {}\n\n        if (\n            CHAT_COMPLETIONS in self.extra_body\n            or MODELS in self.extra_body\n            or TEXT_COMPLETIONS in self.extra_body\n        ):\n            return self.extra_body.get(endpoint_type, {})\n\n        return self.extra_body\n\n    def _completions_payload(\n        self,\n        body: Optional[dict],\n        orig_kwargs: Optional[dict],\n        max_output_tokens: Optional[int],\n        **kwargs,\n    ) -&gt; dict:\n        payload = body or {}\n        payload.update(orig_kwargs or {})\n        payload.update(kwargs)\n        payload[\"model\"] = self.model\n        payload[\"stream\"] = True\n        payload[\"stream_options\"] = {\n            \"include_usage\": True,\n        }\n\n        if max_output_tokens or self.max_output_tokens:\n            logger.debug(\n                \"{} adding payload args for setting output_token_count: {}\",\n                self.__class__.__name__,\n                max_output_tokens or self.max_output_tokens,\n            )\n            payload[\"max_tokens\"] = max_output_tokens or self.max_output_tokens\n            payload[\"max_completion_tokens\"] = payload[\"max_tokens\"]\n\n            if max_output_tokens:\n                # only set stop and ignore_eos if max_output_tokens set at request level\n                # otherwise the instance value is just the max to enforce we stay below\n                payload[\"stop\"] = None\n                payload[\"ignore_eos\"] = True\n\n        return payload\n\n    @staticmethod\n    def _create_chat_messages(\n        content: Union[\n            str,\n            list[Union[str, dict[str, Union[str, dict[str, str]]], Path, Image.Image]],\n            Any,\n        ],\n    ) -&gt; list[dict]:\n        if isinstance(content, str):\n            return [\n                {\n                    \"role\": \"user\",\n                    \"content\": content,\n                }\n            ]\n\n        if isinstance(content, list):\n            resolved_content = []\n\n            for item in content:\n                if isinstance(item, dict):\n                    resolved_content.append(item)\n                elif isinstance(item, str):\n                    resolved_content.append({\"type\": \"text\", \"text\": item})\n                elif isinstance(item, Image.Image) or (\n                    isinstance(item, Path) and item.suffix.lower() in [\".jpg\", \".jpeg\"]\n                ):\n                    image = item if isinstance(item, Image.Image) else Image.open(item)\n                    encoded = base64.b64encode(image.tobytes()).decode(\"utf-8\")\n                    resolved_content.append(\n                        {\n                            \"type\": \"image\",\n                            \"image\": {\n                                \"url\": f\"data:image/jpeg;base64,{encoded}\",\n                            },\n                        }\n                    )\n                elif isinstance(item, Path) and item.suffix.lower() in [\".wav\"]:\n                    encoded = base64.b64encode(item.read_bytes()).decode(\"utf-8\")\n                    resolved_content.append(\n                        {\n                            \"type\": \"input_audio\",\n                            \"input_audio\": {\n                                \"data\": f\"{encoded}\",\n                                \"format\": \"wav\",\n                            },\n                        }\n                    )\n                else:\n                    raise ValueError(\n                        f\"Unsupported content item type: {item} in list: {content}\"\n                    )\n\n            return [\n                {\n                    \"role\": \"user\",\n                    \"content\": resolved_content,\n                }\n            ]\n\n        raise ValueError(f\"Unsupported content type: {content}\")\n\n    async def _iterative_completions_request(\n        self,\n        type_: Literal[\"text_completions\", \"chat_completions\"],\n        request_id: Optional[str],\n        request_prompt_tokens: Optional[int],\n        request_output_tokens: Optional[int],\n        headers: dict[str, str],\n        params: dict[str, str],\n        payload: dict[str, Any],\n    ) -&gt; AsyncGenerator[Union[StreamingTextResponse, ResponseSummary], None]:\n        if type_ == \"text_completions\":\n            target = f\"{self.target}{TEXT_COMPLETIONS_PATH}\"\n        elif type_ == \"chat_completions\":\n            target = f\"{self.target}{CHAT_COMPLETIONS_PATH}\"\n        else:\n            raise ValueError(f\"Unsupported type: {type_}\")\n\n        logger.info(\n            \"{} making request: {} to target: {} using http2: {} following \"\n            \"redirects: {} for timeout: {} with headers: {} and params: {} and \",\n            \"payload: {}\",\n            self.__class__.__name__,\n            request_id,\n            target,\n            self.http2,\n            self.follow_redirects,\n            self.timeout,\n            headers,\n            params,\n            payload,\n        )\n\n        response_value = \"\"\n        response_prompt_count: Optional[int] = None\n        response_output_count: Optional[int] = None\n        iter_count = 0\n        start_time = time.time()\n        iter_time = start_time\n        first_iter_time: Optional[float] = None\n        last_iter_time: Optional[float] = None\n\n        yield StreamingTextResponse(\n            type_=\"start\",\n            value=\"\",\n            start_time=start_time,\n            first_iter_time=None,\n            iter_count=iter_count,\n            delta=\"\",\n            time=start_time,\n            request_id=request_id,\n        )\n\n        # reset start time after yielding start response to ensure accurate timing\n        start_time = time.time()\n\n        async with self._get_async_client().stream(\n            \"POST\", target, headers=headers, params=params, json=payload\n        ) as stream:\n            stream.raise_for_status()\n\n            async for line in stream.aiter_lines():\n                iter_time = time.time()\n                logger.debug(\n                    \"{} request: {} recieved iter response line: {}\",\n                    self.__class__.__name__,\n                    request_id,\n                    line,\n                )\n\n                if not line or not line.strip().startswith(\"data:\"):\n                    continue\n\n                if line.strip() == \"data: [DONE]\":\n                    break\n\n                data = json.loads(line.strip()[len(\"data: \") :])\n                if delta := self._extract_completions_delta_content(type_, data):\n                    if first_iter_time is None:\n                        first_iter_time = iter_time\n                    last_iter_time = iter_time\n\n                    iter_count += 1\n                    response_value += delta\n\n                    yield StreamingTextResponse(\n                        type_=\"iter\",\n                        value=response_value,\n                        iter_count=iter_count,\n                        start_time=start_time,\n                        first_iter_time=first_iter_time,\n                        delta=delta,\n                        time=iter_time,\n                        request_id=request_id,\n                    )\n\n                if usage := self._extract_completions_usage(data):\n                    response_prompt_count = usage[\"prompt\"]\n                    response_output_count = usage[\"output\"]\n\n        logger.info(\n            \"{} request: {} with headers: {} and params: {} and payload: {} completed\"\n            \"with: {}\",\n            self.__class__.__name__,\n            request_id,\n            headers,\n            params,\n            payload,\n            response_value,\n        )\n\n        yield ResponseSummary(\n            value=response_value,\n            request_args=RequestArgs(\n                target=target,\n                headers=headers,\n                params=params,\n                payload=payload,\n                timeout=self.timeout,\n                http2=self.http2,\n                follow_redirects=self.follow_redirects,\n            ),\n            start_time=start_time,\n            end_time=iter_time,\n            first_iter_time=first_iter_time,\n            last_iter_time=last_iter_time,\n            iterations=iter_count,\n            request_prompt_tokens=request_prompt_tokens,\n            request_output_tokens=request_output_tokens,\n            response_prompt_tokens=response_prompt_count,\n            response_output_tokens=response_output_count,\n            request_id=request_id,\n        )\n\n    @staticmethod\n    def _extract_completions_delta_content(\n        type_: Literal[\"text_completions\", \"chat_completions\"], data: dict\n    ) -&gt; Optional[str]:\n        if \"choices\" not in data or not data[\"choices\"]:\n            return None\n\n        if type_ == \"text_completions\":\n            return data[\"choices\"][0][\"text\"]\n\n        if type_ == \"chat_completions\":\n            return data[\"choices\"][0][\"delta\"][\"content\"]\n\n        raise ValueError(f\"Unsupported type: {type_}\")\n\n    @staticmethod\n    def _extract_completions_usage(\n        data: dict,\n    ) -&gt; Optional[dict[Literal[\"prompt\", \"output\"], int]]:\n        if \"usage\" not in data or not data[\"usage\"]:\n            return None\n\n        return {\n            \"prompt\": data[\"usage\"][\"prompt_tokens\"],\n            \"output\": data[\"usage\"][\"completion_tokens\"],\n        }\n</code></pre>"},{"location":"reference/guidellm/backend/#guidellm.backend.OpenAIHTTPBackend.info","title":"<code>info</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The information about the backend.</p>"},{"location":"reference/guidellm/backend/#guidellm.backend.OpenAIHTTPBackend.model","title":"<code>model</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>Optional[str]</code> <p>The model to use for all requests on the target server. If validate hasn't been called yet and no model was passed in, this will be None until validate is called to set the default.</p>"},{"location":"reference/guidellm/backend/#guidellm.backend.OpenAIHTTPBackend.target","title":"<code>target</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>str</code> <p>The target URL string for the OpenAI server.</p>"},{"location":"reference/guidellm/backend/#guidellm.backend.OpenAIHTTPBackend.available_models","title":"<code>available_models()</code>  <code>async</code>","text":"<p>Get the available models for the target server using the OpenAI models endpoint: /v1/models</p> Source code in <code>src/guidellm/backend/openai.py</code> <pre><code>async def available_models(self) -&gt; list[str]:\n    \"\"\"\n    Get the available models for the target server using the OpenAI models endpoint:\n    /v1/models\n    \"\"\"\n    target = f\"{self.target}/v1/models\"\n    headers = self._headers()\n    params = self._params(MODELS)\n    response = await self._get_async_client().get(\n        target, headers=headers, params=params\n    )\n    response.raise_for_status()\n\n    models = []\n\n    for item in response.json()[\"data\"]:\n        models.append(item[\"id\"])\n\n    return models\n</code></pre>"},{"location":"reference/guidellm/backend/#guidellm.backend.OpenAIHTTPBackend.chat_completions","title":"<code>chat_completions(content, request_id=None, prompt_token_count=None, output_token_count=None, raw_content=False, **kwargs)</code>  <code>async</code>","text":"<p>Generate chat completions for the given content using the OpenAI chat completions endpoint: /v1/chat/completions.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>Union[str, list[Union[str, dict[str, Union[str, dict[str, str]]], Path, Image]], Any]</code> <p>The content (or list of content) to generate a completion for. This supports any combination of text, images, and audio (model dependent). Supported text only request examples: content=\"Sample prompt\", content=[\"Sample prompt\", \"Second prompt\"], content=[{\"type\": \"text\", \"value\": \"Sample prompt\"}. Supported text and image request examples: content=[\"Describe the image\", PIL.Image.open(\"image.jpg\")], content=[\"Describe the image\", Path(\"image.jpg\")], content=[\"Describe the image\", {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}]. Supported text and audio request examples: content=[\"Transcribe the audio\", Path(\"audio.wav\")], content=[\"Transcribe the audio\", {\"type\": \"input_audio\", \"input_audio\": {\"data\": f\"{base64_bytes}\", \"format\": \"wav}]. Additionally, if raw_content=True then the content is passed directly to the backend without any processing.</p> required <code>request_id</code> <code>Optional[str]</code> <p>The unique identifier for the request, if any. Added to logging statements and the response for tracking purposes.</p> <code>None</code> <code>prompt_token_count</code> <code>Optional[int]</code> <p>The number of tokens measured in the prompt, if any. Returned in the response stats for later analysis, if applicable.</p> <code>None</code> <code>output_token_count</code> <code>Optional[int]</code> <p>If supplied, the number of tokens to enforce generation of for the output for this request.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass with the request.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncGenerator[Union[StreamingTextResponse, ResponseSummary], None]</code> <p>An async generator that yields a StreamingTextResponse for start, a StreamingTextResponse for each received iteration, and a ResponseSummary for the final response.</p> Source code in <code>src/guidellm/backend/openai.py</code> <pre><code>async def chat_completions(  # type: ignore[override]\n    self,\n    content: Union[\n        str,\n        list[Union[str, dict[str, Union[str, dict[str, str]]], Path, Image.Image]],\n        Any,\n    ],\n    request_id: Optional[str] = None,\n    prompt_token_count: Optional[int] = None,\n    output_token_count: Optional[int] = None,\n    raw_content: bool = False,\n    **kwargs,\n) -&gt; AsyncGenerator[Union[StreamingTextResponse, ResponseSummary], None]:\n    \"\"\"\n    Generate chat completions for the given content using the OpenAI\n    chat completions endpoint: /v1/chat/completions.\n\n    :param content: The content (or list of content) to generate a completion for.\n        This supports any combination of text, images, and audio (model dependent).\n        Supported text only request examples:\n            content=\"Sample prompt\", content=[\"Sample prompt\", \"Second prompt\"],\n            content=[{\"type\": \"text\", \"value\": \"Sample prompt\"}.\n        Supported text and image request examples:\n            content=[\"Describe the image\", PIL.Image.open(\"image.jpg\")],\n            content=[\"Describe the image\", Path(\"image.jpg\")],\n            content=[\"Describe the image\", {\"type\": \"image_url\",\n                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}].\n        Supported text and audio request examples:\n            content=[\"Transcribe the audio\", Path(\"audio.wav\")],\n            content=[\"Transcribe the audio\", {\"type\": \"input_audio\",\n            \"input_audio\": {\"data\": f\"{base64_bytes}\", \"format\": \"wav}].\n        Additionally, if raw_content=True then the content is passed directly to the\n        backend without any processing.\n    :param request_id: The unique identifier for the request, if any.\n        Added to logging statements and the response for tracking purposes.\n    :param prompt_token_count: The number of tokens measured in the prompt, if any.\n        Returned in the response stats for later analysis, if applicable.\n    :param output_token_count: If supplied, the number of tokens to enforce\n        generation of for the output for this request.\n    :param kwargs: Additional keyword arguments to pass with the request.\n    :return: An async generator that yields a StreamingTextResponse for start,\n        a StreamingTextResponse for each received iteration,\n        and a ResponseSummary for the final response.\n    \"\"\"\n    logger.debug(\"{} invocation with args: {}\", self.__class__.__name__, locals())\n    headers = self._headers()\n    params = self._params(CHAT_COMPLETIONS)\n    body = self._body(CHAT_COMPLETIONS)\n    messages = (\n        content if raw_content else self._create_chat_messages(content=content)\n    )\n    payload = self._completions_payload(\n        body=body,\n        orig_kwargs=kwargs,\n        max_output_tokens=output_token_count,\n        messages=messages,\n    )\n\n    try:\n        async for resp in self._iterative_completions_request(\n            type_=\"chat_completions\",\n            request_id=request_id,\n            request_prompt_tokens=prompt_token_count,\n            request_output_tokens=output_token_count,\n            headers=headers,\n            params=params,\n            payload=payload,\n        ):\n            yield resp\n    except Exception as ex:\n        logger.error(\n            \"{} request with headers: {} and params: {} and payload: {} failed: {}\",\n            self.__class__.__name__,\n            headers,\n            params,\n            payload,\n            ex,\n        )\n        raise ex\n</code></pre>"},{"location":"reference/guidellm/backend/#guidellm.backend.OpenAIHTTPBackend.check_setup","title":"<code>check_setup()</code>  <code>async</code>","text":"<p>Check if the backend is setup correctly and can be used for requests. Specifically, if a model is not provided, it grabs the first available model. If no models are available, raises a ValueError. If a model is provided and not available, raises a ValueError.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no models or the provided model is not available.</p> Source code in <code>src/guidellm/backend/openai.py</code> <pre><code>async def check_setup(self):\n    \"\"\"\n    Check if the backend is setup correctly and can be used for requests.\n    Specifically, if a model is not provided, it grabs the first available model.\n    If no models are available, raises a ValueError.\n    If a model is provided and not available, raises a ValueError.\n\n    :raises ValueError: If no models or the provided model is not available.\n    \"\"\"\n    models = await self.available_models()\n    if not models:\n        raise ValueError(f\"No models available for target: {self.target}\")\n\n    if not self.model:\n        self._model = models[0]\n    elif self.model not in models:\n        raise ValueError(\n            f\"Model {self.model} not found in available models:\"\n            f\"{models} for target: {self.target}\"\n        )\n</code></pre>"},{"location":"reference/guidellm/backend/#guidellm.backend.OpenAIHTTPBackend.prepare_multiprocessing","title":"<code>prepare_multiprocessing()</code>  <code>async</code>","text":"<p>Prepare the backend for use in a multiprocessing environment. Clears out the sync and async clients to ensure they are re-initialized for each process.</p> Source code in <code>src/guidellm/backend/openai.py</code> <pre><code>async def prepare_multiprocessing(self):\n    \"\"\"\n    Prepare the backend for use in a multiprocessing environment.\n    Clears out the sync and async clients to ensure they are re-initialized\n    for each process.\n    \"\"\"\n    if self._async_client is not None:\n        await self._async_client.aclose()\n        self._async_client = None\n</code></pre>"},{"location":"reference/guidellm/backend/#guidellm.backend.OpenAIHTTPBackend.text_completions","title":"<code>text_completions(prompt, request_id=None, prompt_token_count=None, output_token_count=None, **kwargs)</code>  <code>async</code>","text":"<p>Generate text completions for the given prompt using the OpenAI completions endpoint: /v1/completions.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Union[str, list[str]]</code> <p>The prompt (or list of prompts) to generate a completion for. If a list is supplied, these are concatenated and run through the model for a single prompt.</p> required <code>request_id</code> <code>Optional[str]</code> <p>The unique identifier for the request, if any. Added to logging statements and the response for tracking purposes.</p> <code>None</code> <code>prompt_token_count</code> <code>Optional[int]</code> <p>The number of tokens measured in the prompt, if any. Returned in the response stats for later analysis, if applicable.</p> <code>None</code> <code>output_token_count</code> <code>Optional[int]</code> <p>If supplied, the number of tokens to enforce generation of for the output for this request.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass with the request.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncGenerator[Union[StreamingTextResponse, ResponseSummary], None]</code> <p>An async generator that yields a StreamingTextResponse for start, a StreamingTextResponse for each received iteration, and a ResponseSummary for the final response.</p> Source code in <code>src/guidellm/backend/openai.py</code> <pre><code>async def text_completions(  # type: ignore[override]\n    self,\n    prompt: Union[str, list[str]],\n    request_id: Optional[str] = None,\n    prompt_token_count: Optional[int] = None,\n    output_token_count: Optional[int] = None,\n    **kwargs,\n) -&gt; AsyncGenerator[Union[StreamingTextResponse, ResponseSummary], None]:\n    \"\"\"\n    Generate text completions for the given prompt using the OpenAI\n    completions endpoint: /v1/completions.\n\n    :param prompt: The prompt (or list of prompts) to generate a completion for.\n        If a list is supplied, these are concatenated and run through the model\n        for a single prompt.\n    :param request_id: The unique identifier for the request, if any.\n        Added to logging statements and the response for tracking purposes.\n    :param prompt_token_count: The number of tokens measured in the prompt, if any.\n        Returned in the response stats for later analysis, if applicable.\n    :param output_token_count: If supplied, the number of tokens to enforce\n        generation of for the output for this request.\n    :param kwargs: Additional keyword arguments to pass with the request.\n    :return: An async generator that yields a StreamingTextResponse for start,\n        a StreamingTextResponse for each received iteration,\n        and a ResponseSummary for the final response.\n    \"\"\"\n    logger.debug(\"{} invocation with args: {}\", self.__class__.__name__, locals())\n\n    if isinstance(prompt, list):\n        raise ValueError(\n            \"List prompts (batching) is currently not supported for \"\n            f\"text_completions OpenAI pathways. Received: {prompt}\"\n        )\n\n    headers = self._headers()\n    params = self._params(TEXT_COMPLETIONS)\n    body = self._body(TEXT_COMPLETIONS)\n    payload = self._completions_payload(\n        body=body,\n        orig_kwargs=kwargs,\n        max_output_tokens=output_token_count,\n        prompt=prompt,\n    )\n\n    try:\n        async for resp in self._iterative_completions_request(\n            type_=\"text_completions\",\n            request_id=request_id,\n            request_prompt_tokens=prompt_token_count,\n            request_output_tokens=output_token_count,\n            headers=headers,\n            params=params,\n            payload=payload,\n        ):\n            yield resp\n    except Exception as ex:\n        logger.error(\n            \"{} request with headers: {} and params: {} and payload: {} failed: {}\",\n            self.__class__.__name__,\n            headers,\n            params,\n            payload,\n            ex,\n        )\n        raise ex\n</code></pre>"},{"location":"reference/guidellm/backend/#guidellm.backend.RequestArgs","title":"<code>RequestArgs</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>A model representing the arguments for a request to a backend. Biases towards an HTTP request, but can be used for other types of backends.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <p>The target URL or function for the request.</p> required <code>headers</code> <p>The headers, if any, included in the request such as authorization.</p> required <code>params</code> <p>The query parameters, if any, included in the request.</p> required <code>payload</code> <p>The payload / arguments for the request including the prompt / content and other configurations.</p> required <code>timeout</code> <p>The timeout for the request in seconds, if any.</p> required <code>http2</code> <p>Whether HTTP/2 was used for the request, if applicable.</p> required <code>follow_redirects</code> <p>Whether the request should follow redirect responses.</p> required Source code in <code>src/guidellm/backend/response.py</code> <pre><code>class RequestArgs(StandardBaseModel):\n    \"\"\"\n    A model representing the arguments for a request to a backend.\n    Biases towards an HTTP request, but can be used for other types of backends.\n\n    :param target: The target URL or function for the request.\n    :param headers: The headers, if any, included in the request such as authorization.\n    :param params: The query parameters, if any, included in the request.\n    :param payload: The payload / arguments for the request including the prompt /\n        content and other configurations.\n    :param timeout: The timeout for the request in seconds, if any.\n    :param http2: Whether HTTP/2 was used for the request, if applicable.\n    :param follow_redirects: Whether the request should follow redirect responses.\n    \"\"\"\n\n    target: str\n    headers: dict[str, str]\n    params: dict[str, str]\n    payload: dict[str, Any]\n    timeout: Optional[float] = None\n    http2: Optional[bool] = None\n    follow_redirects: Optional[bool] = None\n</code></pre>"},{"location":"reference/guidellm/backend/#guidellm.backend.ResponseSummary","title":"<code>ResponseSummary</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>A model representing a summary of a backend request. Always returned as the final iteration of a streaming request.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <p>The final value returned from the request.</p> required <code>request_args</code> <p>The arguments used to make the request.</p> required <code>iterations</code> <p>The number of iterations in the request.</p> required <code>start_time</code> <p>The time the request started.</p> required <code>end_time</code> <p>The time the request ended.</p> required <code>first_iter_time</code> <p>The time the first iteration was received.</p> required <code>last_iter_time</code> <p>The time the last iteration was received.</p> required <code>request_prompt_tokens</code> <p>The number of tokens measured in the prompt for the request, if any.</p> required <code>request_output_tokens</code> <p>The number of tokens enforced for the output for the request, if any.</p> required <code>response_prompt_tokens</code> <p>The number of tokens measured in the prompt for the response, if any.</p> required <code>response_output_tokens</code> <p>The number of tokens measured in the output for the response, if any.</p> required <code>request_id</code> <p>The unique identifier for the request, if any.</p> required <code>error</code> <p>The error message, if any, returned from making the request.</p> required Source code in <code>src/guidellm/backend/response.py</code> <pre><code>class ResponseSummary(StandardBaseModel):\n    \"\"\"\n    A model representing a summary of a backend request.\n    Always returned as the final iteration of a streaming request.\n\n    :param value: The final value returned from the request.\n    :param request_args: The arguments used to make the request.\n    :param iterations: The number of iterations in the request.\n    :param start_time: The time the request started.\n    :param end_time: The time the request ended.\n    :param first_iter_time: The time the first iteration was received.\n    :param last_iter_time: The time the last iteration was received.\n    :param request_prompt_tokens: The number of tokens measured in the prompt\n        for the request, if any.\n    :param request_output_tokens: The number of tokens enforced for the output\n        for the request, if any.\n    :param response_prompt_tokens: The number of tokens measured in the prompt\n        for the response, if any.\n    :param response_output_tokens: The number of tokens measured in the output\n        for the response, if any.\n    :param request_id: The unique identifier for the request, if any.\n    :param error: The error message, if any, returned from making the request.\n    \"\"\"\n\n    value: str\n    request_args: RequestArgs\n    iterations: int = 0\n    start_time: float\n    end_time: float\n    first_iter_time: Optional[float]\n    last_iter_time: Optional[float]\n    request_prompt_tokens: Optional[int] = None\n    request_output_tokens: Optional[int] = None\n    response_prompt_tokens: Optional[int] = None\n    response_output_tokens: Optional[int] = None\n    request_id: Optional[str] = None\n    error: Optional[str] = None\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def prompt_tokens(self) -&gt; Optional[int]:\n        \"\"\"\n        The number of tokens measured in the prompt based on preferences\n        for trusting the input or response.\n\n        :return: The number of tokens in the prompt, if any.\n        \"\"\"\n        if settings.preferred_prompt_tokens_source == \"request\":\n            return self.request_prompt_tokens or self.response_prompt_tokens\n\n        return self.response_prompt_tokens or self.request_prompt_tokens\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def output_tokens(self) -&gt; Optional[int]:\n        \"\"\"\n        The number of tokens measured in the output based on preferences\n        for trusting the input or response.\n\n        :return: The number of tokens in the output, if any.\n        \"\"\"\n        if self.error is not None:\n            # error occurred, can't trust request tokens were all generated\n            return self.response_prompt_tokens\n\n        if settings.preferred_output_tokens_source == \"request\":\n            return self.request_output_tokens or self.response_output_tokens\n\n        return self.response_output_tokens or self.request_output_tokens\n</code></pre>"},{"location":"reference/guidellm/backend/#guidellm.backend.ResponseSummary.output_tokens","title":"<code>output_tokens</code>  <code>property</code>","text":"<p>The number of tokens measured in the output based on preferences for trusting the input or response.</p> <p>Returns:</p> Type Description <code>Optional[int]</code> <p>The number of tokens in the output, if any.</p>"},{"location":"reference/guidellm/backend/#guidellm.backend.ResponseSummary.prompt_tokens","title":"<code>prompt_tokens</code>  <code>property</code>","text":"<p>The number of tokens measured in the prompt based on preferences for trusting the input or response.</p> <p>Returns:</p> Type Description <code>Optional[int]</code> <p>The number of tokens in the prompt, if any.</p>"},{"location":"reference/guidellm/backend/#guidellm.backend.StreamingTextResponse","title":"<code>StreamingTextResponse</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>A model representing the response content for a streaming text request.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <p>The type of the response; either 'start' or 'iter'.</p> required <code>value</code> <p>The value of the response up to this iteration.</p> required <code>start_time</code> <p>The time.time() the request started.</p> required <code>iter_count</code> <p>The iteration count for the response. For 'start' this is 0 and for the first 'iter' it is 1.</p> required <code>delta</code> <p>The text delta added to the response for this stream iteration.</p> required <code>time</code> <p>If 'start', the time.time() the request started. If 'iter', the time.time() the iteration was received.</p> required <code>request_id</code> <p>The unique identifier for the request, if any.</p> required Source code in <code>src/guidellm/backend/response.py</code> <pre><code>class StreamingTextResponse(StandardBaseModel):\n    \"\"\"\n    A model representing the response content for a streaming text request.\n\n    :param type_: The type of the response; either 'start' or 'iter'.\n    :param value: The value of the response up to this iteration.\n    :param start_time: The time.time() the request started.\n    :param iter_count: The iteration count for the response. For 'start' this is 0\n        and for the first 'iter' it is 1.\n    :param delta: The text delta added to the response for this stream iteration.\n    :param time: If 'start', the time.time() the request started.\n        If 'iter', the time.time() the iteration was received.\n    :param request_id: The unique identifier for the request, if any.\n    \"\"\"\n\n    type_: StreamingResponseType\n    value: str\n    start_time: float\n    first_iter_time: Optional[float]\n    iter_count: int\n    delta: str\n    time: float\n    request_id: Optional[str] = None\n</code></pre>"},{"location":"reference/guidellm/backend/backend/","title":"guidellm.backend.backend","text":""},{"location":"reference/guidellm/backend/backend/#guidellm.backend.backend.Backend","title":"<code>Backend</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for generative AI backends.</p> <p>This class provides a common interface for creating and interacting with different generative AI backends. Subclasses should implement the abstract methods to define specific backend behavior.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <code>BackendType</code> <p>The type of the backend.</p> required <p>Attributes:</p> Name Type Description <code>_registry</code> <code>dict[BackendType, type[Backend]]</code> <p>A registration dictionary that maps BackendType to backend classes.</p> Source code in <code>src/guidellm/backend/backend.py</code> <pre><code>class Backend(ABC):\n    \"\"\"\n    Abstract base class for generative AI backends.\n\n    This class provides a common interface for creating and interacting with different\n    generative AI backends. Subclasses should implement the abstract methods to\n    define specific backend behavior.\n\n    :cvar _registry: A registration dictionary that maps BackendType to backend classes.\n    :param type_: The type of the backend.\n    \"\"\"\n\n    _registry: dict[BackendType, \"type[Backend]\"] = {}\n\n    @classmethod\n    def register(cls, backend_type: BackendType):\n        \"\"\"\n        A decorator to register a backend class in the backend registry.\n\n        :param backend_type: The type of backend to register.\n        :type backend_type: BackendType\n        :return: The decorated backend class.\n        :rtype: Type[Backend]\n        \"\"\"\n        if backend_type in cls._registry:\n            raise ValueError(f\"Backend type already registered: {backend_type}\")\n\n        if not issubclass(cls, Backend):\n            raise TypeError(\"Only subclasses of Backend can be registered\")\n\n        def inner_wrapper(wrapped_class: type[\"Backend\"]):\n            cls._registry[backend_type] = wrapped_class\n            logger.info(\"Registered backend type: {}\", backend_type)\n            return wrapped_class\n\n        return inner_wrapper\n\n    @classmethod\n    def create(cls, type_: BackendType, **kwargs) -&gt; \"Backend\":\n        \"\"\"\n        Factory method to create a backend instance based on the backend type.\n\n        :param type_: The type of backend to create.\n        :type type_: BackendType\n        :param kwargs: Additional arguments for backend initialization.\n        :return: An instance of a subclass of Backend.\n        :rtype: Backend\n        :raises ValueError: If the backend type is not registered.\n        \"\"\"\n\n        logger.info(\"Creating backend of type {}\", type_)\n\n        if type_ not in cls._registry:\n            err = ValueError(f\"Unsupported backend type: {type_}\")\n            logger.error(\"{}\", err)\n            raise err\n\n        return Backend._registry[type_](**kwargs)\n\n    def __init__(self, type_: BackendType):\n        self._type = type_\n\n    @property\n    def type_(self) -&gt; BackendType:\n        \"\"\"\n        :return: The type of the backend.\n        \"\"\"\n        return self._type\n\n    @property\n    @abstractmethod\n    def target(self) -&gt; str:\n        \"\"\"\n        :return: The target location for the backend.\n        \"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def model(self) -&gt; Optional[str]:\n        \"\"\"\n        :return: The model used for the backend requests.\n        \"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def info(self) -&gt; dict[str, Any]:\n        \"\"\"\n        :return: The information about the backend.\n        \"\"\"\n        ...\n\n    async def validate(self):\n        \"\"\"\n        Handle final setup and validate the backend is ready for use.\n        If not successful, raises the appropriate exception.\n        \"\"\"\n        logger.info(\"{} validating backend {}\", self.__class__.__name__, self.type_)\n        await self.check_setup()\n        models = await self.available_models()\n        if not models:\n            raise ValueError(\"No models available for the backend\")\n\n        async for _ in self.text_completions(\n            prompt=\"Test connection\", output_token_count=1\n        ):  # type: ignore[attr-defined]\n            pass\n\n    @abstractmethod\n    async def check_setup(self):\n        \"\"\"\n        Check the setup for the backend.\n        If unsuccessful, raises the appropriate exception.\n\n        :raises ValueError: If the setup check fails.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    async def prepare_multiprocessing(self):\n        \"\"\"\n        Prepare the backend for use in a multiprocessing environment.\n        This is useful for backends that have instance state that can not\n        be shared across processes and should be cleared out and re-initialized\n        for each new process.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    async def available_models(self) -&gt; list[str]:\n        \"\"\"\n        Get the list of available models for the backend.\n\n        :return: The list of available models.\n        :rtype: List[str]\n        \"\"\"\n        ...\n\n    @abstractmethod\n    async def text_completions(\n        self,\n        prompt: Union[str, list[str]],\n        request_id: Optional[str] = None,\n        prompt_token_count: Optional[int] = None,\n        output_token_count: Optional[int] = None,\n        **kwargs,\n    ) -&gt; AsyncGenerator[Union[StreamingTextResponse, ResponseSummary], None]:\n        \"\"\"\n        Generate text only completions for the given prompt.\n        Does not support multiple modalities, complicated chat interfaces,\n        or chat templates. Specifically, it requests with only the prompt.\n\n        :param prompt: The prompt (or list of prompts) to generate a completion for.\n            If a list is supplied, these are concatenated and run through the model\n            for a single prompt.\n        :param request_id: The unique identifier for the request, if any.\n            Added to logging statements and the response for tracking purposes.\n        :param prompt_token_count: The number of tokens measured in the prompt, if any.\n            Returned in the response stats for later analysis, if applicable.\n        :param output_token_count: If supplied, the number of tokens to enforce\n            generation of for the output for this request.\n        :param kwargs: Additional keyword arguments to pass with the request.\n        :return: An async generator that yields a StreamingTextResponse for start,\n            a StreamingTextResponse for each received iteration,\n            and a ResponseSummary for the final response.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    async def chat_completions(\n        self,\n        content: Union[\n            str,\n            list[Union[str, dict[str, Union[str, dict[str, str]]], Path, Image.Image]],\n            Any,\n        ],\n        request_id: Optional[str] = None,\n        prompt_token_count: Optional[int] = None,\n        output_token_count: Optional[int] = None,\n        raw_content: bool = False,\n        **kwargs,\n    ) -&gt; AsyncGenerator[Union[StreamingTextResponse, ResponseSummary], None]:\n        \"\"\"\n        Generate chat completions for the given content.\n        Supports multiple modalities, complicated chat interfaces, and chat templates.\n        Specifically, it requests with the content, which can be any combination of\n        text, images, and audio provided the target model supports it,\n        and returns the output text. Additionally, any chat templates\n        for the model are applied within the backend.\n\n        :param content: The content (or list of content) to generate a completion for.\n            This supports any combination of text, images, and audio (model dependent).\n            Supported text only request examples:\n                content=\"Sample prompt\", content=[\"Sample prompt\", \"Second prompt\"],\n                content=[{\"type\": \"text\", \"value\": \"Sample prompt\"}.\n            Supported text and image request examples:\n                content=[\"Describe the image\", PIL.Image.open(\"image.jpg\")],\n                content=[\"Describe the image\", Path(\"image.jpg\")],\n                content=[\"Describe the image\", {\"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}].\n            Supported text and audio request examples:\n                content=[\"Transcribe the audio\", Path(\"audio.wav\")],\n                content=[\"Transcribe the audio\", {\"type\": \"input_audio\",\n                \"input_audio\": {\"data\": f\"{base64_bytes}\", \"format\": \"wav}].\n            Additionally, if raw_content=True then the content is passed directly to the\n            backend without any processing.\n        :param request_id: The unique identifier for the request, if any.\n            Added to logging statements and the response for tracking purposes.\n        :param prompt_token_count: The number of tokens measured in the prompt, if any.\n            Returned in the response stats for later analysis, if applicable.\n        :param output_token_count: If supplied, the number of tokens to enforce\n            generation of for the output for this request.\n        :param kwargs: Additional keyword arguments to pass with the request.\n        :return: An async generator that yields a StreamingTextResponse for start,\n            a StreamingTextResponse for each received iteration,\n            and a ResponseSummary for the final response.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/guidellm/backend/backend/#guidellm.backend.backend.Backend.info","title":"<code>info</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The information about the backend.</p>"},{"location":"reference/guidellm/backend/backend/#guidellm.backend.backend.Backend.model","title":"<code>model</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns:</p> Type Description <code>Optional[str]</code> <p>The model used for the backend requests.</p>"},{"location":"reference/guidellm/backend/backend/#guidellm.backend.backend.Backend.target","title":"<code>target</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns:</p> Type Description <code>str</code> <p>The target location for the backend.</p>"},{"location":"reference/guidellm/backend/backend/#guidellm.backend.backend.Backend.type_","title":"<code>type_</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>BackendType</code> <p>The type of the backend.</p>"},{"location":"reference/guidellm/backend/backend/#guidellm.backend.backend.Backend.available_models","title":"<code>available_models()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Get the list of available models for the backend.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>The list of available models.</p> Source code in <code>src/guidellm/backend/backend.py</code> <pre><code>@abstractmethod\nasync def available_models(self) -&gt; list[str]:\n    \"\"\"\n    Get the list of available models for the backend.\n\n    :return: The list of available models.\n    :rtype: List[str]\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/guidellm/backend/backend/#guidellm.backend.backend.Backend.chat_completions","title":"<code>chat_completions(content, request_id=None, prompt_token_count=None, output_token_count=None, raw_content=False, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Generate chat completions for the given content. Supports multiple modalities, complicated chat interfaces, and chat templates. Specifically, it requests with the content, which can be any combination of text, images, and audio provided the target model supports it, and returns the output text. Additionally, any chat templates for the model are applied within the backend.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>Union[str, list[Union[str, dict[str, Union[str, dict[str, str]]], Path, Image]], Any]</code> <p>The content (or list of content) to generate a completion for. This supports any combination of text, images, and audio (model dependent). Supported text only request examples: content=\"Sample prompt\", content=[\"Sample prompt\", \"Second prompt\"], content=[{\"type\": \"text\", \"value\": \"Sample prompt\"}. Supported text and image request examples: content=[\"Describe the image\", PIL.Image.open(\"image.jpg\")], content=[\"Describe the image\", Path(\"image.jpg\")], content=[\"Describe the image\", {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}]. Supported text and audio request examples: content=[\"Transcribe the audio\", Path(\"audio.wav\")], content=[\"Transcribe the audio\", {\"type\": \"input_audio\", \"input_audio\": {\"data\": f\"{base64_bytes}\", \"format\": \"wav}]. Additionally, if raw_content=True then the content is passed directly to the backend without any processing.</p> required <code>request_id</code> <code>Optional[str]</code> <p>The unique identifier for the request, if any. Added to logging statements and the response for tracking purposes.</p> <code>None</code> <code>prompt_token_count</code> <code>Optional[int]</code> <p>The number of tokens measured in the prompt, if any. Returned in the response stats for later analysis, if applicable.</p> <code>None</code> <code>output_token_count</code> <code>Optional[int]</code> <p>If supplied, the number of tokens to enforce generation of for the output for this request.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass with the request.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncGenerator[Union[StreamingTextResponse, ResponseSummary], None]</code> <p>An async generator that yields a StreamingTextResponse for start, a StreamingTextResponse for each received iteration, and a ResponseSummary for the final response.</p> Source code in <code>src/guidellm/backend/backend.py</code> <pre><code>@abstractmethod\nasync def chat_completions(\n    self,\n    content: Union[\n        str,\n        list[Union[str, dict[str, Union[str, dict[str, str]]], Path, Image.Image]],\n        Any,\n    ],\n    request_id: Optional[str] = None,\n    prompt_token_count: Optional[int] = None,\n    output_token_count: Optional[int] = None,\n    raw_content: bool = False,\n    **kwargs,\n) -&gt; AsyncGenerator[Union[StreamingTextResponse, ResponseSummary], None]:\n    \"\"\"\n    Generate chat completions for the given content.\n    Supports multiple modalities, complicated chat interfaces, and chat templates.\n    Specifically, it requests with the content, which can be any combination of\n    text, images, and audio provided the target model supports it,\n    and returns the output text. Additionally, any chat templates\n    for the model are applied within the backend.\n\n    :param content: The content (or list of content) to generate a completion for.\n        This supports any combination of text, images, and audio (model dependent).\n        Supported text only request examples:\n            content=\"Sample prompt\", content=[\"Sample prompt\", \"Second prompt\"],\n            content=[{\"type\": \"text\", \"value\": \"Sample prompt\"}.\n        Supported text and image request examples:\n            content=[\"Describe the image\", PIL.Image.open(\"image.jpg\")],\n            content=[\"Describe the image\", Path(\"image.jpg\")],\n            content=[\"Describe the image\", {\"type\": \"image_url\",\n                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}].\n        Supported text and audio request examples:\n            content=[\"Transcribe the audio\", Path(\"audio.wav\")],\n            content=[\"Transcribe the audio\", {\"type\": \"input_audio\",\n            \"input_audio\": {\"data\": f\"{base64_bytes}\", \"format\": \"wav}].\n        Additionally, if raw_content=True then the content is passed directly to the\n        backend without any processing.\n    :param request_id: The unique identifier for the request, if any.\n        Added to logging statements and the response for tracking purposes.\n    :param prompt_token_count: The number of tokens measured in the prompt, if any.\n        Returned in the response stats for later analysis, if applicable.\n    :param output_token_count: If supplied, the number of tokens to enforce\n        generation of for the output for this request.\n    :param kwargs: Additional keyword arguments to pass with the request.\n    :return: An async generator that yields a StreamingTextResponse for start,\n        a StreamingTextResponse for each received iteration,\n        and a ResponseSummary for the final response.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/guidellm/backend/backend/#guidellm.backend.backend.Backend.check_setup","title":"<code>check_setup()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Check the setup for the backend. If unsuccessful, raises the appropriate exception.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the setup check fails.</p> Source code in <code>src/guidellm/backend/backend.py</code> <pre><code>@abstractmethod\nasync def check_setup(self):\n    \"\"\"\n    Check the setup for the backend.\n    If unsuccessful, raises the appropriate exception.\n\n    :raises ValueError: If the setup check fails.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/guidellm/backend/backend/#guidellm.backend.backend.Backend.create","title":"<code>create(type_, **kwargs)</code>  <code>classmethod</code>","text":"<p>Factory method to create a backend instance based on the backend type.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <code>BackendType</code> <p>The type of backend to create.</p> required <code>kwargs</code> <p>Additional arguments for backend initialization.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Backend</code> <p>An instance of a subclass of Backend.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the backend type is not registered.</p> Source code in <code>src/guidellm/backend/backend.py</code> <pre><code>@classmethod\ndef create(cls, type_: BackendType, **kwargs) -&gt; \"Backend\":\n    \"\"\"\n    Factory method to create a backend instance based on the backend type.\n\n    :param type_: The type of backend to create.\n    :type type_: BackendType\n    :param kwargs: Additional arguments for backend initialization.\n    :return: An instance of a subclass of Backend.\n    :rtype: Backend\n    :raises ValueError: If the backend type is not registered.\n    \"\"\"\n\n    logger.info(\"Creating backend of type {}\", type_)\n\n    if type_ not in cls._registry:\n        err = ValueError(f\"Unsupported backend type: {type_}\")\n        logger.error(\"{}\", err)\n        raise err\n\n    return Backend._registry[type_](**kwargs)\n</code></pre>"},{"location":"reference/guidellm/backend/backend/#guidellm.backend.backend.Backend.prepare_multiprocessing","title":"<code>prepare_multiprocessing()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Prepare the backend for use in a multiprocessing environment. This is useful for backends that have instance state that can not be shared across processes and should be cleared out and re-initialized for each new process.</p> Source code in <code>src/guidellm/backend/backend.py</code> <pre><code>@abstractmethod\nasync def prepare_multiprocessing(self):\n    \"\"\"\n    Prepare the backend for use in a multiprocessing environment.\n    This is useful for backends that have instance state that can not\n    be shared across processes and should be cleared out and re-initialized\n    for each new process.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/guidellm/backend/backend/#guidellm.backend.backend.Backend.register","title":"<code>register(backend_type)</code>  <code>classmethod</code>","text":"<p>A decorator to register a backend class in the backend registry.</p> <p>Parameters:</p> Name Type Description Default <code>backend_type</code> <code>BackendType</code> <p>The type of backend to register.</p> required <p>Returns:</p> Type Description <code>Type[Backend]</code> <p>The decorated backend class.</p> Source code in <code>src/guidellm/backend/backend.py</code> <pre><code>@classmethod\ndef register(cls, backend_type: BackendType):\n    \"\"\"\n    A decorator to register a backend class in the backend registry.\n\n    :param backend_type: The type of backend to register.\n    :type backend_type: BackendType\n    :return: The decorated backend class.\n    :rtype: Type[Backend]\n    \"\"\"\n    if backend_type in cls._registry:\n        raise ValueError(f\"Backend type already registered: {backend_type}\")\n\n    if not issubclass(cls, Backend):\n        raise TypeError(\"Only subclasses of Backend can be registered\")\n\n    def inner_wrapper(wrapped_class: type[\"Backend\"]):\n        cls._registry[backend_type] = wrapped_class\n        logger.info(\"Registered backend type: {}\", backend_type)\n        return wrapped_class\n\n    return inner_wrapper\n</code></pre>"},{"location":"reference/guidellm/backend/backend/#guidellm.backend.backend.Backend.text_completions","title":"<code>text_completions(prompt, request_id=None, prompt_token_count=None, output_token_count=None, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Generate text only completions for the given prompt. Does not support multiple modalities, complicated chat interfaces, or chat templates. Specifically, it requests with only the prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Union[str, list[str]]</code> <p>The prompt (or list of prompts) to generate a completion for. If a list is supplied, these are concatenated and run through the model for a single prompt.</p> required <code>request_id</code> <code>Optional[str]</code> <p>The unique identifier for the request, if any. Added to logging statements and the response for tracking purposes.</p> <code>None</code> <code>prompt_token_count</code> <code>Optional[int]</code> <p>The number of tokens measured in the prompt, if any. Returned in the response stats for later analysis, if applicable.</p> <code>None</code> <code>output_token_count</code> <code>Optional[int]</code> <p>If supplied, the number of tokens to enforce generation of for the output for this request.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass with the request.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncGenerator[Union[StreamingTextResponse, ResponseSummary], None]</code> <p>An async generator that yields a StreamingTextResponse for start, a StreamingTextResponse for each received iteration, and a ResponseSummary for the final response.</p> Source code in <code>src/guidellm/backend/backend.py</code> <pre><code>@abstractmethod\nasync def text_completions(\n    self,\n    prompt: Union[str, list[str]],\n    request_id: Optional[str] = None,\n    prompt_token_count: Optional[int] = None,\n    output_token_count: Optional[int] = None,\n    **kwargs,\n) -&gt; AsyncGenerator[Union[StreamingTextResponse, ResponseSummary], None]:\n    \"\"\"\n    Generate text only completions for the given prompt.\n    Does not support multiple modalities, complicated chat interfaces,\n    or chat templates. Specifically, it requests with only the prompt.\n\n    :param prompt: The prompt (or list of prompts) to generate a completion for.\n        If a list is supplied, these are concatenated and run through the model\n        for a single prompt.\n    :param request_id: The unique identifier for the request, if any.\n        Added to logging statements and the response for tracking purposes.\n    :param prompt_token_count: The number of tokens measured in the prompt, if any.\n        Returned in the response stats for later analysis, if applicable.\n    :param output_token_count: If supplied, the number of tokens to enforce\n        generation of for the output for this request.\n    :param kwargs: Additional keyword arguments to pass with the request.\n    :return: An async generator that yields a StreamingTextResponse for start,\n        a StreamingTextResponse for each received iteration,\n        and a ResponseSummary for the final response.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/guidellm/backend/backend/#guidellm.backend.backend.Backend.validate","title":"<code>validate()</code>  <code>async</code>","text":"<p>Handle final setup and validate the backend is ready for use. If not successful, raises the appropriate exception.</p> Source code in <code>src/guidellm/backend/backend.py</code> <pre><code>async def validate(self):\n    \"\"\"\n    Handle final setup and validate the backend is ready for use.\n    If not successful, raises the appropriate exception.\n    \"\"\"\n    logger.info(\"{} validating backend {}\", self.__class__.__name__, self.type_)\n    await self.check_setup()\n    models = await self.available_models()\n    if not models:\n        raise ValueError(\"No models available for the backend\")\n\n    async for _ in self.text_completions(\n        prompt=\"Test connection\", output_token_count=1\n    ):  # type: ignore[attr-defined]\n        pass\n</code></pre>"},{"location":"reference/guidellm/backend/openai/","title":"guidellm.backend.openai","text":""},{"location":"reference/guidellm/backend/openai/#guidellm.backend.openai.OpenAIHTTPBackend","title":"<code>OpenAIHTTPBackend</code>","text":"<p>               Bases: <code>Backend</code></p> <p>A HTTP-based backend implementation for requests to an OpenAI compatible server. For example, a vLLM server instance or requests to OpenAI's API.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Optional[str]</code> <p>The target URL string for the OpenAI server. ex: http://0.0.0.0:8000</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>The model to use for all requests on the target server. If none is provided, the first available model will be used.</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>The API key to use for requests to the OpenAI server. If provided, adds an Authorization header with the value \"Authorization: Bearer {api_key}\". If not provided, no Authorization header is added.</p> <code>None</code> <code>organization</code> <code>Optional[str]</code> <p>The organization to use for requests to the OpenAI server. For example, if set to \"org_123\", adds an OpenAI-Organization header with the value \"OpenAI-Organization: org_123\". If not provided, no OpenAI-Organization header is added.</p> <code>None</code> <code>project</code> <code>Optional[str]</code> <p>The project to use for requests to the OpenAI server. For example, if set to \"project_123\", adds an OpenAI-Project header with the value \"OpenAI-Project: project_123\". If not provided, no OpenAI-Project header is added.</p> <code>None</code> <code>timeout</code> <code>Optional[float]</code> <p>The timeout to use for requests to the OpenAI server. If not provided, the default timeout provided from settings is used.</p> <code>None</code> <code>http2</code> <code>Optional[bool]</code> <p>If True, uses HTTP/2 for requests to the OpenAI server. Defaults to True.</p> <code>True</code> <code>follow_redirects</code> <code>Optional[bool]</code> <p>If True, the HTTP client will follow redirect responses. If not provided, the default value from settings is used.</p> <code>None</code> <code>max_output_tokens</code> <code>Optional[int]</code> <p>The maximum number of tokens to request for completions. If not provided, the default maximum tokens provided from settings is used.</p> <code>None</code> <code>extra_query</code> <code>Optional[dict]</code> <p>Query parameters to include in requests to the OpenAI server. If \"chat_completions\", \"models\", or \"text_completions\" are included as keys, the values of these keys will be used as the parameters for the respective endpoint. If not provided, no extra query parameters are added.</p> <code>None</code> Source code in <code>src/guidellm/backend/openai.py</code> <pre><code>@Backend.register(\"openai_http\")\nclass OpenAIHTTPBackend(Backend):\n    \"\"\"\n    A HTTP-based backend implementation for requests to an OpenAI compatible server.\n    For example, a vLLM server instance or requests to OpenAI's API.\n\n    :param target: The target URL string for the OpenAI server. ex: http://0.0.0.0:8000\n    :param model: The model to use for all requests on the target server.\n        If none is provided, the first available model will be used.\n    :param api_key: The API key to use for requests to the OpenAI server.\n        If provided, adds an Authorization header with the value\n        \"Authorization: Bearer {api_key}\".\n        If not provided, no Authorization header is added.\n    :param organization: The organization to use for requests to the OpenAI server.\n        For example, if set to \"org_123\", adds an OpenAI-Organization header with the\n        value \"OpenAI-Organization: org_123\".\n        If not provided, no OpenAI-Organization header is added.\n    :param project: The project to use for requests to the OpenAI server.\n        For example, if set to \"project_123\", adds an OpenAI-Project header with the\n        value \"OpenAI-Project: project_123\".\n        If not provided, no OpenAI-Project header is added.\n    :param timeout: The timeout to use for requests to the OpenAI server.\n        If not provided, the default timeout provided from settings is used.\n    :param http2: If True, uses HTTP/2 for requests to the OpenAI server.\n        Defaults to True.\n    :param follow_redirects: If True, the HTTP client will follow redirect responses.\n        If not provided, the default value from settings is used.\n    :param max_output_tokens: The maximum number of tokens to request for completions.\n        If not provided, the default maximum tokens provided from settings is used.\n    :param extra_query: Query parameters to include in requests to the OpenAI server.\n        If \"chat_completions\", \"models\", or \"text_completions\" are included as keys,\n        the values of these keys will be used as the parameters for the respective\n        endpoint.\n        If not provided, no extra query parameters are added.\n    \"\"\"\n\n    def __init__(\n        self,\n        target: Optional[str] = None,\n        model: Optional[str] = None,\n        api_key: Optional[str] = None,\n        organization: Optional[str] = None,\n        project: Optional[str] = None,\n        timeout: Optional[float] = None,\n        http2: Optional[bool] = True,\n        follow_redirects: Optional[bool] = None,\n        max_output_tokens: Optional[int] = None,\n        extra_query: Optional[dict] = None,\n        extra_body: Optional[dict] = None,\n    ):\n        super().__init__(type_=\"openai_http\")\n        self._target = target or settings.openai.base_url\n\n        if not self._target:\n            raise ValueError(\"Target URL must be provided for OpenAI HTTP backend.\")\n\n        if self._target.endswith(\"/v1\") or self._target.endswith(\"/v1/\"):\n            # backwards compatability, strip v1 off\n            self._target = self._target[:-3]\n\n        if self._target.endswith(\"/\"):\n            self._target = self._target[:-1]\n\n        self._model = model\n\n        api_key = api_key or settings.openai.api_key\n        self.authorization = (\n            f\"Bearer {api_key}\" if api_key else settings.openai.bearer_token\n        )\n\n        self.organization = organization or settings.openai.organization\n        self.project = project or settings.openai.project\n        self.timeout = timeout if timeout is not None else settings.request_timeout\n        self.http2 = http2 if http2 is not None else settings.request_http2\n        self.follow_redirects = (\n            follow_redirects\n            if follow_redirects is not None\n            else settings.request_follow_redirects\n        )\n        self.max_output_tokens = (\n            max_output_tokens\n            if max_output_tokens is not None\n            else settings.openai.max_output_tokens\n        )\n        self.extra_query = extra_query\n        self.extra_body = extra_body\n        self._async_client: Optional[httpx.AsyncClient] = None\n\n    @property\n    def target(self) -&gt; str:\n        \"\"\"\n        :return: The target URL string for the OpenAI server.\n        \"\"\"\n        return self._target\n\n    @property\n    def model(self) -&gt; Optional[str]:\n        \"\"\"\n        :return: The model to use for all requests on the target server.\n            If validate hasn't been called yet and no model was passed in,\n            this will be None until validate is called to set the default.\n        \"\"\"\n        return self._model\n\n    @property\n    def info(self) -&gt; dict[str, Any]:\n        \"\"\"\n        :return: The information about the backend.\n        \"\"\"\n        return {\n            \"max_output_tokens\": self.max_output_tokens,\n            \"timeout\": self.timeout,\n            \"http2\": self.http2,\n            \"follow_redirects\": self.follow_redirects,\n            \"authorization\": bool(self.authorization),\n            \"organization\": self.organization,\n            \"project\": self.project,\n            \"text_completions_path\": TEXT_COMPLETIONS_PATH,\n            \"chat_completions_path\": CHAT_COMPLETIONS_PATH,\n        }\n\n    async def check_setup(self):\n        \"\"\"\n        Check if the backend is setup correctly and can be used for requests.\n        Specifically, if a model is not provided, it grabs the first available model.\n        If no models are available, raises a ValueError.\n        If a model is provided and not available, raises a ValueError.\n\n        :raises ValueError: If no models or the provided model is not available.\n        \"\"\"\n        models = await self.available_models()\n        if not models:\n            raise ValueError(f\"No models available for target: {self.target}\")\n\n        if not self.model:\n            self._model = models[0]\n        elif self.model not in models:\n            raise ValueError(\n                f\"Model {self.model} not found in available models:\"\n                f\"{models} for target: {self.target}\"\n            )\n\n    async def prepare_multiprocessing(self):\n        \"\"\"\n        Prepare the backend for use in a multiprocessing environment.\n        Clears out the sync and async clients to ensure they are re-initialized\n        for each process.\n        \"\"\"\n        if self._async_client is not None:\n            await self._async_client.aclose()\n            self._async_client = None\n\n    async def available_models(self) -&gt; list[str]:\n        \"\"\"\n        Get the available models for the target server using the OpenAI models endpoint:\n        /v1/models\n        \"\"\"\n        target = f\"{self.target}/v1/models\"\n        headers = self._headers()\n        params = self._params(MODELS)\n        response = await self._get_async_client().get(\n            target, headers=headers, params=params\n        )\n        response.raise_for_status()\n\n        models = []\n\n        for item in response.json()[\"data\"]:\n            models.append(item[\"id\"])\n\n        return models\n\n    async def text_completions(  # type: ignore[override]\n        self,\n        prompt: Union[str, list[str]],\n        request_id: Optional[str] = None,\n        prompt_token_count: Optional[int] = None,\n        output_token_count: Optional[int] = None,\n        **kwargs,\n    ) -&gt; AsyncGenerator[Union[StreamingTextResponse, ResponseSummary], None]:\n        \"\"\"\n        Generate text completions for the given prompt using the OpenAI\n        completions endpoint: /v1/completions.\n\n        :param prompt: The prompt (or list of prompts) to generate a completion for.\n            If a list is supplied, these are concatenated and run through the model\n            for a single prompt.\n        :param request_id: The unique identifier for the request, if any.\n            Added to logging statements and the response for tracking purposes.\n        :param prompt_token_count: The number of tokens measured in the prompt, if any.\n            Returned in the response stats for later analysis, if applicable.\n        :param output_token_count: If supplied, the number of tokens to enforce\n            generation of for the output for this request.\n        :param kwargs: Additional keyword arguments to pass with the request.\n        :return: An async generator that yields a StreamingTextResponse for start,\n            a StreamingTextResponse for each received iteration,\n            and a ResponseSummary for the final response.\n        \"\"\"\n        logger.debug(\"{} invocation with args: {}\", self.__class__.__name__, locals())\n\n        if isinstance(prompt, list):\n            raise ValueError(\n                \"List prompts (batching) is currently not supported for \"\n                f\"text_completions OpenAI pathways. Received: {prompt}\"\n            )\n\n        headers = self._headers()\n        params = self._params(TEXT_COMPLETIONS)\n        body = self._body(TEXT_COMPLETIONS)\n        payload = self._completions_payload(\n            body=body,\n            orig_kwargs=kwargs,\n            max_output_tokens=output_token_count,\n            prompt=prompt,\n        )\n\n        try:\n            async for resp in self._iterative_completions_request(\n                type_=\"text_completions\",\n                request_id=request_id,\n                request_prompt_tokens=prompt_token_count,\n                request_output_tokens=output_token_count,\n                headers=headers,\n                params=params,\n                payload=payload,\n            ):\n                yield resp\n        except Exception as ex:\n            logger.error(\n                \"{} request with headers: {} and params: {} and payload: {} failed: {}\",\n                self.__class__.__name__,\n                headers,\n                params,\n                payload,\n                ex,\n            )\n            raise ex\n\n    async def chat_completions(  # type: ignore[override]\n        self,\n        content: Union[\n            str,\n            list[Union[str, dict[str, Union[str, dict[str, str]]], Path, Image.Image]],\n            Any,\n        ],\n        request_id: Optional[str] = None,\n        prompt_token_count: Optional[int] = None,\n        output_token_count: Optional[int] = None,\n        raw_content: bool = False,\n        **kwargs,\n    ) -&gt; AsyncGenerator[Union[StreamingTextResponse, ResponseSummary], None]:\n        \"\"\"\n        Generate chat completions for the given content using the OpenAI\n        chat completions endpoint: /v1/chat/completions.\n\n        :param content: The content (or list of content) to generate a completion for.\n            This supports any combination of text, images, and audio (model dependent).\n            Supported text only request examples:\n                content=\"Sample prompt\", content=[\"Sample prompt\", \"Second prompt\"],\n                content=[{\"type\": \"text\", \"value\": \"Sample prompt\"}.\n            Supported text and image request examples:\n                content=[\"Describe the image\", PIL.Image.open(\"image.jpg\")],\n                content=[\"Describe the image\", Path(\"image.jpg\")],\n                content=[\"Describe the image\", {\"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}].\n            Supported text and audio request examples:\n                content=[\"Transcribe the audio\", Path(\"audio.wav\")],\n                content=[\"Transcribe the audio\", {\"type\": \"input_audio\",\n                \"input_audio\": {\"data\": f\"{base64_bytes}\", \"format\": \"wav}].\n            Additionally, if raw_content=True then the content is passed directly to the\n            backend without any processing.\n        :param request_id: The unique identifier for the request, if any.\n            Added to logging statements and the response for tracking purposes.\n        :param prompt_token_count: The number of tokens measured in the prompt, if any.\n            Returned in the response stats for later analysis, if applicable.\n        :param output_token_count: If supplied, the number of tokens to enforce\n            generation of for the output for this request.\n        :param kwargs: Additional keyword arguments to pass with the request.\n        :return: An async generator that yields a StreamingTextResponse for start,\n            a StreamingTextResponse for each received iteration,\n            and a ResponseSummary for the final response.\n        \"\"\"\n        logger.debug(\"{} invocation with args: {}\", self.__class__.__name__, locals())\n        headers = self._headers()\n        params = self._params(CHAT_COMPLETIONS)\n        body = self._body(CHAT_COMPLETIONS)\n        messages = (\n            content if raw_content else self._create_chat_messages(content=content)\n        )\n        payload = self._completions_payload(\n            body=body,\n            orig_kwargs=kwargs,\n            max_output_tokens=output_token_count,\n            messages=messages,\n        )\n\n        try:\n            async for resp in self._iterative_completions_request(\n                type_=\"chat_completions\",\n                request_id=request_id,\n                request_prompt_tokens=prompt_token_count,\n                request_output_tokens=output_token_count,\n                headers=headers,\n                params=params,\n                payload=payload,\n            ):\n                yield resp\n        except Exception as ex:\n            logger.error(\n                \"{} request with headers: {} and params: {} and payload: {} failed: {}\",\n                self.__class__.__name__,\n                headers,\n                params,\n                payload,\n                ex,\n            )\n            raise ex\n\n    def _get_async_client(self) -&gt; httpx.AsyncClient:\n        \"\"\"\n        Get the async HTTP client for making requests.\n        If the client has not been created yet, it will create one.\n\n        :return: The async HTTP client.\n        \"\"\"\n        if self._async_client is None:\n            client = httpx.AsyncClient(\n                http2=self.http2,\n                timeout=self.timeout,\n                follow_redirects=self.follow_redirects,\n            )\n            self._async_client = client\n        else:\n            client = self._async_client\n\n        return client\n\n    def _headers(self) -&gt; dict[str, str]:\n        headers = {\n            \"Content-Type\": \"application/json\",\n        }\n\n        if self.authorization:\n            headers[\"Authorization\"] = self.authorization\n\n        if self.organization:\n            headers[\"OpenAI-Organization\"] = self.organization\n\n        if self.project:\n            headers[\"OpenAI-Project\"] = self.project\n\n        return headers\n\n    def _params(self, endpoint_type: EndpointType) -&gt; dict[str, str]:\n        if self.extra_query is None:\n            return {}\n\n        if (\n            CHAT_COMPLETIONS in self.extra_query\n            or MODELS in self.extra_query\n            or TEXT_COMPLETIONS in self.extra_query\n        ):\n            return self.extra_query.get(endpoint_type, {})\n\n        return self.extra_query\n\n    def _body(self, endpoint_type: EndpointType) -&gt; dict[str, str]:\n        if self.extra_body is None:\n            return {}\n\n        if (\n            CHAT_COMPLETIONS in self.extra_body\n            or MODELS in self.extra_body\n            or TEXT_COMPLETIONS in self.extra_body\n        ):\n            return self.extra_body.get(endpoint_type, {})\n\n        return self.extra_body\n\n    def _completions_payload(\n        self,\n        body: Optional[dict],\n        orig_kwargs: Optional[dict],\n        max_output_tokens: Optional[int],\n        **kwargs,\n    ) -&gt; dict:\n        payload = body or {}\n        payload.update(orig_kwargs or {})\n        payload.update(kwargs)\n        payload[\"model\"] = self.model\n        payload[\"stream\"] = True\n        payload[\"stream_options\"] = {\n            \"include_usage\": True,\n        }\n\n        if max_output_tokens or self.max_output_tokens:\n            logger.debug(\n                \"{} adding payload args for setting output_token_count: {}\",\n                self.__class__.__name__,\n                max_output_tokens or self.max_output_tokens,\n            )\n            payload[\"max_tokens\"] = max_output_tokens or self.max_output_tokens\n            payload[\"max_completion_tokens\"] = payload[\"max_tokens\"]\n\n            if max_output_tokens:\n                # only set stop and ignore_eos if max_output_tokens set at request level\n                # otherwise the instance value is just the max to enforce we stay below\n                payload[\"stop\"] = None\n                payload[\"ignore_eos\"] = True\n\n        return payload\n\n    @staticmethod\n    def _create_chat_messages(\n        content: Union[\n            str,\n            list[Union[str, dict[str, Union[str, dict[str, str]]], Path, Image.Image]],\n            Any,\n        ],\n    ) -&gt; list[dict]:\n        if isinstance(content, str):\n            return [\n                {\n                    \"role\": \"user\",\n                    \"content\": content,\n                }\n            ]\n\n        if isinstance(content, list):\n            resolved_content = []\n\n            for item in content:\n                if isinstance(item, dict):\n                    resolved_content.append(item)\n                elif isinstance(item, str):\n                    resolved_content.append({\"type\": \"text\", \"text\": item})\n                elif isinstance(item, Image.Image) or (\n                    isinstance(item, Path) and item.suffix.lower() in [\".jpg\", \".jpeg\"]\n                ):\n                    image = item if isinstance(item, Image.Image) else Image.open(item)\n                    encoded = base64.b64encode(image.tobytes()).decode(\"utf-8\")\n                    resolved_content.append(\n                        {\n                            \"type\": \"image\",\n                            \"image\": {\n                                \"url\": f\"data:image/jpeg;base64,{encoded}\",\n                            },\n                        }\n                    )\n                elif isinstance(item, Path) and item.suffix.lower() in [\".wav\"]:\n                    encoded = base64.b64encode(item.read_bytes()).decode(\"utf-8\")\n                    resolved_content.append(\n                        {\n                            \"type\": \"input_audio\",\n                            \"input_audio\": {\n                                \"data\": f\"{encoded}\",\n                                \"format\": \"wav\",\n                            },\n                        }\n                    )\n                else:\n                    raise ValueError(\n                        f\"Unsupported content item type: {item} in list: {content}\"\n                    )\n\n            return [\n                {\n                    \"role\": \"user\",\n                    \"content\": resolved_content,\n                }\n            ]\n\n        raise ValueError(f\"Unsupported content type: {content}\")\n\n    async def _iterative_completions_request(\n        self,\n        type_: Literal[\"text_completions\", \"chat_completions\"],\n        request_id: Optional[str],\n        request_prompt_tokens: Optional[int],\n        request_output_tokens: Optional[int],\n        headers: dict[str, str],\n        params: dict[str, str],\n        payload: dict[str, Any],\n    ) -&gt; AsyncGenerator[Union[StreamingTextResponse, ResponseSummary], None]:\n        if type_ == \"text_completions\":\n            target = f\"{self.target}{TEXT_COMPLETIONS_PATH}\"\n        elif type_ == \"chat_completions\":\n            target = f\"{self.target}{CHAT_COMPLETIONS_PATH}\"\n        else:\n            raise ValueError(f\"Unsupported type: {type_}\")\n\n        logger.info(\n            \"{} making request: {} to target: {} using http2: {} following \"\n            \"redirects: {} for timeout: {} with headers: {} and params: {} and \",\n            \"payload: {}\",\n            self.__class__.__name__,\n            request_id,\n            target,\n            self.http2,\n            self.follow_redirects,\n            self.timeout,\n            headers,\n            params,\n            payload,\n        )\n\n        response_value = \"\"\n        response_prompt_count: Optional[int] = None\n        response_output_count: Optional[int] = None\n        iter_count = 0\n        start_time = time.time()\n        iter_time = start_time\n        first_iter_time: Optional[float] = None\n        last_iter_time: Optional[float] = None\n\n        yield StreamingTextResponse(\n            type_=\"start\",\n            value=\"\",\n            start_time=start_time,\n            first_iter_time=None,\n            iter_count=iter_count,\n            delta=\"\",\n            time=start_time,\n            request_id=request_id,\n        )\n\n        # reset start time after yielding start response to ensure accurate timing\n        start_time = time.time()\n\n        async with self._get_async_client().stream(\n            \"POST\", target, headers=headers, params=params, json=payload\n        ) as stream:\n            stream.raise_for_status()\n\n            async for line in stream.aiter_lines():\n                iter_time = time.time()\n                logger.debug(\n                    \"{} request: {} recieved iter response line: {}\",\n                    self.__class__.__name__,\n                    request_id,\n                    line,\n                )\n\n                if not line or not line.strip().startswith(\"data:\"):\n                    continue\n\n                if line.strip() == \"data: [DONE]\":\n                    break\n\n                data = json.loads(line.strip()[len(\"data: \") :])\n                if delta := self._extract_completions_delta_content(type_, data):\n                    if first_iter_time is None:\n                        first_iter_time = iter_time\n                    last_iter_time = iter_time\n\n                    iter_count += 1\n                    response_value += delta\n\n                    yield StreamingTextResponse(\n                        type_=\"iter\",\n                        value=response_value,\n                        iter_count=iter_count,\n                        start_time=start_time,\n                        first_iter_time=first_iter_time,\n                        delta=delta,\n                        time=iter_time,\n                        request_id=request_id,\n                    )\n\n                if usage := self._extract_completions_usage(data):\n                    response_prompt_count = usage[\"prompt\"]\n                    response_output_count = usage[\"output\"]\n\n        logger.info(\n            \"{} request: {} with headers: {} and params: {} and payload: {} completed\"\n            \"with: {}\",\n            self.__class__.__name__,\n            request_id,\n            headers,\n            params,\n            payload,\n            response_value,\n        )\n\n        yield ResponseSummary(\n            value=response_value,\n            request_args=RequestArgs(\n                target=target,\n                headers=headers,\n                params=params,\n                payload=payload,\n                timeout=self.timeout,\n                http2=self.http2,\n                follow_redirects=self.follow_redirects,\n            ),\n            start_time=start_time,\n            end_time=iter_time,\n            first_iter_time=first_iter_time,\n            last_iter_time=last_iter_time,\n            iterations=iter_count,\n            request_prompt_tokens=request_prompt_tokens,\n            request_output_tokens=request_output_tokens,\n            response_prompt_tokens=response_prompt_count,\n            response_output_tokens=response_output_count,\n            request_id=request_id,\n        )\n\n    @staticmethod\n    def _extract_completions_delta_content(\n        type_: Literal[\"text_completions\", \"chat_completions\"], data: dict\n    ) -&gt; Optional[str]:\n        if \"choices\" not in data or not data[\"choices\"]:\n            return None\n\n        if type_ == \"text_completions\":\n            return data[\"choices\"][0][\"text\"]\n\n        if type_ == \"chat_completions\":\n            return data[\"choices\"][0][\"delta\"][\"content\"]\n\n        raise ValueError(f\"Unsupported type: {type_}\")\n\n    @staticmethod\n    def _extract_completions_usage(\n        data: dict,\n    ) -&gt; Optional[dict[Literal[\"prompt\", \"output\"], int]]:\n        if \"usage\" not in data or not data[\"usage\"]:\n            return None\n\n        return {\n            \"prompt\": data[\"usage\"][\"prompt_tokens\"],\n            \"output\": data[\"usage\"][\"completion_tokens\"],\n        }\n</code></pre>"},{"location":"reference/guidellm/backend/openai/#guidellm.backend.openai.OpenAIHTTPBackend.info","title":"<code>info</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The information about the backend.</p>"},{"location":"reference/guidellm/backend/openai/#guidellm.backend.openai.OpenAIHTTPBackend.model","title":"<code>model</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>Optional[str]</code> <p>The model to use for all requests on the target server. If validate hasn't been called yet and no model was passed in, this will be None until validate is called to set the default.</p>"},{"location":"reference/guidellm/backend/openai/#guidellm.backend.openai.OpenAIHTTPBackend.target","title":"<code>target</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>str</code> <p>The target URL string for the OpenAI server.</p>"},{"location":"reference/guidellm/backend/openai/#guidellm.backend.openai.OpenAIHTTPBackend.available_models","title":"<code>available_models()</code>  <code>async</code>","text":"<p>Get the available models for the target server using the OpenAI models endpoint: /v1/models</p> Source code in <code>src/guidellm/backend/openai.py</code> <pre><code>async def available_models(self) -&gt; list[str]:\n    \"\"\"\n    Get the available models for the target server using the OpenAI models endpoint:\n    /v1/models\n    \"\"\"\n    target = f\"{self.target}/v1/models\"\n    headers = self._headers()\n    params = self._params(MODELS)\n    response = await self._get_async_client().get(\n        target, headers=headers, params=params\n    )\n    response.raise_for_status()\n\n    models = []\n\n    for item in response.json()[\"data\"]:\n        models.append(item[\"id\"])\n\n    return models\n</code></pre>"},{"location":"reference/guidellm/backend/openai/#guidellm.backend.openai.OpenAIHTTPBackend.chat_completions","title":"<code>chat_completions(content, request_id=None, prompt_token_count=None, output_token_count=None, raw_content=False, **kwargs)</code>  <code>async</code>","text":"<p>Generate chat completions for the given content using the OpenAI chat completions endpoint: /v1/chat/completions.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>Union[str, list[Union[str, dict[str, Union[str, dict[str, str]]], Path, Image]], Any]</code> <p>The content (or list of content) to generate a completion for. This supports any combination of text, images, and audio (model dependent). Supported text only request examples: content=\"Sample prompt\", content=[\"Sample prompt\", \"Second prompt\"], content=[{\"type\": \"text\", \"value\": \"Sample prompt\"}. Supported text and image request examples: content=[\"Describe the image\", PIL.Image.open(\"image.jpg\")], content=[\"Describe the image\", Path(\"image.jpg\")], content=[\"Describe the image\", {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}]. Supported text and audio request examples: content=[\"Transcribe the audio\", Path(\"audio.wav\")], content=[\"Transcribe the audio\", {\"type\": \"input_audio\", \"input_audio\": {\"data\": f\"{base64_bytes}\", \"format\": \"wav}]. Additionally, if raw_content=True then the content is passed directly to the backend without any processing.</p> required <code>request_id</code> <code>Optional[str]</code> <p>The unique identifier for the request, if any. Added to logging statements and the response for tracking purposes.</p> <code>None</code> <code>prompt_token_count</code> <code>Optional[int]</code> <p>The number of tokens measured in the prompt, if any. Returned in the response stats for later analysis, if applicable.</p> <code>None</code> <code>output_token_count</code> <code>Optional[int]</code> <p>If supplied, the number of tokens to enforce generation of for the output for this request.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass with the request.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncGenerator[Union[StreamingTextResponse, ResponseSummary], None]</code> <p>An async generator that yields a StreamingTextResponse for start, a StreamingTextResponse for each received iteration, and a ResponseSummary for the final response.</p> Source code in <code>src/guidellm/backend/openai.py</code> <pre><code>async def chat_completions(  # type: ignore[override]\n    self,\n    content: Union[\n        str,\n        list[Union[str, dict[str, Union[str, dict[str, str]]], Path, Image.Image]],\n        Any,\n    ],\n    request_id: Optional[str] = None,\n    prompt_token_count: Optional[int] = None,\n    output_token_count: Optional[int] = None,\n    raw_content: bool = False,\n    **kwargs,\n) -&gt; AsyncGenerator[Union[StreamingTextResponse, ResponseSummary], None]:\n    \"\"\"\n    Generate chat completions for the given content using the OpenAI\n    chat completions endpoint: /v1/chat/completions.\n\n    :param content: The content (or list of content) to generate a completion for.\n        This supports any combination of text, images, and audio (model dependent).\n        Supported text only request examples:\n            content=\"Sample prompt\", content=[\"Sample prompt\", \"Second prompt\"],\n            content=[{\"type\": \"text\", \"value\": \"Sample prompt\"}.\n        Supported text and image request examples:\n            content=[\"Describe the image\", PIL.Image.open(\"image.jpg\")],\n            content=[\"Describe the image\", Path(\"image.jpg\")],\n            content=[\"Describe the image\", {\"type\": \"image_url\",\n                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}].\n        Supported text and audio request examples:\n            content=[\"Transcribe the audio\", Path(\"audio.wav\")],\n            content=[\"Transcribe the audio\", {\"type\": \"input_audio\",\n            \"input_audio\": {\"data\": f\"{base64_bytes}\", \"format\": \"wav}].\n        Additionally, if raw_content=True then the content is passed directly to the\n        backend without any processing.\n    :param request_id: The unique identifier for the request, if any.\n        Added to logging statements and the response for tracking purposes.\n    :param prompt_token_count: The number of tokens measured in the prompt, if any.\n        Returned in the response stats for later analysis, if applicable.\n    :param output_token_count: If supplied, the number of tokens to enforce\n        generation of for the output for this request.\n    :param kwargs: Additional keyword arguments to pass with the request.\n    :return: An async generator that yields a StreamingTextResponse for start,\n        a StreamingTextResponse for each received iteration,\n        and a ResponseSummary for the final response.\n    \"\"\"\n    logger.debug(\"{} invocation with args: {}\", self.__class__.__name__, locals())\n    headers = self._headers()\n    params = self._params(CHAT_COMPLETIONS)\n    body = self._body(CHAT_COMPLETIONS)\n    messages = (\n        content if raw_content else self._create_chat_messages(content=content)\n    )\n    payload = self._completions_payload(\n        body=body,\n        orig_kwargs=kwargs,\n        max_output_tokens=output_token_count,\n        messages=messages,\n    )\n\n    try:\n        async for resp in self._iterative_completions_request(\n            type_=\"chat_completions\",\n            request_id=request_id,\n            request_prompt_tokens=prompt_token_count,\n            request_output_tokens=output_token_count,\n            headers=headers,\n            params=params,\n            payload=payload,\n        ):\n            yield resp\n    except Exception as ex:\n        logger.error(\n            \"{} request with headers: {} and params: {} and payload: {} failed: {}\",\n            self.__class__.__name__,\n            headers,\n            params,\n            payload,\n            ex,\n        )\n        raise ex\n</code></pre>"},{"location":"reference/guidellm/backend/openai/#guidellm.backend.openai.OpenAIHTTPBackend.check_setup","title":"<code>check_setup()</code>  <code>async</code>","text":"<p>Check if the backend is setup correctly and can be used for requests. Specifically, if a model is not provided, it grabs the first available model. If no models are available, raises a ValueError. If a model is provided and not available, raises a ValueError.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no models or the provided model is not available.</p> Source code in <code>src/guidellm/backend/openai.py</code> <pre><code>async def check_setup(self):\n    \"\"\"\n    Check if the backend is setup correctly and can be used for requests.\n    Specifically, if a model is not provided, it grabs the first available model.\n    If no models are available, raises a ValueError.\n    If a model is provided and not available, raises a ValueError.\n\n    :raises ValueError: If no models or the provided model is not available.\n    \"\"\"\n    models = await self.available_models()\n    if not models:\n        raise ValueError(f\"No models available for target: {self.target}\")\n\n    if not self.model:\n        self._model = models[0]\n    elif self.model not in models:\n        raise ValueError(\n            f\"Model {self.model} not found in available models:\"\n            f\"{models} for target: {self.target}\"\n        )\n</code></pre>"},{"location":"reference/guidellm/backend/openai/#guidellm.backend.openai.OpenAIHTTPBackend.prepare_multiprocessing","title":"<code>prepare_multiprocessing()</code>  <code>async</code>","text":"<p>Prepare the backend for use in a multiprocessing environment. Clears out the sync and async clients to ensure they are re-initialized for each process.</p> Source code in <code>src/guidellm/backend/openai.py</code> <pre><code>async def prepare_multiprocessing(self):\n    \"\"\"\n    Prepare the backend for use in a multiprocessing environment.\n    Clears out the sync and async clients to ensure they are re-initialized\n    for each process.\n    \"\"\"\n    if self._async_client is not None:\n        await self._async_client.aclose()\n        self._async_client = None\n</code></pre>"},{"location":"reference/guidellm/backend/openai/#guidellm.backend.openai.OpenAIHTTPBackend.text_completions","title":"<code>text_completions(prompt, request_id=None, prompt_token_count=None, output_token_count=None, **kwargs)</code>  <code>async</code>","text":"<p>Generate text completions for the given prompt using the OpenAI completions endpoint: /v1/completions.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Union[str, list[str]]</code> <p>The prompt (or list of prompts) to generate a completion for. If a list is supplied, these are concatenated and run through the model for a single prompt.</p> required <code>request_id</code> <code>Optional[str]</code> <p>The unique identifier for the request, if any. Added to logging statements and the response for tracking purposes.</p> <code>None</code> <code>prompt_token_count</code> <code>Optional[int]</code> <p>The number of tokens measured in the prompt, if any. Returned in the response stats for later analysis, if applicable.</p> <code>None</code> <code>output_token_count</code> <code>Optional[int]</code> <p>If supplied, the number of tokens to enforce generation of for the output for this request.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass with the request.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncGenerator[Union[StreamingTextResponse, ResponseSummary], None]</code> <p>An async generator that yields a StreamingTextResponse for start, a StreamingTextResponse for each received iteration, and a ResponseSummary for the final response.</p> Source code in <code>src/guidellm/backend/openai.py</code> <pre><code>async def text_completions(  # type: ignore[override]\n    self,\n    prompt: Union[str, list[str]],\n    request_id: Optional[str] = None,\n    prompt_token_count: Optional[int] = None,\n    output_token_count: Optional[int] = None,\n    **kwargs,\n) -&gt; AsyncGenerator[Union[StreamingTextResponse, ResponseSummary], None]:\n    \"\"\"\n    Generate text completions for the given prompt using the OpenAI\n    completions endpoint: /v1/completions.\n\n    :param prompt: The prompt (or list of prompts) to generate a completion for.\n        If a list is supplied, these are concatenated and run through the model\n        for a single prompt.\n    :param request_id: The unique identifier for the request, if any.\n        Added to logging statements and the response for tracking purposes.\n    :param prompt_token_count: The number of tokens measured in the prompt, if any.\n        Returned in the response stats for later analysis, if applicable.\n    :param output_token_count: If supplied, the number of tokens to enforce\n        generation of for the output for this request.\n    :param kwargs: Additional keyword arguments to pass with the request.\n    :return: An async generator that yields a StreamingTextResponse for start,\n        a StreamingTextResponse for each received iteration,\n        and a ResponseSummary for the final response.\n    \"\"\"\n    logger.debug(\"{} invocation with args: {}\", self.__class__.__name__, locals())\n\n    if isinstance(prompt, list):\n        raise ValueError(\n            \"List prompts (batching) is currently not supported for \"\n            f\"text_completions OpenAI pathways. Received: {prompt}\"\n        )\n\n    headers = self._headers()\n    params = self._params(TEXT_COMPLETIONS)\n    body = self._body(TEXT_COMPLETIONS)\n    payload = self._completions_payload(\n        body=body,\n        orig_kwargs=kwargs,\n        max_output_tokens=output_token_count,\n        prompt=prompt,\n    )\n\n    try:\n        async for resp in self._iterative_completions_request(\n            type_=\"text_completions\",\n            request_id=request_id,\n            request_prompt_tokens=prompt_token_count,\n            request_output_tokens=output_token_count,\n            headers=headers,\n            params=params,\n            payload=payload,\n        ):\n            yield resp\n    except Exception as ex:\n        logger.error(\n            \"{} request with headers: {} and params: {} and payload: {} failed: {}\",\n            self.__class__.__name__,\n            headers,\n            params,\n            payload,\n            ex,\n        )\n        raise ex\n</code></pre>"},{"location":"reference/guidellm/backend/response/","title":"guidellm.backend.response","text":""},{"location":"reference/guidellm/backend/response/#guidellm.backend.response.RequestArgs","title":"<code>RequestArgs</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>A model representing the arguments for a request to a backend. Biases towards an HTTP request, but can be used for other types of backends.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <p>The target URL or function for the request.</p> required <code>headers</code> <p>The headers, if any, included in the request such as authorization.</p> required <code>params</code> <p>The query parameters, if any, included in the request.</p> required <code>payload</code> <p>The payload / arguments for the request including the prompt / content and other configurations.</p> required <code>timeout</code> <p>The timeout for the request in seconds, if any.</p> required <code>http2</code> <p>Whether HTTP/2 was used for the request, if applicable.</p> required <code>follow_redirects</code> <p>Whether the request should follow redirect responses.</p> required Source code in <code>src/guidellm/backend/response.py</code> <pre><code>class RequestArgs(StandardBaseModel):\n    \"\"\"\n    A model representing the arguments for a request to a backend.\n    Biases towards an HTTP request, but can be used for other types of backends.\n\n    :param target: The target URL or function for the request.\n    :param headers: The headers, if any, included in the request such as authorization.\n    :param params: The query parameters, if any, included in the request.\n    :param payload: The payload / arguments for the request including the prompt /\n        content and other configurations.\n    :param timeout: The timeout for the request in seconds, if any.\n    :param http2: Whether HTTP/2 was used for the request, if applicable.\n    :param follow_redirects: Whether the request should follow redirect responses.\n    \"\"\"\n\n    target: str\n    headers: dict[str, str]\n    params: dict[str, str]\n    payload: dict[str, Any]\n    timeout: Optional[float] = None\n    http2: Optional[bool] = None\n    follow_redirects: Optional[bool] = None\n</code></pre>"},{"location":"reference/guidellm/backend/response/#guidellm.backend.response.ResponseSummary","title":"<code>ResponseSummary</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>A model representing a summary of a backend request. Always returned as the final iteration of a streaming request.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <p>The final value returned from the request.</p> required <code>request_args</code> <p>The arguments used to make the request.</p> required <code>iterations</code> <p>The number of iterations in the request.</p> required <code>start_time</code> <p>The time the request started.</p> required <code>end_time</code> <p>The time the request ended.</p> required <code>first_iter_time</code> <p>The time the first iteration was received.</p> required <code>last_iter_time</code> <p>The time the last iteration was received.</p> required <code>request_prompt_tokens</code> <p>The number of tokens measured in the prompt for the request, if any.</p> required <code>request_output_tokens</code> <p>The number of tokens enforced for the output for the request, if any.</p> required <code>response_prompt_tokens</code> <p>The number of tokens measured in the prompt for the response, if any.</p> required <code>response_output_tokens</code> <p>The number of tokens measured in the output for the response, if any.</p> required <code>request_id</code> <p>The unique identifier for the request, if any.</p> required <code>error</code> <p>The error message, if any, returned from making the request.</p> required Source code in <code>src/guidellm/backend/response.py</code> <pre><code>class ResponseSummary(StandardBaseModel):\n    \"\"\"\n    A model representing a summary of a backend request.\n    Always returned as the final iteration of a streaming request.\n\n    :param value: The final value returned from the request.\n    :param request_args: The arguments used to make the request.\n    :param iterations: The number of iterations in the request.\n    :param start_time: The time the request started.\n    :param end_time: The time the request ended.\n    :param first_iter_time: The time the first iteration was received.\n    :param last_iter_time: The time the last iteration was received.\n    :param request_prompt_tokens: The number of tokens measured in the prompt\n        for the request, if any.\n    :param request_output_tokens: The number of tokens enforced for the output\n        for the request, if any.\n    :param response_prompt_tokens: The number of tokens measured in the prompt\n        for the response, if any.\n    :param response_output_tokens: The number of tokens measured in the output\n        for the response, if any.\n    :param request_id: The unique identifier for the request, if any.\n    :param error: The error message, if any, returned from making the request.\n    \"\"\"\n\n    value: str\n    request_args: RequestArgs\n    iterations: int = 0\n    start_time: float\n    end_time: float\n    first_iter_time: Optional[float]\n    last_iter_time: Optional[float]\n    request_prompt_tokens: Optional[int] = None\n    request_output_tokens: Optional[int] = None\n    response_prompt_tokens: Optional[int] = None\n    response_output_tokens: Optional[int] = None\n    request_id: Optional[str] = None\n    error: Optional[str] = None\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def prompt_tokens(self) -&gt; Optional[int]:\n        \"\"\"\n        The number of tokens measured in the prompt based on preferences\n        for trusting the input or response.\n\n        :return: The number of tokens in the prompt, if any.\n        \"\"\"\n        if settings.preferred_prompt_tokens_source == \"request\":\n            return self.request_prompt_tokens or self.response_prompt_tokens\n\n        return self.response_prompt_tokens or self.request_prompt_tokens\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def output_tokens(self) -&gt; Optional[int]:\n        \"\"\"\n        The number of tokens measured in the output based on preferences\n        for trusting the input or response.\n\n        :return: The number of tokens in the output, if any.\n        \"\"\"\n        if self.error is not None:\n            # error occurred, can't trust request tokens were all generated\n            return self.response_prompt_tokens\n\n        if settings.preferred_output_tokens_source == \"request\":\n            return self.request_output_tokens or self.response_output_tokens\n\n        return self.response_output_tokens or self.request_output_tokens\n</code></pre>"},{"location":"reference/guidellm/backend/response/#guidellm.backend.response.ResponseSummary.output_tokens","title":"<code>output_tokens</code>  <code>property</code>","text":"<p>The number of tokens measured in the output based on preferences for trusting the input or response.</p> <p>Returns:</p> Type Description <code>Optional[int]</code> <p>The number of tokens in the output, if any.</p>"},{"location":"reference/guidellm/backend/response/#guidellm.backend.response.ResponseSummary.prompt_tokens","title":"<code>prompt_tokens</code>  <code>property</code>","text":"<p>The number of tokens measured in the prompt based on preferences for trusting the input or response.</p> <p>Returns:</p> Type Description <code>Optional[int]</code> <p>The number of tokens in the prompt, if any.</p>"},{"location":"reference/guidellm/backend/response/#guidellm.backend.response.StreamingTextResponse","title":"<code>StreamingTextResponse</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>A model representing the response content for a streaming text request.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <p>The type of the response; either 'start' or 'iter'.</p> required <code>value</code> <p>The value of the response up to this iteration.</p> required <code>start_time</code> <p>The time.time() the request started.</p> required <code>iter_count</code> <p>The iteration count for the response. For 'start' this is 0 and for the first 'iter' it is 1.</p> required <code>delta</code> <p>The text delta added to the response for this stream iteration.</p> required <code>time</code> <p>If 'start', the time.time() the request started. If 'iter', the time.time() the iteration was received.</p> required <code>request_id</code> <p>The unique identifier for the request, if any.</p> required Source code in <code>src/guidellm/backend/response.py</code> <pre><code>class StreamingTextResponse(StandardBaseModel):\n    \"\"\"\n    A model representing the response content for a streaming text request.\n\n    :param type_: The type of the response; either 'start' or 'iter'.\n    :param value: The value of the response up to this iteration.\n    :param start_time: The time.time() the request started.\n    :param iter_count: The iteration count for the response. For 'start' this is 0\n        and for the first 'iter' it is 1.\n    :param delta: The text delta added to the response for this stream iteration.\n    :param time: If 'start', the time.time() the request started.\n        If 'iter', the time.time() the iteration was received.\n    :param request_id: The unique identifier for the request, if any.\n    \"\"\"\n\n    type_: StreamingResponseType\n    value: str\n    start_time: float\n    first_iter_time: Optional[float]\n    iter_count: int\n    delta: str\n    time: float\n    request_id: Optional[str] = None\n</code></pre>"},{"location":"reference/guidellm/benchmark/","title":"guidellm.benchmark","text":""},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.Benchmark","title":"<code>Benchmark</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>The base serializable model representing a benchmark run and its results. Specific benchmarker implementations should extend this model to include additional information or metadata as needed.</p> <p>Note, requests_per_second and request_concurrency are kept at this level and are expected to be populated by the subclass implementation to ensure the logic for Profiles can include more complicated logic for determining what rates and concurrency values to use for subsequent strategies.</p> Source code in <code>src/guidellm/benchmark/benchmark.py</code> <pre><code>class Benchmark(StandardBaseModel):\n    \"\"\"\n    The base serializable model representing a benchmark run and its results.\n    Specific benchmarker implementations should extend this model to include\n    additional information or metadata as needed.\n\n    Note, requests_per_second and request_concurrency are kept at this level\n    and are expected to be populated by the subclass implementation to ensure\n    the logic for Profiles can include more complicated logic for determining\n    what rates and concurrency values to use for subsequent strategies.\n    \"\"\"\n\n    type_: Literal[\"benchmark\"] = \"benchmark\"\n    id_: str = Field(\n        default_factory=lambda: str(uuid.uuid4()),\n        description=\"The unique identifier for the benchmark.\",\n    )\n    run_id: str = Field(\n        description=(\n            \"The unique identifier for the encompasing benchmark run that this \"\n            \"benchmark was a part of.\"\n        )\n    )\n    args: BenchmarkArgs = Field(\n        description=(\n            \"The arguments used to specify how to run the benchmark and collect data.\"\n        )\n    )\n    run_stats: BenchmarkRunStats = Field(\n        description=(\n            \"The process statistics for the entire benchmark run across all requests.\"\n        )\n    )\n    worker: Union[WorkerDescription] = Field(\n        description=(\n            \"The description and specifics for the worker used to resolve requests \"\n            \"for this benchmark.\"\n        ),\n    )\n    request_loader: Union[RequestLoaderDescription] = Field(\n        description=(\n            \"The description and specifics for the request loader used to create \"\n            \"requests for this benchmark.\"\n        ),\n    )\n    extras: dict[str, Any] = Field(\n        description=(\n            \"Any additional information or metadata that was passed for this benchmark.\"\n        )\n    )\n    metrics: BenchmarkMetrics = Field(\n        description=(\n            \"The metrics for the benchmark run represented as a distribution of \"\n            \"various per-request statistics.\"\n        ),\n    )\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.BenchmarkAggregator","title":"<code>BenchmarkAggregator</code>","text":"<p>               Bases: <code>ABC</code>, <code>StandardBaseModel</code>, <code>Generic[BenchmarkT, RequestT, ResponseT]</code></p> <p>A pydantic base class representing the base class for aggregating benchmark results. The purpose is to receive and process results from a Benchmarker as it iterates through a Scheduler for an individual benchmark run. As results are added, lightweight statistics are updated and stored for immediate progress and informational updates to the caller. Once the benchmark run is complete, the <code>compile</code> method is called to finalize the benchmark and return a Benchmark object with all the results and statistics fully calculated.</p> Source code in <code>src/guidellm/benchmark/aggregator.py</code> <pre><code>class BenchmarkAggregator(\n    ABC, StandardBaseModel, Generic[BenchmarkT, RequestT, ResponseT]\n):\n    \"\"\"\n    A pydantic base class representing the base class for aggregating benchmark results.\n    The purpose is to receive and process results from a Benchmarker as it iterates\n    through a Scheduler for an individual benchmark run.\n    As results are added, lightweight statistics are updated and stored for immediate\n    progress and informational updates to the caller.\n    Once the benchmark run is complete, the `compile` method is called to finalize\n    the benchmark and return a Benchmark object with all the results and statistics\n    fully calculated.\n    \"\"\"\n\n    type_: Literal[\"benchmark_aggregator\"] = \"benchmark_aggregator\"\n    run_id: str = Field(\n        description=(\n            \"The unique identifier for the encompasing benchmark run that this \"\n            \"benchmark was a part of.\"\n        )\n    )\n    args: BenchmarkArgs = Field(\n        description=(\n            \"The arguments used to create the benchmark run that this benchmark was \"\n            \"a part of.\"\n        )\n    )\n    worker_description: Union[\n        GenerativeRequestsWorkerDescription, WorkerDescription\n    ] = Field(\n        description=(\n            \"The description and specifics for the worker used to resolve requests \"\n            \"for this benchmark.\"\n        ),\n        discriminator=\"type_\",\n    )\n    request_loader_description: Union[\n        GenerativeRequestLoaderDescription, RequestLoaderDescription\n    ] = Field(\n        description=(\n            \"The description and specifics for the request loader used to create \"\n            \"requests for this benchmark.\"\n        ),\n        discriminator=\"type_\",\n    )\n    extras: dict[str, Any] = Field(\n        description=(\n            \"Any additional information or metadata that was passed for this benchmark.\"\n        )\n    )\n    in_warmup: bool = Field(\n        description=(\n            \"A flag to indicate if the benchmark is currently in the warmup phase.\"\n        ),\n        default=False,\n        exclude=True,\n    )\n    in_cooldown: bool = Field(\n        description=(\n            \"A flag to indicate if the benchmark is currently in the cooldown phase.\"\n        ),\n        default=False,\n        exclude=True,\n    )\n    scheduler_stats: SchedulerRunningStats = Field(\n        description=(\n            \"The running statistics for the scheduler for this benchmark run. \"\n            \"This includes all requests created, regardless of their status.\"\n        ),\n        default_factory=SchedulerRunningStats,\n    )\n    requests_stats: RequestsRunningStats = Field(\n        description=(\n            \"The running statistics for the requests for this benchmark run. \"\n            \"This includes all requests created, regardless of their status.\"\n        ),\n        default_factory=RequestsRunningStats,\n    )\n    results: StatusBreakdown[\n        list[SchedulerRequestResult[RequestT, ResponseT]],\n        list[SchedulerRequestResult[RequestT, ResponseT]],\n        list[SchedulerRequestResult[RequestT, ResponseT]],\n        None,\n    ] = Field(\n        description=(\n            \"The completed requests for this benchmark run broken down by status\"\n            \"and excluding warmup and cooldown requests.\"\n        ),\n        default_factory=lambda: StatusBreakdown(  # type: ignore[arg-type]\n            successful=[],\n            errored=[],\n            incomplete=[],\n            total=None,\n        ),\n    )\n\n    def add_result(\n        self,\n        result: SchedulerRequestResult[RequestT, ResponseT],\n    ) -&gt; bool:\n        \"\"\"\n        Add a result to the aggregator. This will update the internal statistics\n        and add the result to the list of results if it is not within the warmup or\n        cooldown period.\n\n        :param result: The result to add to the aggregator.\n        :return: True if the result was added, False if it was added because it\n            did not fit within the warmup or cooldown period, was not requested,\n            or is not finished\n        \"\"\"\n        # Add scheduler statistics\n        self.scheduler_stats.created_requests += max(\n            0, result.run_info.created_requests\n        )\n        self.scheduler_stats.queued_requests += max(0, result.run_info.queued_requests)\n        self.scheduler_stats.scheduled_requests += max(\n            0, result.run_info.scheduled_requests\n        )\n        self.scheduler_stats.processing_requests += max(\n            0, result.run_info.processing_requests\n        )\n        self.scheduler_stats.completed_requests += max(\n            0, result.run_info.completed_requests\n        )\n\n        if result.type_ != \"request_complete\" or (\n            result.request_info.canceled and not result.request_info.requested\n        ):\n            # If the result is not completed yet, don't add to the results\n            # If the result was canceled and not started, ignore it\n            return False\n\n        # Add request statistics\n        self.requests_stats.totals.total += 1\n        if result.request_info.canceled:\n            self.requests_stats.totals.incomplete += 1\n        elif result.request_info.errored:\n            self.requests_stats.totals.errored += 1\n        elif result.request_info.completed:\n            self.requests_stats.totals.successful += 1\n        else:\n            raise ValueError(\n                \"Unexpected state: request_info must be either \"\n                \"completed, canceled, or errored. \"\n                f\"Got {result.request_info}\"\n            )\n\n        self.requests_stats.queued_time.update(\n            result.request_info.dequeued_time - result.request_info.queued_time\n        )\n        self.requests_stats.scheduled_time_delay.update(\n            result.request_info.scheduled_time - result.request_info.dequeued_time\n        )\n        sleep_time = max(\n            0.0,\n            result.request_info.targeted_start_time\n            - result.request_info.scheduled_time,\n        )\n        self.requests_stats.scheduled_time_sleep.update(sleep_time)\n        time_to_worker_start = (\n            result.request_info.worker_start - result.request_info.scheduled_time\n        )\n        self.requests_stats.worker_start_delay.update(time_to_worker_start - sleep_time)\n        self.requests_stats.worker_time.update(\n            result.request_info.worker_end - result.request_info.worker_start\n        )\n        self.requests_stats.worker_start_time_targeted_delay.update(\n            result.request_info.worker_start - result.request_info.targeted_start_time\n        )\n        self.requests_stats.request_start_time_delay.update(\n            result.request_info.worker_start - result.request_info.targeted_start_time\n        )\n        self.requests_stats.request_start_time_targeted_delay.update(\n            result.request_info.worker_start - result.request_info.targeted_start_time\n        )\n        self.requests_stats.request_time_delay.update(\n            (result.request_info.worker_end - result.request_info.worker_start)\n            - (result.request_info.worker_end - result.request_info.worker_start)\n        )\n        self.requests_stats.request_time.update(\n            result.request_info.worker_end - result.request_info.worker_start\n        )\n\n        # Add result to the list of results provided we are not in warmup or cooldown\n        total_completed = self.requests_stats.totals.total.total\n        global_start_time = self.requests_stats.totals.total.start_time\n\n        in_warmup_number = (\n            self.args.warmup_number and total_completed &lt;= self.args.warmup_number\n        )\n        in_warmup_duration = (\n            self.args.warmup_duration\n            and result.request_info.worker_start\n            &lt;= (global_start_time - self.args.warmup_duration)\n        )\n\n        if in_warmup_number or in_warmup_duration:\n            self.in_warmup = True\n            return True\n\n        self.in_warmup = False\n        in_cooldown_number = (\n            self.args.cooldown_number\n            and self.args.max_number\n            and total_completed &gt; self.args.max_number - self.args.cooldown_number\n        )\n        in_cooldown_duration = (\n            self.args.cooldown_duration\n            and self.args.max_duration\n            and result.request_info.worker_start\n            &gt; global_start_time + self.args.max_duration - self.args.cooldown_duration\n        )\n\n        if in_cooldown_number or in_cooldown_duration:\n            self.in_cooldown = True\n            return True\n\n        self.in_cooldown = False\n\n        if result.request_info.canceled:\n            self.results.incomplete.append(result)\n        elif result.request_info.errored:\n            self.results.errored.append(result)\n        elif result.request_info.completed:\n            self.results.successful.append(result)\n        else:\n            raise ValueError(\n                \"Unexpected state: request_info must be either \"\n                \"completed, canceled, or errored. \"\n                f\"Got {result.request_info}\"\n            )\n\n        return True\n\n    @abstractmethod\n    def compile(self) -&gt; BenchmarkT:\n        \"\"\"\n        Compile the benchmark results and statistics into a Benchmark object.\n        This is required to be implemented by subclasses to finalize the benchmark\n        and return the compiled object.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.BenchmarkAggregator.add_result","title":"<code>add_result(result)</code>","text":"<p>Add a result to the aggregator. This will update the internal statistics and add the result to the list of results if it is not within the warmup or cooldown period.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>SchedulerRequestResult[RequestT, ResponseT]</code> <p>The result to add to the aggregator.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the result was added, False if it was added because it did not fit within the warmup or cooldown period, was not requested, or is not finished</p> Source code in <code>src/guidellm/benchmark/aggregator.py</code> <pre><code>def add_result(\n    self,\n    result: SchedulerRequestResult[RequestT, ResponseT],\n) -&gt; bool:\n    \"\"\"\n    Add a result to the aggregator. This will update the internal statistics\n    and add the result to the list of results if it is not within the warmup or\n    cooldown period.\n\n    :param result: The result to add to the aggregator.\n    :return: True if the result was added, False if it was added because it\n        did not fit within the warmup or cooldown period, was not requested,\n        or is not finished\n    \"\"\"\n    # Add scheduler statistics\n    self.scheduler_stats.created_requests += max(\n        0, result.run_info.created_requests\n    )\n    self.scheduler_stats.queued_requests += max(0, result.run_info.queued_requests)\n    self.scheduler_stats.scheduled_requests += max(\n        0, result.run_info.scheduled_requests\n    )\n    self.scheduler_stats.processing_requests += max(\n        0, result.run_info.processing_requests\n    )\n    self.scheduler_stats.completed_requests += max(\n        0, result.run_info.completed_requests\n    )\n\n    if result.type_ != \"request_complete\" or (\n        result.request_info.canceled and not result.request_info.requested\n    ):\n        # If the result is not completed yet, don't add to the results\n        # If the result was canceled and not started, ignore it\n        return False\n\n    # Add request statistics\n    self.requests_stats.totals.total += 1\n    if result.request_info.canceled:\n        self.requests_stats.totals.incomplete += 1\n    elif result.request_info.errored:\n        self.requests_stats.totals.errored += 1\n    elif result.request_info.completed:\n        self.requests_stats.totals.successful += 1\n    else:\n        raise ValueError(\n            \"Unexpected state: request_info must be either \"\n            \"completed, canceled, or errored. \"\n            f\"Got {result.request_info}\"\n        )\n\n    self.requests_stats.queued_time.update(\n        result.request_info.dequeued_time - result.request_info.queued_time\n    )\n    self.requests_stats.scheduled_time_delay.update(\n        result.request_info.scheduled_time - result.request_info.dequeued_time\n    )\n    sleep_time = max(\n        0.0,\n        result.request_info.targeted_start_time\n        - result.request_info.scheduled_time,\n    )\n    self.requests_stats.scheduled_time_sleep.update(sleep_time)\n    time_to_worker_start = (\n        result.request_info.worker_start - result.request_info.scheduled_time\n    )\n    self.requests_stats.worker_start_delay.update(time_to_worker_start - sleep_time)\n    self.requests_stats.worker_time.update(\n        result.request_info.worker_end - result.request_info.worker_start\n    )\n    self.requests_stats.worker_start_time_targeted_delay.update(\n        result.request_info.worker_start - result.request_info.targeted_start_time\n    )\n    self.requests_stats.request_start_time_delay.update(\n        result.request_info.worker_start - result.request_info.targeted_start_time\n    )\n    self.requests_stats.request_start_time_targeted_delay.update(\n        result.request_info.worker_start - result.request_info.targeted_start_time\n    )\n    self.requests_stats.request_time_delay.update(\n        (result.request_info.worker_end - result.request_info.worker_start)\n        - (result.request_info.worker_end - result.request_info.worker_start)\n    )\n    self.requests_stats.request_time.update(\n        result.request_info.worker_end - result.request_info.worker_start\n    )\n\n    # Add result to the list of results provided we are not in warmup or cooldown\n    total_completed = self.requests_stats.totals.total.total\n    global_start_time = self.requests_stats.totals.total.start_time\n\n    in_warmup_number = (\n        self.args.warmup_number and total_completed &lt;= self.args.warmup_number\n    )\n    in_warmup_duration = (\n        self.args.warmup_duration\n        and result.request_info.worker_start\n        &lt;= (global_start_time - self.args.warmup_duration)\n    )\n\n    if in_warmup_number or in_warmup_duration:\n        self.in_warmup = True\n        return True\n\n    self.in_warmup = False\n    in_cooldown_number = (\n        self.args.cooldown_number\n        and self.args.max_number\n        and total_completed &gt; self.args.max_number - self.args.cooldown_number\n    )\n    in_cooldown_duration = (\n        self.args.cooldown_duration\n        and self.args.max_duration\n        and result.request_info.worker_start\n        &gt; global_start_time + self.args.max_duration - self.args.cooldown_duration\n    )\n\n    if in_cooldown_number or in_cooldown_duration:\n        self.in_cooldown = True\n        return True\n\n    self.in_cooldown = False\n\n    if result.request_info.canceled:\n        self.results.incomplete.append(result)\n    elif result.request_info.errored:\n        self.results.errored.append(result)\n    elif result.request_info.completed:\n        self.results.successful.append(result)\n    else:\n        raise ValueError(\n            \"Unexpected state: request_info must be either \"\n            \"completed, canceled, or errored. \"\n            f\"Got {result.request_info}\"\n        )\n\n    return True\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.BenchmarkAggregator.compile","title":"<code>compile()</code>  <code>abstractmethod</code>","text":"<p>Compile the benchmark results and statistics into a Benchmark object. This is required to be implemented by subclasses to finalize the benchmark and return the compiled object.</p> Source code in <code>src/guidellm/benchmark/aggregator.py</code> <pre><code>@abstractmethod\ndef compile(self) -&gt; BenchmarkT:\n    \"\"\"\n    Compile the benchmark results and statistics into a Benchmark object.\n    This is required to be implemented by subclasses to finalize the benchmark\n    and return the compiled object.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.BenchmarkArgs","title":"<code>BenchmarkArgs</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>A serializable model representing the arguments used to specify a benchmark run and how data was collected for it.</p> Source code in <code>src/guidellm/benchmark/benchmark.py</code> <pre><code>class BenchmarkArgs(StandardBaseModel):\n    \"\"\"\n    A serializable model representing the arguments used to specify a benchmark run\n    and how data was collected for it.\n    \"\"\"\n\n    profile: Union[\n        AsyncProfile,\n        SweepProfile,\n        ConcurrentProfile,\n        ThroughputProfile,\n        SynchronousProfile,\n        Profile,\n    ] = Field(\n        description=(\n            \"The profile used for the entire benchmark run that the strategy for \"\n            \"this benchmark was pulled from.\"\n        ),\n        discriminator=\"type_\",\n    )\n    strategy_index: int = Field(\n        description=(\n            \"The index of the strategy in the profile that was used for this benchmark.\"\n        )\n    )\n    strategy: Union[\n        ConcurrentStrategy,\n        SchedulingStrategy,\n        ThroughputStrategy,\n        SynchronousStrategy,\n        AsyncPoissonStrategy,\n        AsyncConstantStrategy,\n        SchedulingStrategy,\n    ] = Field(\n        description=\"The scheduling strategy used to run this benchmark. \",\n        discriminator=\"type_\",\n    )\n    max_number: Optional[int] = Field(\n        description=\"The maximum number of requests to run for this benchmark, if any.\"\n    )\n    max_duration: Optional[float] = Field(\n        description=\"The maximum duration in seconds to run this benchmark, if any.\"\n    )\n    warmup_number: Optional[int] = Field(\n        description=(\n            \"The number of requests to run for the warmup phase of this benchmark, \"\n            \"if any. These are requests that were not included in the final results.\"\n        )\n    )\n    warmup_duration: Optional[float] = Field(\n        description=(\n            \"The duration in seconds to run for the warmup phase of this benchmark, \"\n            \"if any. These are requests that were not included in the final results.\"\n        )\n    )\n    cooldown_number: Optional[int] = Field(\n        description=(\n            \"The number of requests to run for the cooldown phase of this benchmark, \"\n            \"if any. These are requests that were not included in the final results.\"\n        )\n    )\n    cooldown_duration: Optional[float] = Field(\n        description=(\n            \"The duration in seconds to run for the cooldown phase of this benchmark, \"\n            \"if any. These are requests that were not included in the final results.\"\n        )\n    )\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.BenchmarkMetrics","title":"<code>BenchmarkMetrics</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>A serializable model representing the metrics for a benchmark run.</p> Source code in <code>src/guidellm/benchmark/benchmark.py</code> <pre><code>class BenchmarkMetrics(StandardBaseModel):\n    \"\"\"\n    A serializable model representing the metrics for a benchmark run.\n    \"\"\"\n\n    requests_per_second: StatusDistributionSummary = Field(\n        description=\"The distribution of requests per second for the benchmark.\",\n    )\n    request_concurrency: StatusDistributionSummary = Field(\n        description=\"The distribution of requests concurrency for the benchmark.\",\n    )\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.BenchmarkRunStats","title":"<code>BenchmarkRunStats</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>A serializable model representing the run process statistics for the entire benchmark run across all requests including warmup and cooldown.</p> Source code in <code>src/guidellm/benchmark/benchmark.py</code> <pre><code>class BenchmarkRunStats(StandardBaseModel):\n    \"\"\"\n    A serializable model representing the run process statistics for the\n    entire benchmark run across all requests including warmup and cooldown.\n    \"\"\"\n\n    start_time: float = Field(\n        description=\"The start time of the benchmark run.\",\n    )\n    end_time: float = Field(\n        description=\"The end time of the benchmark run.\",\n    )\n    requests_made: StatusBreakdown[int, int, int, int] = Field(\n        description=(\n            \"The number of requests made for the benchmark run broken down by \"\n            \"status including successful, incomplete, errored, and the sum of all three\"\n        )\n    )\n    queued_time_avg: float = Field(\n        description=(\n            \"The average time spent in the queue for each request in the benchmark \"\n            \"run until it was dequeued by a worker.\"\n        )\n    )\n    scheduled_time_delay_avg: float = Field(\n        description=(\n            \"The average time delay between when a request was dequeued and when it \"\n            \"was scheduled to be processed by a worker in the benchmark run. \"\n            \"This should be as close to 0 as possible, any additional time is \"\n            \"overheads from the system or the worker.\"\n        )\n    )\n    scheduled_time_sleep_avg: float = Field(\n        description=(\n            \"The average time spent sleeping til the desired start time was reached \"\n            \"after being scheduled by the worker in the benchmark run.\"\n        )\n    )\n    worker_start_delay_avg: float = Field(\n        description=(\n            \"The average time delay between when a request was scheduled and when \"\n            \"the worker started processing it in the benchmark run. \"\n            \"This should be as close to 0 as possible, any additional time is \"\n            \"overheads from the system or the worker.\"\n        )\n    )\n    worker_time_avg: float = Field(\n        description=(\n            \"The average time taken by the worker to process each request in the \"\n            \"benchmark run. This includes the time to generate the response and \"\n            \"any additional processing time.\"\n        )\n    )\n    worker_start_time_targeted_delay_avg: float = Field(\n        description=(\n            \"The average time delay between when a request was targeted to start \"\n            \"and when the worker actually started processing it in the benchmark \"\n            \"run. For async strategies, this represents delays from the ideal \"\n            \"system. For sync strategies, since those are doubled in queue, \"\n            \"this should be as close to the time for a request to be processed \"\n            \"as possible. Any additional time is overhead from the system or \"\n            \"the worker.\"\n        )\n    )\n    request_start_time_delay_avg: float = Field(\n        description=(\n            \"The average time delay between the actual request being made \"\n            \"and the time the worker started on the request for all requests \"\n            \"that completed within the benchmark run. This time should be as close \"\n            \"to 0 as possible, any additional time is overhead from the system or \"\n            \"the worker.\"\n        )\n    )\n    request_start_time_targeted_delay_avg: float = Field(\n        description=(\n            \"The average time delay between when the targeted start time and \"\n            \"the actual start time for each request in the benchmark run. \"\n            \"For async strategies, this represents delays from the ideal \"\n            \"system. For sync strategies, this should be as close to the \"\n            \"time for a request to be processed as possible. Any additional \"\n            \"time is overhead from the system or the worker.\"\n        )\n    )\n    request_time_delay_avg: float = Field(\n        description=(\n            \"The average time delay between the total request time and the \"\n            \"worker time. This should be as close to 0 as possible, any additional \"\n            \"time is overhead from the system or the worker. \"\n        )\n    )\n    request_time_avg: float = Field(\n        description=(\n            \"The average time spent processing all requests in the benchmark run. \"\n            \"This is the time from when the actual request was started to when \"\n            \"it was completed.\"\n        )\n    )\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeBenchmark","title":"<code>GenerativeBenchmark</code>","text":"<p>               Bases: <code>Benchmark</code></p> <p>A serializable model representing a benchmark run and its results for generative requests and responses. Includes the completed and errored requests, the start and end times for the benchmark, and the statistics for the requests and responses.</p> Source code in <code>src/guidellm/benchmark/benchmark.py</code> <pre><code>class GenerativeBenchmark(Benchmark):\n    \"\"\"\n    A serializable model representing a benchmark run and its results for generative\n    requests and responses. Includes the completed and errored requests, the start\n    and end times for the benchmark, and the statistics for the requests and responses.\n    \"\"\"\n\n    type_: Literal[\"generative_benchmark\"] = \"generative_benchmark\"  # type: ignore[assignment]\n    start_time: float = Field(\n        description=\"The start time of the first request for the benchmark.\",\n    )\n    end_time: float = Field(\n        description=\"The end time of the last request for the benchmark.\",\n    )\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def duration(self) -&gt; float:\n        \"\"\"\n        :return: The duration of the benchmark in seconds from the start of the\n            first request to the end of the last request.\n        \"\"\"\n        return self.end_time - self.start_time\n\n    worker: GenerativeRequestsWorkerDescription = Field(\n        description=(\n            \"The description and specifics for the worker used to resolve requests \"\n            \"for this benchmark.\"\n        ),\n    )\n    request_loader: GenerativeRequestLoaderDescription = Field(\n        description=(\n            \"The description and specifics for the request loader used to create \"\n            \"requests for this benchmark.\"\n        ),\n    )\n    metrics: GenerativeMetrics = Field(\n        description=(\n            \"The metrics for the benchmark run represented as a distribution of \"\n            \"various per-request statistics.\"\n        ),\n    )\n    # Output is ordered so keep the requests at the end for better readability in files\n    request_totals: StatusBreakdown[int, int, int, int] = Field(\n        description=(\n            \"The number of requests made for the benchmark broken down by status \"\n            \"including successful, incomplete, errored, and the sum of all three\"\n        )\n    )\n    request_samples: Optional[StatusBreakdown[int, int, int, None]] = Field(\n        description=(\n            \"The number of requests that were randomly sampled for \"\n            \"the benchmark. None if no sampling was applied.\"\n        ),\n        default=None,\n    )\n    requests: StatusBreakdown[\n        list[GenerativeTextResponseStats],\n        list[GenerativeTextErrorStats],\n        list[GenerativeTextErrorStats],\n        None,\n    ] = Field(\n        description=(\n            \"The breakdown of requests for the benchmark run including successful, \"\n            \"incomplete, and errored requests.\"\n        ),\n    )\n\n    def set_sample_size(self, sample_size: Optional[int]) -&gt; \"GenerativeBenchmark\":\n        \"\"\"\n        Set the sample size for the benchmark. This will randomly sample the\n        requests for each status type to the given sample size or the maximum\n        number of requests for that status type, whichever is smaller.\n        This is applied to requests.successful, requests.errored, and\n        requests.incomplete.\n        If None, no sampling is applied and the state is kept.\n\n        :param sample_size: The number of requests to sample for each status type.\n        :return: The benchmark with the sampled requests.\n        :raises ValueError: If the sample size is invalid.\n        \"\"\"\n\n        if sample_size is not None:\n            if sample_size &lt; 0 or not isinstance(sample_size, int):\n                raise ValueError(\n                    f\"Sample size must be non-negative integer, given {sample_size}\"\n                )\n\n            sample_size = min(sample_size, len(self.requests.successful))\n            error_sample_size = min(sample_size, len(self.requests.errored))\n            incomplete_sample_size = min(sample_size, len(self.requests.incomplete))\n\n            self.requests.successful = random.sample(\n                self.requests.successful, sample_size\n            )\n            self.requests.errored = random.sample(\n                self.requests.errored, error_sample_size\n            )\n            self.requests.incomplete = random.sample(\n                self.requests.incomplete, incomplete_sample_size\n            )\n            self.request_samples = StatusBreakdown(\n                successful=len(self.requests.successful),\n                incomplete=len(self.requests.incomplete),\n                errored=len(self.requests.errored),\n            )\n\n        return self\n\n    @staticmethod\n    def from_stats(\n        run_id: str,\n        successful: list[GenerativeTextResponseStats],\n        incomplete: list[GenerativeTextErrorStats],\n        errored: list[GenerativeTextErrorStats],\n        args: BenchmarkArgs,\n        run_stats: BenchmarkRunStats,\n        worker: GenerativeRequestsWorkerDescription,\n        requests_loader: GenerativeRequestLoaderDescription,\n        extras: Optional[dict[str, Any]],\n    ) -&gt; \"GenerativeBenchmark\":\n        \"\"\"\n        Create a GenerativeBenchmark instance from the given statistics and metadata.\n        Given the completed and errored requests, the benchmark will fill in the\n        remaining statistics for the various metrics required for a benchmark.\n        This is the preferred method for creating a GenerativeBenchmark instance\n        to ensure all statistics are properly calculated and populated.\n\n        :param run_id: The unique identifier for the benchmark run.\n        :param completed: The list of completed requests.\n        :param errored: The list of errored requests.\n        :param args: The arguments used to specify how to run the benchmark\n            and collect data.\n        :param run_stats: The process statistics for the entire benchmark run across\n            all requests.\n        :param worker: The description and specifics for the worker used to resolve\n            requests.\n        :param requests_loader: The description and specifics for the request loader\n            used to create requests.\n        :param extras: Any additional information or metadata that was passed for\n            this benchmark.\n        :return: A GenerativeBenchmark instance with the given statistics and metadata\n            populated and calculated\n        \"\"\"\n        total = successful + incomplete + errored\n        total_types: list[Literal[\"successful\", \"incomplete\", \"error\"]] = [\n            *[\"successful\"] * len(successful),  # type: ignore[list-item]\n            *[\"incomplete\"] * len(incomplete),  # type: ignore[list-item]\n            *[\"error\"] * len(errored),  # type: ignore[list-item]\n        ]\n        start_time = min(req.start_time for req in total)\n        end_time = max(req.end_time for req in total)\n\n        total_with_prompt, total_types_with_prompt = (\n            zip(*filtered)\n            if (\n                filtered := list(\n                    filter(lambda val: bool(val[0].prompt), zip(total, total_types))\n                )\n            )\n            else ([], [])\n        )\n        total_with_output_first, total_types_with_output_first = (\n            zip(*filtered)\n            if (\n                filtered := list(\n                    filter(\n                        lambda val: bool(val[0].output_tokens &gt; 0),\n                        zip(total, total_types),\n                    )\n                )\n            )\n            else ([], [])\n        )\n        total_with_output_multi, total_types_with_output_multi = (\n            zip(*filtered)\n            if (\n                filtered := list(\n                    filter(\n                        lambda val: bool(val[0].output_tokens &gt; 1),\n                        zip(total, total_types),\n                    )\n                )\n            )\n            else ([], [])\n        )\n\n        return GenerativeBenchmark(\n            run_id=run_id,\n            args=args,\n            run_stats=run_stats,\n            extras=extras or {},\n            start_time=start_time,\n            end_time=end_time,\n            worker=worker,\n            request_loader=requests_loader,\n            metrics=GenerativeMetrics(\n                requests_per_second=StatusDistributionSummary.from_request_times(\n                    request_types=total_types,\n                    requests=[(req.start_time, req.end_time) for req in total],\n                    distribution_type=\"rate\",\n                ),\n                request_concurrency=StatusDistributionSummary.from_request_times(\n                    request_types=total_types,\n                    requests=[(req.start_time, req.end_time) for req in total],\n                    distribution_type=\"concurrency\",\n                ),\n                request_latency=StatusDistributionSummary.from_values(\n                    value_types=total_types,\n                    values=[req.request_latency for req in total],\n                ),\n                prompt_token_count=StatusDistributionSummary.from_values(\n                    value_types=list(total_types_with_prompt),\n                    values=[req.prompt_tokens for req in total_with_prompt],\n                ),\n                output_token_count=StatusDistributionSummary.from_values(\n                    value_types=list(total_types_with_output_first),\n                    values=[req.output_tokens for req in total_with_output_first],\n                ),\n                time_to_first_token_ms=StatusDistributionSummary.from_values(\n                    value_types=list(total_types_with_output_first),\n                    values=[\n                        req.time_to_first_token_ms or 0\n                        for req in total_with_output_first\n                    ],\n                ),\n                time_per_output_token_ms=StatusDistributionSummary.from_values(\n                    value_types=list(total_types_with_output_first),\n                    values=[\n                        req.time_per_output_token_ms or 0\n                        for req in total_with_output_first\n                    ],\n                    weights=[req.output_tokens for req in total_with_output_first],\n                ),\n                inter_token_latency_ms=StatusDistributionSummary.from_values(\n                    value_types=list(total_types_with_output_multi),\n                    values=[\n                        req.inter_token_latency_ms or 0\n                        for req in total_with_output_multi\n                    ],\n                    weights=[req.output_tokens - 1 for req in total_with_output_multi],\n                ),\n                output_tokens_per_second=StatusDistributionSummary.from_iterable_request_times(\n                    request_types=list(total_types_with_output_first),\n                    requests=[\n                        (req.start_time, req.end_time)\n                        for req in total_with_output_first\n                    ],\n                    first_iter_times=[\n                        req.first_token_time or req.start_time\n                        for req in total_with_output_first\n                    ],\n                    iter_counts=[req.output_tokens for req in total_with_output_first],\n                ),\n                tokens_per_second=StatusDistributionSummary.from_iterable_request_times(\n                    request_types=list(total_types_with_output_first),\n                    requests=[\n                        (req.start_time, req.end_time)\n                        for req in total_with_output_first\n                    ],\n                    first_iter_times=[\n                        req.first_token_time or req.start_time\n                        for req in total_with_output_first\n                    ],\n                    iter_counts=[\n                        req.prompt_tokens + req.output_tokens\n                        for req in total_with_output_first\n                    ],\n                    first_iter_counts=[\n                        req.prompt_tokens for req in total_with_output_first\n                    ],\n                ),\n            ),\n            request_totals=StatusBreakdown(\n                successful=len(successful),\n                incomplete=len(incomplete),\n                errored=len(errored),\n                total=len(total),\n            ),\n            requests=StatusBreakdown(\n                successful=successful,\n                incomplete=incomplete,\n                errored=errored,\n            ),\n        )\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeBenchmark.duration","title":"<code>duration</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>float</code> <p>The duration of the benchmark in seconds from the start of the first request to the end of the last request.</p>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeBenchmark.from_stats","title":"<code>from_stats(run_id, successful, incomplete, errored, args, run_stats, worker, requests_loader, extras)</code>  <code>staticmethod</code>","text":"<p>Create a GenerativeBenchmark instance from the given statistics and metadata. Given the completed and errored requests, the benchmark will fill in the remaining statistics for the various metrics required for a benchmark. This is the preferred method for creating a GenerativeBenchmark instance to ensure all statistics are properly calculated and populated.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>The unique identifier for the benchmark run.</p> required <code>completed</code> <p>The list of completed requests.</p> required <code>errored</code> <code>list[GenerativeTextErrorStats]</code> <p>The list of errored requests.</p> required <code>args</code> <code>BenchmarkArgs</code> <p>The arguments used to specify how to run the benchmark and collect data.</p> required <code>run_stats</code> <code>BenchmarkRunStats</code> <p>The process statistics for the entire benchmark run across all requests.</p> required <code>worker</code> <code>GenerativeRequestsWorkerDescription</code> <p>The description and specifics for the worker used to resolve requests.</p> required <code>requests_loader</code> <code>GenerativeRequestLoaderDescription</code> <p>The description and specifics for the request loader used to create requests.</p> required <code>extras</code> <code>Optional[dict[str, Any]]</code> <p>Any additional information or metadata that was passed for this benchmark.</p> required <p>Returns:</p> Type Description <code>GenerativeBenchmark</code> <p>A GenerativeBenchmark instance with the given statistics and metadata populated and calculated</p> Source code in <code>src/guidellm/benchmark/benchmark.py</code> <pre><code>@staticmethod\ndef from_stats(\n    run_id: str,\n    successful: list[GenerativeTextResponseStats],\n    incomplete: list[GenerativeTextErrorStats],\n    errored: list[GenerativeTextErrorStats],\n    args: BenchmarkArgs,\n    run_stats: BenchmarkRunStats,\n    worker: GenerativeRequestsWorkerDescription,\n    requests_loader: GenerativeRequestLoaderDescription,\n    extras: Optional[dict[str, Any]],\n) -&gt; \"GenerativeBenchmark\":\n    \"\"\"\n    Create a GenerativeBenchmark instance from the given statistics and metadata.\n    Given the completed and errored requests, the benchmark will fill in the\n    remaining statistics for the various metrics required for a benchmark.\n    This is the preferred method for creating a GenerativeBenchmark instance\n    to ensure all statistics are properly calculated and populated.\n\n    :param run_id: The unique identifier for the benchmark run.\n    :param completed: The list of completed requests.\n    :param errored: The list of errored requests.\n    :param args: The arguments used to specify how to run the benchmark\n        and collect data.\n    :param run_stats: The process statistics for the entire benchmark run across\n        all requests.\n    :param worker: The description and specifics for the worker used to resolve\n        requests.\n    :param requests_loader: The description and specifics for the request loader\n        used to create requests.\n    :param extras: Any additional information or metadata that was passed for\n        this benchmark.\n    :return: A GenerativeBenchmark instance with the given statistics and metadata\n        populated and calculated\n    \"\"\"\n    total = successful + incomplete + errored\n    total_types: list[Literal[\"successful\", \"incomplete\", \"error\"]] = [\n        *[\"successful\"] * len(successful),  # type: ignore[list-item]\n        *[\"incomplete\"] * len(incomplete),  # type: ignore[list-item]\n        *[\"error\"] * len(errored),  # type: ignore[list-item]\n    ]\n    start_time = min(req.start_time for req in total)\n    end_time = max(req.end_time for req in total)\n\n    total_with_prompt, total_types_with_prompt = (\n        zip(*filtered)\n        if (\n            filtered := list(\n                filter(lambda val: bool(val[0].prompt), zip(total, total_types))\n            )\n        )\n        else ([], [])\n    )\n    total_with_output_first, total_types_with_output_first = (\n        zip(*filtered)\n        if (\n            filtered := list(\n                filter(\n                    lambda val: bool(val[0].output_tokens &gt; 0),\n                    zip(total, total_types),\n                )\n            )\n        )\n        else ([], [])\n    )\n    total_with_output_multi, total_types_with_output_multi = (\n        zip(*filtered)\n        if (\n            filtered := list(\n                filter(\n                    lambda val: bool(val[0].output_tokens &gt; 1),\n                    zip(total, total_types),\n                )\n            )\n        )\n        else ([], [])\n    )\n\n    return GenerativeBenchmark(\n        run_id=run_id,\n        args=args,\n        run_stats=run_stats,\n        extras=extras or {},\n        start_time=start_time,\n        end_time=end_time,\n        worker=worker,\n        request_loader=requests_loader,\n        metrics=GenerativeMetrics(\n            requests_per_second=StatusDistributionSummary.from_request_times(\n                request_types=total_types,\n                requests=[(req.start_time, req.end_time) for req in total],\n                distribution_type=\"rate\",\n            ),\n            request_concurrency=StatusDistributionSummary.from_request_times(\n                request_types=total_types,\n                requests=[(req.start_time, req.end_time) for req in total],\n                distribution_type=\"concurrency\",\n            ),\n            request_latency=StatusDistributionSummary.from_values(\n                value_types=total_types,\n                values=[req.request_latency for req in total],\n            ),\n            prompt_token_count=StatusDistributionSummary.from_values(\n                value_types=list(total_types_with_prompt),\n                values=[req.prompt_tokens for req in total_with_prompt],\n            ),\n            output_token_count=StatusDistributionSummary.from_values(\n                value_types=list(total_types_with_output_first),\n                values=[req.output_tokens for req in total_with_output_first],\n            ),\n            time_to_first_token_ms=StatusDistributionSummary.from_values(\n                value_types=list(total_types_with_output_first),\n                values=[\n                    req.time_to_first_token_ms or 0\n                    for req in total_with_output_first\n                ],\n            ),\n            time_per_output_token_ms=StatusDistributionSummary.from_values(\n                value_types=list(total_types_with_output_first),\n                values=[\n                    req.time_per_output_token_ms or 0\n                    for req in total_with_output_first\n                ],\n                weights=[req.output_tokens for req in total_with_output_first],\n            ),\n            inter_token_latency_ms=StatusDistributionSummary.from_values(\n                value_types=list(total_types_with_output_multi),\n                values=[\n                    req.inter_token_latency_ms or 0\n                    for req in total_with_output_multi\n                ],\n                weights=[req.output_tokens - 1 for req in total_with_output_multi],\n            ),\n            output_tokens_per_second=StatusDistributionSummary.from_iterable_request_times(\n                request_types=list(total_types_with_output_first),\n                requests=[\n                    (req.start_time, req.end_time)\n                    for req in total_with_output_first\n                ],\n                first_iter_times=[\n                    req.first_token_time or req.start_time\n                    for req in total_with_output_first\n                ],\n                iter_counts=[req.output_tokens for req in total_with_output_first],\n            ),\n            tokens_per_second=StatusDistributionSummary.from_iterable_request_times(\n                request_types=list(total_types_with_output_first),\n                requests=[\n                    (req.start_time, req.end_time)\n                    for req in total_with_output_first\n                ],\n                first_iter_times=[\n                    req.first_token_time or req.start_time\n                    for req in total_with_output_first\n                ],\n                iter_counts=[\n                    req.prompt_tokens + req.output_tokens\n                    for req in total_with_output_first\n                ],\n                first_iter_counts=[\n                    req.prompt_tokens for req in total_with_output_first\n                ],\n            ),\n        ),\n        request_totals=StatusBreakdown(\n            successful=len(successful),\n            incomplete=len(incomplete),\n            errored=len(errored),\n            total=len(total),\n        ),\n        requests=StatusBreakdown(\n            successful=successful,\n            incomplete=incomplete,\n            errored=errored,\n        ),\n    )\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeBenchmark.set_sample_size","title":"<code>set_sample_size(sample_size)</code>","text":"<p>Set the sample size for the benchmark. This will randomly sample the requests for each status type to the given sample size or the maximum number of requests for that status type, whichever is smaller. This is applied to requests.successful, requests.errored, and requests.incomplete. If None, no sampling is applied and the state is kept.</p> <p>Parameters:</p> Name Type Description Default <code>sample_size</code> <code>Optional[int]</code> <p>The number of requests to sample for each status type.</p> required <p>Returns:</p> Type Description <code>GenerativeBenchmark</code> <p>The benchmark with the sampled requests.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the sample size is invalid.</p> Source code in <code>src/guidellm/benchmark/benchmark.py</code> <pre><code>def set_sample_size(self, sample_size: Optional[int]) -&gt; \"GenerativeBenchmark\":\n    \"\"\"\n    Set the sample size for the benchmark. This will randomly sample the\n    requests for each status type to the given sample size or the maximum\n    number of requests for that status type, whichever is smaller.\n    This is applied to requests.successful, requests.errored, and\n    requests.incomplete.\n    If None, no sampling is applied and the state is kept.\n\n    :param sample_size: The number of requests to sample for each status type.\n    :return: The benchmark with the sampled requests.\n    :raises ValueError: If the sample size is invalid.\n    \"\"\"\n\n    if sample_size is not None:\n        if sample_size &lt; 0 or not isinstance(sample_size, int):\n            raise ValueError(\n                f\"Sample size must be non-negative integer, given {sample_size}\"\n            )\n\n        sample_size = min(sample_size, len(self.requests.successful))\n        error_sample_size = min(sample_size, len(self.requests.errored))\n        incomplete_sample_size = min(sample_size, len(self.requests.incomplete))\n\n        self.requests.successful = random.sample(\n            self.requests.successful, sample_size\n        )\n        self.requests.errored = random.sample(\n            self.requests.errored, error_sample_size\n        )\n        self.requests.incomplete = random.sample(\n            self.requests.incomplete, incomplete_sample_size\n        )\n        self.request_samples = StatusBreakdown(\n            successful=len(self.requests.successful),\n            incomplete=len(self.requests.incomplete),\n            errored=len(self.requests.errored),\n        )\n\n    return self\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeBenchmarkAggregator","title":"<code>GenerativeBenchmarkAggregator</code>","text":"<p>               Bases: <code>BenchmarkAggregator[GenerativeBenchmark, GenerationRequest, ResponseSummary]</code></p> Source code in <code>src/guidellm/benchmark/aggregator.py</code> <pre><code>class GenerativeBenchmarkAggregator(\n    BenchmarkAggregator[GenerativeBenchmark, GenerationRequest, ResponseSummary]\n):\n    type_: Literal[\"generative_benchmark_aggregator\"] = (\n        \"generative_benchmark_aggregator\"  # type: ignore[assignment]\n    )\n    processor: Optional[Union[str, Path, Any]] = Field(\n        description=(\n            \"The tokenizer to use for calculating token counts when none are \"\n            \"avaiable that match the preferred source.\"\n        )\n    )\n    processor_args: Optional[dict[str, Any]] = Field(\n        description=(\n            \"Additional arguments to pass to the tokenizer if it requires \"\n            \"any specific configuration for loading or processing.\"\n        ),\n    )\n    worker_description: GenerativeRequestsWorkerDescription = Field(\n        description=(\n            \"The description and specifics for the worker used to resolve requests \"\n            \"for this benchmark.\"\n        ),\n        discriminator=\"type_\",\n    )\n    request_loader_description: GenerativeRequestLoaderDescription = Field(\n        description=(\n            \"The description and specifics for the request loader used to create \"\n            \"requests for this benchmark.\"\n        ),\n        discriminator=\"type_\",\n    )\n    requests_stats: GenerativeRequestsRunningStats = Field(\n        description=(\n            \"The running statistics for the requests for this benchmark run. \"\n            \"This includes all requests created, regardless of their status.\"\n        ),\n        default_factory=GenerativeRequestsRunningStats,\n    )\n\n    def add_result(\n        self, result: SchedulerRequestResult[GenerationRequest, ResponseSummary]\n    ) -&gt; bool:\n        \"\"\"\n        Add a result to the aggregator. This will update the internal statistics\n        and add the result to the list of results if it is not within the warmup or\n        cooldown period.\n\n        :param result: The result to add to the aggregator.\n        \"\"\"\n        if not super().add_result(result):\n            return False\n\n        if result.request is None:\n            raise ValueError(\"Request is None, cannot add result.\")\n\n        if result.response is None:\n            raise ValueError(\"Response is None, cannot add result.\")\n\n        self.requests_stats.request_start_time_delay.update(\n            result.response.start_time - result.request_info.worker_start\n        )\n        self.requests_stats.request_start_time_targeted_delay.update(\n            result.response.start_time - result.request_info.targeted_start_time\n        )\n        self.requests_stats.request_time_delay.update(\n            (result.response.start_time - result.request_info.worker_start)\n            + result.request_info.worker_end\n            - result.response.end_time\n        )\n        self.requests_stats.request_time.update(\n            result.response.end_time - result.response.start_time\n        )\n        if result.response.first_iter_time:\n            self.requests_stats.time_to_first_token.update(\n                result.response.first_iter_time - result.response.start_time\n            )\n        if result.response.last_iter_time and result.response.first_iter_time:\n            self.requests_stats.inter_token_latency.update(\n                result.response.last_iter_time - result.response.first_iter_time,\n                count=(result.response.output_tokens or 1) - 1,\n            )\n        self.requests_stats.prompt_tokens += result.response.request_prompt_tokens or 0\n        self.requests_stats.output_tokens += result.response.request_output_tokens or 0\n        total_tokens = (result.response.request_prompt_tokens or 0) + (\n            result.response.request_output_tokens or 0\n        )\n        self.requests_stats.total_tokens += total_tokens\n\n        return True\n\n    def compile(self) -&gt; GenerativeBenchmark:\n        \"\"\"\n        Compile the benchmark results and statistics into a GenerativeBenchmark object.\n        This is required to be implemented by subclasses to finalize the benchmark\n        and return the compiled object.\n        \"\"\"\n        successful, incomplete, errored = self._compile_results()\n\n        return GenerativeBenchmark.from_stats(\n            run_id=self.run_id,\n            successful=successful,\n            incomplete=incomplete,\n            errored=errored,\n            args=self.args,\n            run_stats=BenchmarkRunStats(\n                start_time=self.requests_stats.totals.total.start_time,\n                end_time=time.time(),\n                requests_made=StatusBreakdown(\n                    successful=int(self.requests_stats.totals.successful.total),\n                    errored=int(self.requests_stats.totals.errored.total),\n                    incomplete=int(self.requests_stats.totals.incomplete.total),\n                    total=int(self.requests_stats.totals.total.total),\n                ),\n                queued_time_avg=self.requests_stats.queued_time.mean,\n                scheduled_time_delay_avg=self.requests_stats.scheduled_time_delay.mean,\n                scheduled_time_sleep_avg=self.requests_stats.scheduled_time_sleep.mean,\n                worker_start_delay_avg=self.requests_stats.worker_start_delay.mean,\n                worker_time_avg=self.requests_stats.worker_time.mean,\n                worker_start_time_targeted_delay_avg=self.requests_stats.worker_start_time_targeted_delay.mean,\n                request_start_time_delay_avg=self.requests_stats.request_start_time_delay.mean,\n                request_start_time_targeted_delay_avg=self.requests_stats.request_start_time_targeted_delay.mean,\n                request_time_delay_avg=self.requests_stats.request_time_delay.mean,\n                request_time_avg=self.requests_stats.request_time.mean,\n            ),\n            worker=self.worker_description,\n            requests_loader=self.request_loader_description,\n            extras=self.extras,\n        )\n\n    def _compile_results(\n        self,\n    ) -&gt; tuple[\n        list[GenerativeTextResponseStats],\n        list[GenerativeTextErrorStats],\n        list[GenerativeTextErrorStats],\n    ]:\n        successful: list[GenerativeTextResponseStats] = [\n            GenerativeTextResponseStats(\n                request_id=result.request.request_id,\n                request_type=result.request.request_type,\n                scheduler_info=result.request_info,\n                prompt=str(result.request.content),\n                prompt_tokens=self._compile_tokens_count(\n                    value=str(result.request.content),\n                    requests_tokens=result.response.request_prompt_tokens,\n                    response_tokens=result.response.response_prompt_tokens,\n                    preferred_tokens_source=settings.preferred_prompt_tokens_source,\n                    errored=False,\n                ),\n                output=result.response.value,\n                output_tokens=self._compile_tokens_count(\n                    value=result.response.value,\n                    requests_tokens=result.response.request_output_tokens,\n                    response_tokens=result.response.response_output_tokens,\n                    preferred_tokens_source=settings.preferred_output_tokens_source,\n                    errored=False,\n                ),\n                start_time=result.response.start_time,\n                end_time=result.response.end_time,\n                first_token_time=result.response.first_iter_time or -1.0,\n                last_token_time=result.response.last_iter_time or -1.0,\n            )\n            for result in self.results.successful\n            if result.request and result.response\n        ]\n        incomplete: list[GenerativeTextErrorStats] = [\n            GenerativeTextErrorStats(\n                error=result.response.error or \"\",\n                request_id=result.request.request_id,\n                request_type=result.request.request_type,\n                scheduler_info=result.request_info,\n                prompt=str(result.request.content),\n                prompt_tokens=self._compile_tokens_count(\n                    value=str(result.request.content),\n                    requests_tokens=result.response.request_prompt_tokens,\n                    response_tokens=result.response.response_prompt_tokens,\n                    preferred_tokens_source=settings.preferred_prompt_tokens_source,\n                    errored=True,\n                ),\n                output=result.response.value,\n                output_tokens=self._compile_tokens_count(\n                    value=result.response.value,\n                    requests_tokens=result.response.request_output_tokens,\n                    response_tokens=result.response.response_output_tokens,\n                    preferred_tokens_source=settings.preferred_output_tokens_source,\n                    errored=True,\n                ),\n                start_time=result.response.start_time,\n                end_time=result.response.end_time,\n                first_token_time=result.response.first_iter_time,\n                last_token_time=result.response.last_iter_time,\n            )\n            for result in self.results.incomplete\n            if result.request and result.response\n        ]\n        error: list[GenerativeTextErrorStats] = [\n            GenerativeTextErrorStats(\n                error=result.response.error or \"\",\n                request_id=result.request.request_id,\n                request_type=result.request.request_type,\n                scheduler_info=result.request_info,\n                prompt=str(result.request.content),\n                prompt_tokens=self._compile_tokens_count(\n                    value=str(result.request.content),\n                    requests_tokens=result.response.request_prompt_tokens,\n                    response_tokens=result.response.response_prompt_tokens,\n                    preferred_tokens_source=settings.preferred_prompt_tokens_source,\n                    errored=True,\n                ),\n                output=result.response.value,\n                output_tokens=self._compile_tokens_count(\n                    value=result.response.value,\n                    requests_tokens=result.response.request_output_tokens,\n                    response_tokens=result.response.response_output_tokens,\n                    preferred_tokens_source=settings.preferred_output_tokens_source,\n                    errored=True,\n                ),\n                start_time=result.response.start_time,\n                end_time=result.response.end_time,\n                first_token_time=result.response.first_iter_time,\n                last_token_time=result.response.last_iter_time,\n            )\n            for result in self.results.errored\n            if result.request and result.response\n        ]\n\n        return successful, incomplete, error\n\n    def _compile_tokens_count(\n        self,\n        value: str,\n        requests_tokens: Optional[int],\n        response_tokens: Optional[int],\n        preferred_tokens_source: Optional[Literal[\"request\", \"response\", \"local\"]],\n        errored: bool,\n    ) -&gt; int:\n        if not errored and preferred_tokens_source == \"response\" and response_tokens:\n            return response_tokens or 0\n\n        if not errored and preferred_tokens_source == \"request\" and requests_tokens:\n            return requests_tokens or 0\n\n        if preferred_tokens_source in {\"response\", \"request\"} and (\n            self.processor is None or errored or response_tokens or requests_tokens\n        ):\n            # we had a preferred tokens source that isn't local and we either\n            # have the data to return something or we don't have the ability\n            # to calculate locally\n            return response_tokens or requests_tokens or 0\n\n        self.processor = check_load_processor(\n            self.processor,\n            processor_args=self.processor_args,\n            error_msg=\"Processor/Tokenizer is required for calculating token counts.\",\n        )\n        return len(self.processor.tokenize(value))\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeBenchmarkAggregator.add_result","title":"<code>add_result(result)</code>","text":"<p>Add a result to the aggregator. This will update the internal statistics and add the result to the list of results if it is not within the warmup or cooldown period.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>SchedulerRequestResult[GenerationRequest, ResponseSummary]</code> <p>The result to add to the aggregator.</p> required Source code in <code>src/guidellm/benchmark/aggregator.py</code> <pre><code>def add_result(\n    self, result: SchedulerRequestResult[GenerationRequest, ResponseSummary]\n) -&gt; bool:\n    \"\"\"\n    Add a result to the aggregator. This will update the internal statistics\n    and add the result to the list of results if it is not within the warmup or\n    cooldown period.\n\n    :param result: The result to add to the aggregator.\n    \"\"\"\n    if not super().add_result(result):\n        return False\n\n    if result.request is None:\n        raise ValueError(\"Request is None, cannot add result.\")\n\n    if result.response is None:\n        raise ValueError(\"Response is None, cannot add result.\")\n\n    self.requests_stats.request_start_time_delay.update(\n        result.response.start_time - result.request_info.worker_start\n    )\n    self.requests_stats.request_start_time_targeted_delay.update(\n        result.response.start_time - result.request_info.targeted_start_time\n    )\n    self.requests_stats.request_time_delay.update(\n        (result.response.start_time - result.request_info.worker_start)\n        + result.request_info.worker_end\n        - result.response.end_time\n    )\n    self.requests_stats.request_time.update(\n        result.response.end_time - result.response.start_time\n    )\n    if result.response.first_iter_time:\n        self.requests_stats.time_to_first_token.update(\n            result.response.first_iter_time - result.response.start_time\n        )\n    if result.response.last_iter_time and result.response.first_iter_time:\n        self.requests_stats.inter_token_latency.update(\n            result.response.last_iter_time - result.response.first_iter_time,\n            count=(result.response.output_tokens or 1) - 1,\n        )\n    self.requests_stats.prompt_tokens += result.response.request_prompt_tokens or 0\n    self.requests_stats.output_tokens += result.response.request_output_tokens or 0\n    total_tokens = (result.response.request_prompt_tokens or 0) + (\n        result.response.request_output_tokens or 0\n    )\n    self.requests_stats.total_tokens += total_tokens\n\n    return True\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeBenchmarkAggregator.compile","title":"<code>compile()</code>","text":"<p>Compile the benchmark results and statistics into a GenerativeBenchmark object. This is required to be implemented by subclasses to finalize the benchmark and return the compiled object.</p> Source code in <code>src/guidellm/benchmark/aggregator.py</code> <pre><code>def compile(self) -&gt; GenerativeBenchmark:\n    \"\"\"\n    Compile the benchmark results and statistics into a GenerativeBenchmark object.\n    This is required to be implemented by subclasses to finalize the benchmark\n    and return the compiled object.\n    \"\"\"\n    successful, incomplete, errored = self._compile_results()\n\n    return GenerativeBenchmark.from_stats(\n        run_id=self.run_id,\n        successful=successful,\n        incomplete=incomplete,\n        errored=errored,\n        args=self.args,\n        run_stats=BenchmarkRunStats(\n            start_time=self.requests_stats.totals.total.start_time,\n            end_time=time.time(),\n            requests_made=StatusBreakdown(\n                successful=int(self.requests_stats.totals.successful.total),\n                errored=int(self.requests_stats.totals.errored.total),\n                incomplete=int(self.requests_stats.totals.incomplete.total),\n                total=int(self.requests_stats.totals.total.total),\n            ),\n            queued_time_avg=self.requests_stats.queued_time.mean,\n            scheduled_time_delay_avg=self.requests_stats.scheduled_time_delay.mean,\n            scheduled_time_sleep_avg=self.requests_stats.scheduled_time_sleep.mean,\n            worker_start_delay_avg=self.requests_stats.worker_start_delay.mean,\n            worker_time_avg=self.requests_stats.worker_time.mean,\n            worker_start_time_targeted_delay_avg=self.requests_stats.worker_start_time_targeted_delay.mean,\n            request_start_time_delay_avg=self.requests_stats.request_start_time_delay.mean,\n            request_start_time_targeted_delay_avg=self.requests_stats.request_start_time_targeted_delay.mean,\n            request_time_delay_avg=self.requests_stats.request_time_delay.mean,\n            request_time_avg=self.requests_stats.request_time.mean,\n        ),\n        worker=self.worker_description,\n        requests_loader=self.request_loader_description,\n        extras=self.extras,\n    )\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeBenchmarksConsole","title":"<code>GenerativeBenchmarksConsole</code>","text":"<p>A class for outputting progress and benchmark results to the console. Utilizes the rich library for formatting, enabling colored and styled output.</p> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>class GenerativeBenchmarksConsole:\n    \"\"\"\n    A class for outputting progress and benchmark results to the console.\n    Utilizes the rich library for formatting, enabling colored and styled output.\n    \"\"\"\n\n    def __init__(self, enabled: bool = True):\n        \"\"\"\n        :param enabled: Whether to enable console output. Defaults to True.\n            If False, all console output will be suppressed.\n        \"\"\"\n        self.enabled = enabled\n        self.benchmarks: Optional[list[GenerativeBenchmark]] = None\n        self.console = Console()\n\n    @property\n    def benchmarks_profile_str(self) -&gt; str:\n        \"\"\"\n        :return: A string representation of the profile used for the benchmarks.\n        \"\"\"\n        profile = self.benchmarks[0].args.profile if self.benchmarks else None\n\n        if profile is None:\n            return \"None\"\n\n        profile_args = OrderedDict(\n            {\n                \"type\": profile.type_,\n                \"strategies\": profile.strategy_types,\n            }\n        )\n\n        if isinstance(profile, ConcurrentProfile):\n            profile_args[\"streams\"] = str(profile.streams)\n        elif isinstance(profile, ThroughputProfile):\n            profile_args[\"max_concurrency\"] = str(profile.max_concurrency)\n        elif isinstance(profile, AsyncProfile):\n            profile_args[\"max_concurrency\"] = str(profile.max_concurrency)\n            profile_args[\"rate\"] = str(profile.rate)\n            profile_args[\"initial_burst\"] = str(profile.initial_burst)\n        elif isinstance(profile, SweepProfile):\n            profile_args[\"sweep_size\"] = str(profile.sweep_size)\n\n        return \", \".join(f\"{key}={value}\" for key, value in profile_args.items())\n\n    @property\n    def benchmarks_args_str(self) -&gt; str:\n        \"\"\"\n        :return: A string representation of the arguments used for the benchmarks.\n        \"\"\"\n        args = self.benchmarks[0].args if self.benchmarks else None\n\n        if args is None:\n            return \"None\"\n\n        args_dict = OrderedDict(\n            {\n                \"max_number\": args.max_number,\n                \"max_duration\": args.max_duration,\n                \"warmup_number\": args.warmup_number,\n                \"warmup_duration\": args.warmup_duration,\n                \"cooldown_number\": args.cooldown_number,\n                \"cooldown_duration\": args.cooldown_duration,\n            }\n        )\n\n        return \", \".join(f\"{key}={value}\" for key, value in args_dict.items())\n\n    @property\n    def benchmarks_worker_desc_str(self) -&gt; str:\n        \"\"\"\n        :return: A string representation of the worker used for the benchmarks.\n        \"\"\"\n        return str(self.benchmarks[0].worker) if self.benchmarks else \"None\"\n\n    @property\n    def benchmarks_request_loader_desc_str(self) -&gt; str:\n        \"\"\"\n        :return: A string representation of the request loader used for the benchmarks.\n        \"\"\"\n        return str(self.benchmarks[0].request_loader) if self.benchmarks else \"None\"\n\n    @property\n    def benchmarks_extras_str(self) -&gt; str:\n        \"\"\"\n        :return: A string representation of the extras used for the benchmarks.\n        \"\"\"\n        extras = self.benchmarks[0].extras if self.benchmarks else None\n\n        if not extras:\n            return \"None\"\n\n        return \", \".join(f\"{key}={value}\" for key, value in extras.items())\n\n    def print_section_header(self, title: str, indent: int = 0, new_lines: int = 2):\n        \"\"\"\n        Print out a styled section header to the console.\n        The title is underlined, bolded, and colored with the INFO color.\n\n        :param title: The title of the section.\n        :param indent: The number of spaces to indent the title.\n            Defaults to 0.\n        :param new_lines: The number of new lines to print before the title.\n            Defaults to 2.\n        \"\"\"\n        self.print_line(\n            value=f\"{title}:\",\n            style=f\"bold underline {Colors.INFO}\",\n            indent=indent,\n            new_lines=new_lines,\n        )\n\n    def print_labeled_line(\n        self, label: str, value: str, indent: int = 4, new_lines: int = 0\n    ):\n        \"\"\"\n        Print out a styled, labeled line (label: value) to the console.\n        The label is bolded and colored with the INFO color,\n        and the value is italicized.\n\n        :param label: The label of the line.\n        :param value: The value of the line.\n        :param indent: The number of spaces to indent the line.\n            Defaults to 4.\n        :param new_lines: The number of new lines to print before the line.\n            Defaults to 0.\n        \"\"\"\n        self.print_line(\n            value=[label + \":\", value],\n            style=[\"bold \" + Colors.INFO, \"italic\"],\n            new_lines=new_lines,\n            indent=indent,\n        )\n\n    def print_line(\n        self,\n        value: Union[str, list[str]],\n        style: Union[str, list[str]] = \"\",\n        indent: int = 0,\n        new_lines: int = 0,\n    ):\n        \"\"\"\n        Print out a a value to the console as a line with optional indentation.\n\n        :param value: The value to print.\n        :param style: The style to apply to the value.\n            Defaults to none.\n        :param indent: The number of spaces to indent the line.\n            Defaults to 0.\n        :param new_lines: The number of new lines to print before the value.\n            Defaults to 0.\n        \"\"\"\n        if not self.enabled:\n            return\n\n        text = Text()\n\n        for _ in range(new_lines):\n            text.append(\"\\n\")\n\n        if not isinstance(value, list):\n            value = [value]\n\n        if not isinstance(style, list):\n            style = [style for _ in range(len(value))]\n\n        if len(value) != len(style):\n            raise ValueError(\n                f\"Value and style length mismatch. Value length: {len(value)}, \"\n                f\"Style length: {len(style)}.\"\n            )\n\n        for val, sty in zip(value, style):\n            text.append(val, style=sty)\n\n        self.console.print(Padding.indent(text, indent))\n\n    def print_table(\n        self,\n        headers: list[str],\n        rows: list[list[Any]],\n        title: str,\n        sections: Optional[dict[str, tuple[int, int]]] = None,\n        max_char_per_col: int = 2**10,\n        indent: int = 0,\n        new_lines: int = 2,\n    ):\n        \"\"\"\n        Print a table to the console with the given headers and rows.\n\n        :param headers: The headers of the table.\n        :param rows: The rows of the table.\n        :param title: The title of the table.\n        :param sections: The sections of the table grouping columns together.\n            This is a mapping of the section display name to a tuple of the start and\n            end column indices. If None, no sections are added (default).\n        :param max_char_per_col: The maximum number of characters per column.\n        :param indent: The number of spaces to indent the table.\n            Defaults to 0.\n        :param new_lines: The number of new lines to print before the table.\n            Defaults to 0.\n        \"\"\"\n\n        if rows and any(len(row) != len(headers) for row in rows):\n            raise ValueError(\n                f\"Headers and rows length mismatch. Headers length: {len(headers)}, \"\n                f\"Row length: {len(rows[0]) if rows else 'N/A'}.\"\n            )\n\n        max_characters_per_column = self.calculate_max_chars_per_column(\n            headers, rows, sections, max_char_per_col\n        )\n\n        self.print_section_header(title, indent=indent, new_lines=new_lines)\n        self.print_table_divider(\n            max_characters_per_column, include_separators=False, indent=indent\n        )\n        if sections:\n            self.print_table_sections(\n                sections, max_characters_per_column, indent=indent\n            )\n        self.print_table_row(\n            split_text_list_by_length(headers, max_characters_per_column),\n            style=f\"bold {Colors.INFO}\",\n            indent=indent,\n        )\n        self.print_table_divider(\n            max_characters_per_column, include_separators=True, indent=indent\n        )\n        for row in rows:\n            self.print_table_row(\n                split_text_list_by_length(row, max_characters_per_column),\n                style=\"italic\",\n                indent=indent,\n            )\n        self.print_table_divider(\n            max_characters_per_column, include_separators=False, indent=indent\n        )\n\n    def calculate_max_chars_per_column(\n        self,\n        headers: list[str],\n        rows: list[list[Any]],\n        sections: Optional[dict[str, tuple[int, int]]],\n        max_char_per_col: int,\n    ) -&gt; list[int]:\n        \"\"\"\n        Calculate the maximum number of characters per column in the table.\n        This is done by checking the length of the headers, rows, and optional sections\n        to ensure all columns are accounted for and spaced correctly.\n\n        :param headers: The headers of the table.\n        :param rows: The rows of the table.\n        :param sections: The sections of the table grouping columns together.\n            This is a mapping of the section display name to a tuple of the start and\n            end column indices. If None, no sections are added (default).\n        :param max_char_per_col: The maximum number of characters per column.\n        :return: A list of the maximum number of characters per column.\n        \"\"\"\n        max_characters_per_column = []\n        for ind in range(len(headers)):\n            max_characters_per_column.append(min(len(headers[ind]), max_char_per_col))\n\n            for row in rows:\n                max_characters_per_column[ind] = max(\n                    max_characters_per_column[ind], len(str(row[ind]))\n                )\n\n        if not sections:\n            return max_characters_per_column\n\n        for section in sections:\n            start_col, end_col = sections[section]\n            min_section_len = len(section) + (\n                end_col - start_col\n            )  # ensure we have enough space for separators\n            chars_in_columns = sum(\n                max_characters_per_column[start_col : end_col + 1]\n            ) + 2 * (end_col - start_col)\n            if min_section_len &gt; chars_in_columns:\n                add_chars_per_col = math.ceil(\n                    (min_section_len - chars_in_columns) / (end_col - start_col + 1)\n                )\n                for col in range(start_col, end_col + 1):\n                    max_characters_per_column[col] += add_chars_per_col\n\n        return max_characters_per_column\n\n    def print_table_divider(\n        self, max_chars_per_column: list[int], include_separators: bool, indent: int = 0\n    ):\n        \"\"\"\n        Print a divider line for the table (top and bottom of table with '=' characters)\n\n        :param max_chars_per_column: The maximum number of characters per column.\n        :param include_separators: Whether to include separators between columns.\n        :param indent: The number of spaces to indent the line.\n            Defaults to 0.\n        \"\"\"\n        if include_separators:\n            columns = [\n                settings.table_headers_border_char * max_chars\n                + settings.table_column_separator_char\n                + settings.table_headers_border_char\n                for max_chars in max_chars_per_column\n            ]\n        else:\n            columns = [\n                settings.table_border_char * (max_chars + 2)\n                for max_chars in max_chars_per_column\n            ]\n\n        columns[-1] = columns[-1][:-2]\n        self.print_line(value=columns, style=Colors.INFO, indent=indent)\n\n    def print_table_sections(\n        self,\n        sections: dict[str, tuple[int, int]],\n        max_chars_per_column: list[int],\n        indent: int = 0,\n    ):\n        \"\"\"\n        Print the sections of the table with corresponding separators to the columns\n        the sections are mapped to to ensure it is compliant with a CSV format.\n        For example, a section named \"Metadata\" with columns 0-3 will print this:\n        Metadata               ,,,,\n        Where the spaces plus the separators at the end will span the columns 0-3.\n        All columns must be accounted for in the sections.\n\n        :param sections: The sections of the table.\n        :param max_chars_per_column: The maximum number of characters per column.\n        :param indent: The number of spaces to indent the line.\n            Defaults to 0.\n        \"\"\"\n        section_tuples = [(start, end, name) for name, (start, end) in sections.items()]\n        section_tuples.sort(key=lambda x: x[0])\n\n        if any(start &gt; end for start, end, _ in section_tuples):\n            raise ValueError(f\"Invalid section ranges: {section_tuples}\")\n\n        if (\n            any(\n                section_tuples[ind][1] + 1 != section_tuples[ind + 1][0]\n                for ind in range(len(section_tuples) - 1)\n            )\n            or section_tuples[0][0] != 0\n            or section_tuples[-1][1] != len(max_chars_per_column) - 1\n        ):\n            raise ValueError(f\"Invalid section ranges: {section_tuples}\")\n\n        line_values = []\n        line_styles = []\n        for section, (start_col, end_col) in sections.items():\n            section_length = sum(max_chars_per_column[start_col : end_col + 1]) + 2 * (\n                end_col - start_col + 1\n            )\n            num_separators = end_col - start_col\n            line_values.append(section)\n            line_styles.append(\"bold \" + Colors.INFO)\n            line_values.append(\n                \" \" * (section_length - len(section) - num_separators - 2)\n            )\n            line_styles.append(\"\")\n            line_values.append(settings.table_column_separator_char * num_separators)\n            line_styles.append(\"\")\n            line_values.append(settings.table_column_separator_char + \" \")\n            line_styles.append(Colors.INFO)\n        line_values = line_values[:-1]\n        line_styles = line_styles[:-1]\n        self.print_line(value=line_values, style=line_styles, indent=indent)\n\n    def print_table_row(\n        self, column_lines: list[list[str]], style: str, indent: int = 0\n    ):\n        \"\"\"\n        Print a single row of a table to the console.\n\n        :param column_lines: The lines of text to print for each column.\n        :param indent: The number of spaces to indent the line.\n            Defaults to 0.\n        \"\"\"\n        for row in range(len(column_lines[0])):\n            print_line = []\n            print_styles = []\n            for column in range(len(column_lines)):\n                print_line.extend(\n                    [\n                        column_lines[column][row],\n                        settings.table_column_separator_char,\n                        \" \",\n                    ]\n                )\n                print_styles.extend([style, Colors.INFO, \"\"])\n            print_line = print_line[:-2]\n            print_styles = print_styles[:-2]\n            self.print_line(value=print_line, style=print_styles, indent=indent)\n\n    def print_benchmarks_metadata(self):\n        \"\"\"\n        Print out the metadata of the benchmarks to the console including the run id,\n        duration, profile, args, worker, request loader, and extras.\n        \"\"\"\n\n        if not self.benchmarks:\n            raise ValueError(\n                \"No benchmarks to print metadata for. Please set benchmarks first.\"\n            )\n\n        start_time = self.benchmarks[0].run_stats.start_time\n        end_time = self.benchmarks[-1].run_stats.end_time\n        duration = end_time - start_time\n\n        self.print_section_header(title=\"Benchmarks Metadata\")\n        self.print_labeled_line(\n            label=\"Run id\",\n            value=str(self.benchmarks[0].run_id),\n        )\n        self.print_labeled_line(\n            label=\"Duration\",\n            value=f\"{duration:.1f} seconds\",\n        )\n        self.print_labeled_line(\n            label=\"Profile\",\n            value=self.benchmarks_profile_str,\n        )\n        self.print_labeled_line(\n            label=\"Args\",\n            value=self.benchmarks_args_str,\n        )\n        self.print_labeled_line(\n            label=\"Worker\",\n            value=self.benchmarks_worker_desc_str,\n        )\n        self.print_labeled_line(\n            label=\"Request Loader\",\n            value=self.benchmarks_request_loader_desc_str,\n        )\n        self.print_labeled_line(\n            label=\"Extras\",\n            value=self.benchmarks_extras_str,\n        )\n\n    def print_benchmarks_info(self):\n        \"\"\"\n        Print out the benchmark information to the console including the start time,\n        end time, duration, request totals, and token totals for each benchmark.\n        \"\"\"\n        if not self.benchmarks:\n            raise ValueError(\n                \"No benchmarks to print info for. Please set benchmarks first.\"\n            )\n\n        sections = {\n            \"Metadata\": (0, 3),\n            \"Requests Made\": (4, 6),\n            \"Prompt Tok/Req\": (7, 9),\n            \"Output Tok/Req\": (10, 12),\n            \"Prompt Tok Total\": (13, 15),\n            \"Output Tok Total\": (16, 18),\n        }\n        headers = [\n            \"Benchmark\",\n            \"Start Time\",\n            \"End Time\",\n            \"Duration (s)\",\n            \"Comp\",\n            \"Inc\",\n            \"Err\",\n            \"Comp\",\n            \"Inc\",\n            \"Err\",\n            \"Comp\",\n            \"Inc\",\n            \"Err\",\n            \"Comp\",\n            \"Inc\",\n            \"Err\",\n            \"Comp\",\n            \"Inc\",\n            \"Err\",\n        ]\n        rows = []\n\n        for benchmark in self.benchmarks:\n            rows.append(\n                [\n                    strategy_display_str(benchmark.args.strategy),\n                    f\"{datetime.fromtimestamp(benchmark.start_time).strftime('%H:%M:%S')}\",\n                    f\"{datetime.fromtimestamp(benchmark.end_time).strftime('%H:%M:%S')}\",\n                    f\"{(benchmark.end_time - benchmark.start_time):.1f}\",\n                    f\"{benchmark.request_totals.successful:.0f}\",\n                    f\"{benchmark.request_totals.incomplete:.0f}\",\n                    f\"{benchmark.request_totals.errored:.0f}\",\n                    f\"{benchmark.metrics.prompt_token_count.successful.mean:.1f}\",\n                    f\"{benchmark.metrics.prompt_token_count.incomplete.mean:.1f}\",\n                    f\"{benchmark.metrics.prompt_token_count.errored.mean:.1f}\",\n                    f\"{benchmark.metrics.output_token_count.successful.mean:.1f}\",\n                    f\"{benchmark.metrics.output_token_count.incomplete.mean:.1f}\",\n                    f\"{benchmark.metrics.output_token_count.errored.mean:.1f}\",\n                    f\"{benchmark.metrics.prompt_token_count.successful.total_sum:.0f}\",\n                    f\"{benchmark.metrics.prompt_token_count.incomplete.total_sum:.0f}\",\n                    f\"{benchmark.metrics.prompt_token_count.errored.total_sum:.0f}\",\n                    f\"{benchmark.metrics.output_token_count.successful.total_sum:.0f}\",\n                    f\"{benchmark.metrics.output_token_count.incomplete.total_sum:.0f}\",\n                    f\"{benchmark.metrics.output_token_count.errored.total_sum:.0f}\",\n                ]\n            )\n\n        self.print_table(\n            headers=headers, rows=rows, title=\"Benchmarks Info\", sections=sections\n        )\n\n    def print_benchmarks_stats(self):\n        \"\"\"\n        Print out the benchmark statistics to the console including the requests per\n        second, request concurrency, output tokens per second, total tokens per second,\n        request latency, time to first token, inter token latency, and time per output\n        token for each benchmark.\n        \"\"\"\n        if not self.benchmarks:\n            raise ValueError(\n                \"No benchmarks to print stats for. Please set benchmarks first.\"\n            )\n\n        sections = {\n            \"Metadata\": (0, 0),\n            \"Request Stats\": (1, 2),\n            \"Out Tok/sec\": (3, 3),\n            \"Tot Tok/sec\": (4, 4),\n            \"Req Latency (sec)\": (5, 7),\n            \"TTFT (ms)\": (8, 10),\n            \"ITL (ms)\": (11, 13),\n            \"TPOT (ms)\": (14, 16),\n        }\n        headers = [\n            \"Benchmark\",\n            \"Per Second\",\n            \"Concurrency\",\n            \"mean\",\n            \"mean\",\n            \"mean\",\n            \"median\",\n            \"p99\",\n            \"mean\",\n            \"median\",\n            \"p99\",\n            \"mean\",\n            \"median\",\n            \"p99\",\n            \"mean\",\n            \"median\",\n            \"p99\",\n        ]\n        rows = []\n\n        for benchmark in self.benchmarks:\n            rows.append(\n                [\n                    strategy_display_str(benchmark.args.strategy),\n                    f\"{benchmark.metrics.requests_per_second.successful.mean:.2f}\",\n                    f\"{benchmark.metrics.request_concurrency.successful.mean:.2f}\",\n                    f\"{benchmark.metrics.output_tokens_per_second.successful.mean:.1f}\",\n                    f\"{benchmark.metrics.tokens_per_second.successful.mean:.1f}\",\n                    f\"{benchmark.metrics.request_latency.successful.mean:.2f}\",\n                    f\"{benchmark.metrics.request_latency.successful.median:.2f}\",\n                    f\"{benchmark.metrics.request_latency.successful.percentiles.p99:.2f}\",\n                    f\"{benchmark.metrics.time_to_first_token_ms.successful.mean:.1f}\",\n                    f\"{benchmark.metrics.time_to_first_token_ms.successful.median:.1f}\",\n                    f\"{benchmark.metrics.time_to_first_token_ms.successful.percentiles.p99:.1f}\",\n                    f\"{benchmark.metrics.inter_token_latency_ms.successful.mean:.1f}\",\n                    f\"{benchmark.metrics.inter_token_latency_ms.successful.median:.1f}\",\n                    f\"{benchmark.metrics.inter_token_latency_ms.successful.percentiles.p99:.1f}\",\n                    f\"{benchmark.metrics.time_per_output_token_ms.successful.mean:.1f}\",\n                    f\"{benchmark.metrics.time_per_output_token_ms.successful.median:.1f}\",\n                    f\"{benchmark.metrics.time_per_output_token_ms.successful.percentiles.p99:.1f}\",\n                ]\n            )\n\n        self.print_table(\n            headers=headers,\n            rows=rows,\n            title=\"Benchmarks Stats\",\n            sections=sections,\n        )\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeBenchmarksConsole.benchmarks_args_str","title":"<code>benchmarks_args_str</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>str</code> <p>A string representation of the arguments used for the benchmarks.</p>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeBenchmarksConsole.benchmarks_extras_str","title":"<code>benchmarks_extras_str</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>str</code> <p>A string representation of the extras used for the benchmarks.</p>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeBenchmarksConsole.benchmarks_profile_str","title":"<code>benchmarks_profile_str</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>str</code> <p>A string representation of the profile used for the benchmarks.</p>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeBenchmarksConsole.benchmarks_request_loader_desc_str","title":"<code>benchmarks_request_loader_desc_str</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>str</code> <p>A string representation of the request loader used for the benchmarks.</p>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeBenchmarksConsole.benchmarks_worker_desc_str","title":"<code>benchmarks_worker_desc_str</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>str</code> <p>A string representation of the worker used for the benchmarks.</p>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeBenchmarksConsole.__init__","title":"<code>__init__(enabled=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>enabled</code> <code>bool</code> <p>Whether to enable console output. Defaults to True. If False, all console output will be suppressed.</p> <code>True</code> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def __init__(self, enabled: bool = True):\n    \"\"\"\n    :param enabled: Whether to enable console output. Defaults to True.\n        If False, all console output will be suppressed.\n    \"\"\"\n    self.enabled = enabled\n    self.benchmarks: Optional[list[GenerativeBenchmark]] = None\n    self.console = Console()\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeBenchmarksConsole.calculate_max_chars_per_column","title":"<code>calculate_max_chars_per_column(headers, rows, sections, max_char_per_col)</code>","text":"<p>Calculate the maximum number of characters per column in the table. This is done by checking the length of the headers, rows, and optional sections to ensure all columns are accounted for and spaced correctly.</p> <p>Parameters:</p> Name Type Description Default <code>headers</code> <code>list[str]</code> <p>The headers of the table.</p> required <code>rows</code> <code>list[list[Any]]</code> <p>The rows of the table.</p> required <code>sections</code> <code>Optional[dict[str, tuple[int, int]]]</code> <p>The sections of the table grouping columns together. This is a mapping of the section display name to a tuple of the start and end column indices. If None, no sections are added (default).</p> required <code>max_char_per_col</code> <code>int</code> <p>The maximum number of characters per column.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>A list of the maximum number of characters per column.</p> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def calculate_max_chars_per_column(\n    self,\n    headers: list[str],\n    rows: list[list[Any]],\n    sections: Optional[dict[str, tuple[int, int]]],\n    max_char_per_col: int,\n) -&gt; list[int]:\n    \"\"\"\n    Calculate the maximum number of characters per column in the table.\n    This is done by checking the length of the headers, rows, and optional sections\n    to ensure all columns are accounted for and spaced correctly.\n\n    :param headers: The headers of the table.\n    :param rows: The rows of the table.\n    :param sections: The sections of the table grouping columns together.\n        This is a mapping of the section display name to a tuple of the start and\n        end column indices. If None, no sections are added (default).\n    :param max_char_per_col: The maximum number of characters per column.\n    :return: A list of the maximum number of characters per column.\n    \"\"\"\n    max_characters_per_column = []\n    for ind in range(len(headers)):\n        max_characters_per_column.append(min(len(headers[ind]), max_char_per_col))\n\n        for row in rows:\n            max_characters_per_column[ind] = max(\n                max_characters_per_column[ind], len(str(row[ind]))\n            )\n\n    if not sections:\n        return max_characters_per_column\n\n    for section in sections:\n        start_col, end_col = sections[section]\n        min_section_len = len(section) + (\n            end_col - start_col\n        )  # ensure we have enough space for separators\n        chars_in_columns = sum(\n            max_characters_per_column[start_col : end_col + 1]\n        ) + 2 * (end_col - start_col)\n        if min_section_len &gt; chars_in_columns:\n            add_chars_per_col = math.ceil(\n                (min_section_len - chars_in_columns) / (end_col - start_col + 1)\n            )\n            for col in range(start_col, end_col + 1):\n                max_characters_per_column[col] += add_chars_per_col\n\n    return max_characters_per_column\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeBenchmarksConsole.print_benchmarks_info","title":"<code>print_benchmarks_info()</code>","text":"<p>Print out the benchmark information to the console including the start time, end time, duration, request totals, and token totals for each benchmark.</p> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def print_benchmarks_info(self):\n    \"\"\"\n    Print out the benchmark information to the console including the start time,\n    end time, duration, request totals, and token totals for each benchmark.\n    \"\"\"\n    if not self.benchmarks:\n        raise ValueError(\n            \"No benchmarks to print info for. Please set benchmarks first.\"\n        )\n\n    sections = {\n        \"Metadata\": (0, 3),\n        \"Requests Made\": (4, 6),\n        \"Prompt Tok/Req\": (7, 9),\n        \"Output Tok/Req\": (10, 12),\n        \"Prompt Tok Total\": (13, 15),\n        \"Output Tok Total\": (16, 18),\n    }\n    headers = [\n        \"Benchmark\",\n        \"Start Time\",\n        \"End Time\",\n        \"Duration (s)\",\n        \"Comp\",\n        \"Inc\",\n        \"Err\",\n        \"Comp\",\n        \"Inc\",\n        \"Err\",\n        \"Comp\",\n        \"Inc\",\n        \"Err\",\n        \"Comp\",\n        \"Inc\",\n        \"Err\",\n        \"Comp\",\n        \"Inc\",\n        \"Err\",\n    ]\n    rows = []\n\n    for benchmark in self.benchmarks:\n        rows.append(\n            [\n                strategy_display_str(benchmark.args.strategy),\n                f\"{datetime.fromtimestamp(benchmark.start_time).strftime('%H:%M:%S')}\",\n                f\"{datetime.fromtimestamp(benchmark.end_time).strftime('%H:%M:%S')}\",\n                f\"{(benchmark.end_time - benchmark.start_time):.1f}\",\n                f\"{benchmark.request_totals.successful:.0f}\",\n                f\"{benchmark.request_totals.incomplete:.0f}\",\n                f\"{benchmark.request_totals.errored:.0f}\",\n                f\"{benchmark.metrics.prompt_token_count.successful.mean:.1f}\",\n                f\"{benchmark.metrics.prompt_token_count.incomplete.mean:.1f}\",\n                f\"{benchmark.metrics.prompt_token_count.errored.mean:.1f}\",\n                f\"{benchmark.metrics.output_token_count.successful.mean:.1f}\",\n                f\"{benchmark.metrics.output_token_count.incomplete.mean:.1f}\",\n                f\"{benchmark.metrics.output_token_count.errored.mean:.1f}\",\n                f\"{benchmark.metrics.prompt_token_count.successful.total_sum:.0f}\",\n                f\"{benchmark.metrics.prompt_token_count.incomplete.total_sum:.0f}\",\n                f\"{benchmark.metrics.prompt_token_count.errored.total_sum:.0f}\",\n                f\"{benchmark.metrics.output_token_count.successful.total_sum:.0f}\",\n                f\"{benchmark.metrics.output_token_count.incomplete.total_sum:.0f}\",\n                f\"{benchmark.metrics.output_token_count.errored.total_sum:.0f}\",\n            ]\n        )\n\n    self.print_table(\n        headers=headers, rows=rows, title=\"Benchmarks Info\", sections=sections\n    )\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeBenchmarksConsole.print_benchmarks_metadata","title":"<code>print_benchmarks_metadata()</code>","text":"<p>Print out the metadata of the benchmarks to the console including the run id, duration, profile, args, worker, request loader, and extras.</p> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def print_benchmarks_metadata(self):\n    \"\"\"\n    Print out the metadata of the benchmarks to the console including the run id,\n    duration, profile, args, worker, request loader, and extras.\n    \"\"\"\n\n    if not self.benchmarks:\n        raise ValueError(\n            \"No benchmarks to print metadata for. Please set benchmarks first.\"\n        )\n\n    start_time = self.benchmarks[0].run_stats.start_time\n    end_time = self.benchmarks[-1].run_stats.end_time\n    duration = end_time - start_time\n\n    self.print_section_header(title=\"Benchmarks Metadata\")\n    self.print_labeled_line(\n        label=\"Run id\",\n        value=str(self.benchmarks[0].run_id),\n    )\n    self.print_labeled_line(\n        label=\"Duration\",\n        value=f\"{duration:.1f} seconds\",\n    )\n    self.print_labeled_line(\n        label=\"Profile\",\n        value=self.benchmarks_profile_str,\n    )\n    self.print_labeled_line(\n        label=\"Args\",\n        value=self.benchmarks_args_str,\n    )\n    self.print_labeled_line(\n        label=\"Worker\",\n        value=self.benchmarks_worker_desc_str,\n    )\n    self.print_labeled_line(\n        label=\"Request Loader\",\n        value=self.benchmarks_request_loader_desc_str,\n    )\n    self.print_labeled_line(\n        label=\"Extras\",\n        value=self.benchmarks_extras_str,\n    )\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeBenchmarksConsole.print_benchmarks_stats","title":"<code>print_benchmarks_stats()</code>","text":"<p>Print out the benchmark statistics to the console including the requests per second, request concurrency, output tokens per second, total tokens per second, request latency, time to first token, inter token latency, and time per output token for each benchmark.</p> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def print_benchmarks_stats(self):\n    \"\"\"\n    Print out the benchmark statistics to the console including the requests per\n    second, request concurrency, output tokens per second, total tokens per second,\n    request latency, time to first token, inter token latency, and time per output\n    token for each benchmark.\n    \"\"\"\n    if not self.benchmarks:\n        raise ValueError(\n            \"No benchmarks to print stats for. Please set benchmarks first.\"\n        )\n\n    sections = {\n        \"Metadata\": (0, 0),\n        \"Request Stats\": (1, 2),\n        \"Out Tok/sec\": (3, 3),\n        \"Tot Tok/sec\": (4, 4),\n        \"Req Latency (sec)\": (5, 7),\n        \"TTFT (ms)\": (8, 10),\n        \"ITL (ms)\": (11, 13),\n        \"TPOT (ms)\": (14, 16),\n    }\n    headers = [\n        \"Benchmark\",\n        \"Per Second\",\n        \"Concurrency\",\n        \"mean\",\n        \"mean\",\n        \"mean\",\n        \"median\",\n        \"p99\",\n        \"mean\",\n        \"median\",\n        \"p99\",\n        \"mean\",\n        \"median\",\n        \"p99\",\n        \"mean\",\n        \"median\",\n        \"p99\",\n    ]\n    rows = []\n\n    for benchmark in self.benchmarks:\n        rows.append(\n            [\n                strategy_display_str(benchmark.args.strategy),\n                f\"{benchmark.metrics.requests_per_second.successful.mean:.2f}\",\n                f\"{benchmark.metrics.request_concurrency.successful.mean:.2f}\",\n                f\"{benchmark.metrics.output_tokens_per_second.successful.mean:.1f}\",\n                f\"{benchmark.metrics.tokens_per_second.successful.mean:.1f}\",\n                f\"{benchmark.metrics.request_latency.successful.mean:.2f}\",\n                f\"{benchmark.metrics.request_latency.successful.median:.2f}\",\n                f\"{benchmark.metrics.request_latency.successful.percentiles.p99:.2f}\",\n                f\"{benchmark.metrics.time_to_first_token_ms.successful.mean:.1f}\",\n                f\"{benchmark.metrics.time_to_first_token_ms.successful.median:.1f}\",\n                f\"{benchmark.metrics.time_to_first_token_ms.successful.percentiles.p99:.1f}\",\n                f\"{benchmark.metrics.inter_token_latency_ms.successful.mean:.1f}\",\n                f\"{benchmark.metrics.inter_token_latency_ms.successful.median:.1f}\",\n                f\"{benchmark.metrics.inter_token_latency_ms.successful.percentiles.p99:.1f}\",\n                f\"{benchmark.metrics.time_per_output_token_ms.successful.mean:.1f}\",\n                f\"{benchmark.metrics.time_per_output_token_ms.successful.median:.1f}\",\n                f\"{benchmark.metrics.time_per_output_token_ms.successful.percentiles.p99:.1f}\",\n            ]\n        )\n\n    self.print_table(\n        headers=headers,\n        rows=rows,\n        title=\"Benchmarks Stats\",\n        sections=sections,\n    )\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeBenchmarksConsole.print_labeled_line","title":"<code>print_labeled_line(label, value, indent=4, new_lines=0)</code>","text":"<p>Print out a styled, labeled line (label: value) to the console. The label is bolded and colored with the INFO color, and the value is italicized.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>str</code> <p>The label of the line.</p> required <code>value</code> <code>str</code> <p>The value of the line.</p> required <code>indent</code> <code>int</code> <p>The number of spaces to indent the line. Defaults to 4.</p> <code>4</code> <code>new_lines</code> <code>int</code> <p>The number of new lines to print before the line. Defaults to 0.</p> <code>0</code> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def print_labeled_line(\n    self, label: str, value: str, indent: int = 4, new_lines: int = 0\n):\n    \"\"\"\n    Print out a styled, labeled line (label: value) to the console.\n    The label is bolded and colored with the INFO color,\n    and the value is italicized.\n\n    :param label: The label of the line.\n    :param value: The value of the line.\n    :param indent: The number of spaces to indent the line.\n        Defaults to 4.\n    :param new_lines: The number of new lines to print before the line.\n        Defaults to 0.\n    \"\"\"\n    self.print_line(\n        value=[label + \":\", value],\n        style=[\"bold \" + Colors.INFO, \"italic\"],\n        new_lines=new_lines,\n        indent=indent,\n    )\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeBenchmarksConsole.print_line","title":"<code>print_line(value, style='', indent=0, new_lines=0)</code>","text":"<p>Print out a a value to the console as a line with optional indentation.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, list[str]]</code> <p>The value to print.</p> required <code>style</code> <code>Union[str, list[str]]</code> <p>The style to apply to the value. Defaults to none.</p> <code>''</code> <code>indent</code> <code>int</code> <p>The number of spaces to indent the line. Defaults to 0.</p> <code>0</code> <code>new_lines</code> <code>int</code> <p>The number of new lines to print before the value. Defaults to 0.</p> <code>0</code> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def print_line(\n    self,\n    value: Union[str, list[str]],\n    style: Union[str, list[str]] = \"\",\n    indent: int = 0,\n    new_lines: int = 0,\n):\n    \"\"\"\n    Print out a a value to the console as a line with optional indentation.\n\n    :param value: The value to print.\n    :param style: The style to apply to the value.\n        Defaults to none.\n    :param indent: The number of spaces to indent the line.\n        Defaults to 0.\n    :param new_lines: The number of new lines to print before the value.\n        Defaults to 0.\n    \"\"\"\n    if not self.enabled:\n        return\n\n    text = Text()\n\n    for _ in range(new_lines):\n        text.append(\"\\n\")\n\n    if not isinstance(value, list):\n        value = [value]\n\n    if not isinstance(style, list):\n        style = [style for _ in range(len(value))]\n\n    if len(value) != len(style):\n        raise ValueError(\n            f\"Value and style length mismatch. Value length: {len(value)}, \"\n            f\"Style length: {len(style)}.\"\n        )\n\n    for val, sty in zip(value, style):\n        text.append(val, style=sty)\n\n    self.console.print(Padding.indent(text, indent))\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeBenchmarksConsole.print_section_header","title":"<code>print_section_header(title, indent=0, new_lines=2)</code>","text":"<p>Print out a styled section header to the console. The title is underlined, bolded, and colored with the INFO color.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>The title of the section.</p> required <code>indent</code> <code>int</code> <p>The number of spaces to indent the title. Defaults to 0.</p> <code>0</code> <code>new_lines</code> <code>int</code> <p>The number of new lines to print before the title. Defaults to 2.</p> <code>2</code> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def print_section_header(self, title: str, indent: int = 0, new_lines: int = 2):\n    \"\"\"\n    Print out a styled section header to the console.\n    The title is underlined, bolded, and colored with the INFO color.\n\n    :param title: The title of the section.\n    :param indent: The number of spaces to indent the title.\n        Defaults to 0.\n    :param new_lines: The number of new lines to print before the title.\n        Defaults to 2.\n    \"\"\"\n    self.print_line(\n        value=f\"{title}:\",\n        style=f\"bold underline {Colors.INFO}\",\n        indent=indent,\n        new_lines=new_lines,\n    )\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeBenchmarksConsole.print_table","title":"<code>print_table(headers, rows, title, sections=None, max_char_per_col=2 ** 10, indent=0, new_lines=2)</code>","text":"<p>Print a table to the console with the given headers and rows.</p> <p>Parameters:</p> Name Type Description Default <code>headers</code> <code>list[str]</code> <p>The headers of the table.</p> required <code>rows</code> <code>list[list[Any]]</code> <p>The rows of the table.</p> required <code>title</code> <code>str</code> <p>The title of the table.</p> required <code>sections</code> <code>Optional[dict[str, tuple[int, int]]]</code> <p>The sections of the table grouping columns together. This is a mapping of the section display name to a tuple of the start and end column indices. If None, no sections are added (default).</p> <code>None</code> <code>max_char_per_col</code> <code>int</code> <p>The maximum number of characters per column.</p> <code>2 ** 10</code> <code>indent</code> <code>int</code> <p>The number of spaces to indent the table. Defaults to 0.</p> <code>0</code> <code>new_lines</code> <code>int</code> <p>The number of new lines to print before the table. Defaults to 0.</p> <code>2</code> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def print_table(\n    self,\n    headers: list[str],\n    rows: list[list[Any]],\n    title: str,\n    sections: Optional[dict[str, tuple[int, int]]] = None,\n    max_char_per_col: int = 2**10,\n    indent: int = 0,\n    new_lines: int = 2,\n):\n    \"\"\"\n    Print a table to the console with the given headers and rows.\n\n    :param headers: The headers of the table.\n    :param rows: The rows of the table.\n    :param title: The title of the table.\n    :param sections: The sections of the table grouping columns together.\n        This is a mapping of the section display name to a tuple of the start and\n        end column indices. If None, no sections are added (default).\n    :param max_char_per_col: The maximum number of characters per column.\n    :param indent: The number of spaces to indent the table.\n        Defaults to 0.\n    :param new_lines: The number of new lines to print before the table.\n        Defaults to 0.\n    \"\"\"\n\n    if rows and any(len(row) != len(headers) for row in rows):\n        raise ValueError(\n            f\"Headers and rows length mismatch. Headers length: {len(headers)}, \"\n            f\"Row length: {len(rows[0]) if rows else 'N/A'}.\"\n        )\n\n    max_characters_per_column = self.calculate_max_chars_per_column(\n        headers, rows, sections, max_char_per_col\n    )\n\n    self.print_section_header(title, indent=indent, new_lines=new_lines)\n    self.print_table_divider(\n        max_characters_per_column, include_separators=False, indent=indent\n    )\n    if sections:\n        self.print_table_sections(\n            sections, max_characters_per_column, indent=indent\n        )\n    self.print_table_row(\n        split_text_list_by_length(headers, max_characters_per_column),\n        style=f\"bold {Colors.INFO}\",\n        indent=indent,\n    )\n    self.print_table_divider(\n        max_characters_per_column, include_separators=True, indent=indent\n    )\n    for row in rows:\n        self.print_table_row(\n            split_text_list_by_length(row, max_characters_per_column),\n            style=\"italic\",\n            indent=indent,\n        )\n    self.print_table_divider(\n        max_characters_per_column, include_separators=False, indent=indent\n    )\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeBenchmarksConsole.print_table_divider","title":"<code>print_table_divider(max_chars_per_column, include_separators, indent=0)</code>","text":"<p>Print a divider line for the table (top and bottom of table with '=' characters)</p> <p>Parameters:</p> Name Type Description Default <code>max_chars_per_column</code> <code>list[int]</code> <p>The maximum number of characters per column.</p> required <code>include_separators</code> <code>bool</code> <p>Whether to include separators between columns.</p> required <code>indent</code> <code>int</code> <p>The number of spaces to indent the line. Defaults to 0.</p> <code>0</code> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def print_table_divider(\n    self, max_chars_per_column: list[int], include_separators: bool, indent: int = 0\n):\n    \"\"\"\n    Print a divider line for the table (top and bottom of table with '=' characters)\n\n    :param max_chars_per_column: The maximum number of characters per column.\n    :param include_separators: Whether to include separators between columns.\n    :param indent: The number of spaces to indent the line.\n        Defaults to 0.\n    \"\"\"\n    if include_separators:\n        columns = [\n            settings.table_headers_border_char * max_chars\n            + settings.table_column_separator_char\n            + settings.table_headers_border_char\n            for max_chars in max_chars_per_column\n        ]\n    else:\n        columns = [\n            settings.table_border_char * (max_chars + 2)\n            for max_chars in max_chars_per_column\n        ]\n\n    columns[-1] = columns[-1][:-2]\n    self.print_line(value=columns, style=Colors.INFO, indent=indent)\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeBenchmarksConsole.print_table_row","title":"<code>print_table_row(column_lines, style, indent=0)</code>","text":"<p>Print a single row of a table to the console.</p> <p>Parameters:</p> Name Type Description Default <code>column_lines</code> <code>list[list[str]]</code> <p>The lines of text to print for each column.</p> required <code>indent</code> <code>int</code> <p>The number of spaces to indent the line. Defaults to 0.</p> <code>0</code> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def print_table_row(\n    self, column_lines: list[list[str]], style: str, indent: int = 0\n):\n    \"\"\"\n    Print a single row of a table to the console.\n\n    :param column_lines: The lines of text to print for each column.\n    :param indent: The number of spaces to indent the line.\n        Defaults to 0.\n    \"\"\"\n    for row in range(len(column_lines[0])):\n        print_line = []\n        print_styles = []\n        for column in range(len(column_lines)):\n            print_line.extend(\n                [\n                    column_lines[column][row],\n                    settings.table_column_separator_char,\n                    \" \",\n                ]\n            )\n            print_styles.extend([style, Colors.INFO, \"\"])\n        print_line = print_line[:-2]\n        print_styles = print_styles[:-2]\n        self.print_line(value=print_line, style=print_styles, indent=indent)\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeBenchmarksConsole.print_table_sections","title":"<code>print_table_sections(sections, max_chars_per_column, indent=0)</code>","text":"<p>Print the sections of the table with corresponding separators to the columns the sections are mapped to to ensure it is compliant with a CSV format. For example, a section named \"Metadata\" with columns 0-3 will print this: Metadata               ,,,, Where the spaces plus the separators at the end will span the columns 0-3. All columns must be accounted for in the sections.</p> <p>Parameters:</p> Name Type Description Default <code>sections</code> <code>dict[str, tuple[int, int]]</code> <p>The sections of the table.</p> required <code>max_chars_per_column</code> <code>list[int]</code> <p>The maximum number of characters per column.</p> required <code>indent</code> <code>int</code> <p>The number of spaces to indent the line. Defaults to 0.</p> <code>0</code> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def print_table_sections(\n    self,\n    sections: dict[str, tuple[int, int]],\n    max_chars_per_column: list[int],\n    indent: int = 0,\n):\n    \"\"\"\n    Print the sections of the table with corresponding separators to the columns\n    the sections are mapped to to ensure it is compliant with a CSV format.\n    For example, a section named \"Metadata\" with columns 0-3 will print this:\n    Metadata               ,,,,\n    Where the spaces plus the separators at the end will span the columns 0-3.\n    All columns must be accounted for in the sections.\n\n    :param sections: The sections of the table.\n    :param max_chars_per_column: The maximum number of characters per column.\n    :param indent: The number of spaces to indent the line.\n        Defaults to 0.\n    \"\"\"\n    section_tuples = [(start, end, name) for name, (start, end) in sections.items()]\n    section_tuples.sort(key=lambda x: x[0])\n\n    if any(start &gt; end for start, end, _ in section_tuples):\n        raise ValueError(f\"Invalid section ranges: {section_tuples}\")\n\n    if (\n        any(\n            section_tuples[ind][1] + 1 != section_tuples[ind + 1][0]\n            for ind in range(len(section_tuples) - 1)\n        )\n        or section_tuples[0][0] != 0\n        or section_tuples[-1][1] != len(max_chars_per_column) - 1\n    ):\n        raise ValueError(f\"Invalid section ranges: {section_tuples}\")\n\n    line_values = []\n    line_styles = []\n    for section, (start_col, end_col) in sections.items():\n        section_length = sum(max_chars_per_column[start_col : end_col + 1]) + 2 * (\n            end_col - start_col + 1\n        )\n        num_separators = end_col - start_col\n        line_values.append(section)\n        line_styles.append(\"bold \" + Colors.INFO)\n        line_values.append(\n            \" \" * (section_length - len(section) - num_separators - 2)\n        )\n        line_styles.append(\"\")\n        line_values.append(settings.table_column_separator_char * num_separators)\n        line_styles.append(\"\")\n        line_values.append(settings.table_column_separator_char + \" \")\n        line_styles.append(Colors.INFO)\n    line_values = line_values[:-1]\n    line_styles = line_styles[:-1]\n    self.print_line(value=line_values, style=line_styles, indent=indent)\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeBenchmarksReport","title":"<code>GenerativeBenchmarksReport</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>A pydantic model representing a completed benchmark report. Contains a list of benchmarks along with convenience methods for finalizing and saving the report.</p> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>class GenerativeBenchmarksReport(StandardBaseModel):\n    \"\"\"\n    A pydantic model representing a completed benchmark report.\n    Contains a list of benchmarks along with convenience methods for finalizing\n    and saving the report.\n    \"\"\"\n\n    @staticmethod\n    def load_file(path: Union[str, Path]) -&gt; \"GenerativeBenchmarksReport\":\n        \"\"\"\n        Load a report from a file. The file type is determined by the file extension.\n        If the file is a directory, it expects a file named benchmarks.json under the\n        directory.\n\n        :param path: The path to load the report from.\n        :return: The loaded report.\n        \"\"\"\n        path, type_ = GenerativeBenchmarksReport._file_setup(path)\n\n        if type_ == \"json\":\n            with path.open(\"r\") as file:\n                model_dict = json.load(file)\n\n            return GenerativeBenchmarksReport.model_validate(model_dict)\n\n        if type_ == \"yaml\":\n            with path.open(\"r\") as file:\n                model_dict = yaml.safe_load(file)\n\n            return GenerativeBenchmarksReport.model_validate(model_dict)\n\n        if type_ == \"csv\":\n            raise ValueError(f\"CSV file type is not supported for loading: {path}.\")\n\n        raise ValueError(f\"Unsupported file type: {type_} for {path}.\")\n\n    benchmarks: list[GenerativeBenchmark] = Field(\n        description=\"The list of completed benchmarks contained within the report.\",\n        default_factory=list,\n    )\n\n    def set_sample_size(\n        self, sample_size: Optional[int]\n    ) -&gt; \"GenerativeBenchmarksReport\":\n        \"\"\"\n        Set the sample size for each benchmark in the report. In doing this, it will\n        reduce the contained requests of each benchmark to the sample size.\n        If sample size is None, it will return the report as is.\n\n        :param sample_size: The sample size to set for each benchmark.\n            If None, the report will be returned as is.\n        :return: The report with the sample size set for each benchmark.\n        \"\"\"\n\n        if sample_size is not None:\n            for benchmark in self.benchmarks:\n                benchmark.set_sample_size(sample_size)\n\n        return self\n\n    def save_file(self, path: Union[str, Path]) -&gt; Path:\n        \"\"\"\n        Save the report to a file. The file type is determined by the file extension.\n        If the file is a directory, it will save the report to a file named\n        benchmarks.json under the directory.\n\n        :param path: The path to save the report to.\n        :return: The path to the saved report.\n        \"\"\"\n        path, type_ = GenerativeBenchmarksReport._file_setup(path)\n\n        if type_ == \"json\":\n            return self.save_json(path)\n\n        if type_ == \"yaml\":\n            return self.save_yaml(path)\n\n        if type_ == \"csv\":\n            return self.save_csv(path)\n\n        raise ValueError(f\"Unsupported file type: {type_} for {path}.\")\n\n    def save_json(self, path: Union[str, Path]) -&gt; Path:\n        \"\"\"\n        Save the report to a JSON file containing all of the report data which is\n        reloadable using the pydantic model. If the file is a directory, it will save\n        the report to a file named benchmarks.json under the directory.\n\n        :param path: The path to save the report to.\n        :return: The path to the saved report.\n        \"\"\"\n        path, type_ = GenerativeBenchmarksReport._file_setup(path, \"json\")\n\n        if type_ != \"json\":\n            raise ValueError(\n                f\"Unsupported file type for saving a JSON: {type_} for {path}.\"\n            )\n\n        model_dict = self.model_dump()\n        model_json = json.dumps(model_dict)\n\n        with path.open(\"w\") as file:\n            file.write(model_json)\n\n        return path\n\n    def save_yaml(self, path: Union[str, Path]) -&gt; Path:\n        \"\"\"\n        Save the report to a YAML file containing all of the report data which is\n        reloadable using the pydantic model. If the file is a directory, it will save\n        the report to a file named benchmarks.yaml under the directory.\n\n        :param path: The path to save the report to.\n        :return: The path to the saved report.\n        \"\"\"\n\n        path, type_ = GenerativeBenchmarksReport._file_setup(path, \"yaml\")\n\n        if type_ != \"yaml\":\n            raise ValueError(\n                f\"Unsupported file type for saving a YAML: {type_} for {path}.\"\n            )\n\n        model_dict = self.model_dump()\n        model_yaml = yaml.dump(model_dict)\n\n        with path.open(\"w\") as file:\n            file.write(model_yaml)\n\n        return path\n\n    def save_csv(self, path: Union[str, Path]) -&gt; Path:\n        \"\"\"\n        Save the report to a CSV file containing the summarized statistics and values\n        for each report. Note, this data is not reloadable using the pydantic model.\n        If the file is a directory, it will save the report to a file named\n        benchmarks.csv under the directory.\n\n        :param path: The path to save the report to.\n        :return: The path to the saved report.\n        \"\"\"\n        path, type_ = GenerativeBenchmarksReport._file_setup(path, \"csv\")\n\n        if type_ != \"csv\":\n            raise ValueError(\n                f\"Unsupported file type for saving a CSV: {type_} for {path}.\"\n            )\n\n        with path.open(\"w\", newline=\"\") as file:\n            writer = csv.writer(file)\n            headers: list[str] = []\n            rows: list[list[Union[str, float, list[float]]]] = []\n\n            for benchmark in self.benchmarks:\n                benchmark_headers: list[str] = []\n                benchmark_values: list[Union[str, float, list[float]]] = []\n\n                desc_headers, desc_values = self._benchmark_desc_headers_and_values(\n                    benchmark\n                )\n                benchmark_headers += desc_headers\n                benchmark_values += desc_values\n\n                for status in StatusDistributionSummary.model_fields:\n                    status_headers, status_values = (\n                        self._benchmark_status_headers_and_values(benchmark, status)\n                    )\n                    benchmark_headers += status_headers\n                    benchmark_values += status_values\n\n                benchmark_extra_headers, benchmark_extra_values = (\n                    self._benchmark_extras_headers_and_values(benchmark)\n                )\n                benchmark_headers += benchmark_extra_headers\n                benchmark_values += benchmark_extra_values\n\n                if not headers:\n                    headers = benchmark_headers\n                rows.append(benchmark_values)\n\n            writer.writerow(headers)\n            for row in rows:\n                writer.writerow(row)\n\n        return path\n\n    @staticmethod\n    def _file_setup(\n        path: Union[str, Path],\n        default_file_type: Literal[\"json\", \"yaml\", \"csv\"] = \"json\",\n    ) -&gt; tuple[Path, Literal[\"json\", \"yaml\", \"csv\"]]:\n        path = Path(path) if not isinstance(path, Path) else path\n\n        if path.is_dir():\n            path = path / f\"benchmarks.{default_file_type}\"\n\n        path.parent.mkdir(parents=True, exist_ok=True)\n        path_suffix = path.suffix.lower()\n\n        if path_suffix == \".json\":\n            return path, \"json\"\n\n        if path_suffix in [\".yaml\", \".yml\"]:\n            return path, \"yaml\"\n\n        if path_suffix in [\".csv\"]:\n            return path, \"csv\"\n\n        raise ValueError(f\"Unsupported file extension: {path_suffix} for {path}.\")\n\n    @staticmethod\n    def _benchmark_desc_headers_and_values(\n        benchmark: GenerativeBenchmark,\n    ) -&gt; tuple[list[str], list[Union[str, float]]]:\n        headers = [\n            \"Type\",\n            \"Run Id\",\n            \"Id\",\n            \"Name\",\n            \"Start Time\",\n            \"End Time\",\n            \"Duration\",\n        ]\n        values: list[Union[str, float]] = [\n            benchmark.type_,\n            benchmark.run_id,\n            benchmark.id_,\n            strategy_display_str(benchmark.args.strategy),\n            datetime.fromtimestamp(benchmark.start_time).strftime(\"%Y-%m-%d %H:%M:%S\"),\n            datetime.fromtimestamp(benchmark.end_time).strftime(\"%Y-%m-%d %H:%M:%S\"),\n            benchmark.duration,\n        ]\n\n        if len(headers) != len(values):\n            raise ValueError(\"Headers and values length mismatch.\")\n\n        return headers, values\n\n    @staticmethod\n    def _benchmark_extras_headers_and_values(\n        benchmark: GenerativeBenchmark,\n    ) -&gt; tuple[list[str], list[str]]:\n        headers = [\"Args\", \"Worker\", \"Request Loader\", \"Extras\"]\n        values: list[str] = [\n            json.dumps(benchmark.args.model_dump()),\n            json.dumps(benchmark.worker.model_dump()),\n            json.dumps(benchmark.request_loader.model_dump()),\n            json.dumps(benchmark.extras),\n        ]\n\n        if len(headers) != len(values):\n            raise ValueError(\"Headers and values length mismatch.\")\n\n        return headers, values\n\n    @staticmethod\n    def _benchmark_status_headers_and_values(\n        benchmark: GenerativeBenchmark, status: str\n    ) -&gt; tuple[list[str], list[Union[float, list[float]]]]:\n        headers = [\n            f\"{status.capitalize()} Requests\",\n        ]\n        values = [\n            getattr(benchmark.request_totals, status),\n        ]\n\n        for metric in GenerativeMetrics.model_fields:\n            metric_headers, metric_values = (\n                GenerativeBenchmarksReport._benchmark_status_metrics_stats(\n                    benchmark, status, metric\n                )\n            )\n            headers += metric_headers\n            values += metric_values\n\n        if len(headers) != len(values):\n            raise ValueError(\"Headers and values length mismatch.\")\n\n        return headers, values\n\n    @staticmethod\n    def _benchmark_status_metrics_stats(\n        benchmark: GenerativeBenchmark,\n        status: str,\n        metric: str,\n    ) -&gt; tuple[list[str], list[Union[float, list[float]]]]:\n        status_display = status.capitalize()\n        metric_display = metric.replace(\"_\", \" \").capitalize()\n        status_dist_summary: StatusDistributionSummary = getattr(\n            benchmark.metrics, metric\n        )\n        dist_summary: DistributionSummary = getattr(status_dist_summary, status)\n        headers = [\n            f\"{status_display} {metric_display} mean\",\n            f\"{status_display} {metric_display} median\",\n            f\"{status_display} {metric_display} std dev\",\n            (\n                f\"{status_display} {metric_display} \"\n                \"[min, 0.1, 1, 5, 10, 25, 75, 90, 95, 99, max]\"\n            ),\n        ]\n        values: list[Union[float, list[float]]] = [\n            dist_summary.mean,\n            dist_summary.median,\n            dist_summary.std_dev,\n            [\n                dist_summary.min,\n                dist_summary.percentiles.p001,\n                dist_summary.percentiles.p01,\n                dist_summary.percentiles.p05,\n                dist_summary.percentiles.p10,\n                dist_summary.percentiles.p25,\n                dist_summary.percentiles.p75,\n                dist_summary.percentiles.p90,\n                dist_summary.percentiles.p95,\n                dist_summary.percentiles.p99,\n                dist_summary.max,\n            ],\n        ]\n\n        if len(headers) != len(values):\n            raise ValueError(\"Headers and values length mismatch.\")\n\n        return headers, values\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeBenchmarksReport.load_file","title":"<code>load_file(path)</code>  <code>staticmethod</code>","text":"<p>Load a report from a file. The file type is determined by the file extension. If the file is a directory, it expects a file named benchmarks.json under the directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>The path to load the report from.</p> required <p>Returns:</p> Type Description <code>GenerativeBenchmarksReport</code> <p>The loaded report.</p> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>@staticmethod\ndef load_file(path: Union[str, Path]) -&gt; \"GenerativeBenchmarksReport\":\n    \"\"\"\n    Load a report from a file. The file type is determined by the file extension.\n    If the file is a directory, it expects a file named benchmarks.json under the\n    directory.\n\n    :param path: The path to load the report from.\n    :return: The loaded report.\n    \"\"\"\n    path, type_ = GenerativeBenchmarksReport._file_setup(path)\n\n    if type_ == \"json\":\n        with path.open(\"r\") as file:\n            model_dict = json.load(file)\n\n        return GenerativeBenchmarksReport.model_validate(model_dict)\n\n    if type_ == \"yaml\":\n        with path.open(\"r\") as file:\n            model_dict = yaml.safe_load(file)\n\n        return GenerativeBenchmarksReport.model_validate(model_dict)\n\n    if type_ == \"csv\":\n        raise ValueError(f\"CSV file type is not supported for loading: {path}.\")\n\n    raise ValueError(f\"Unsupported file type: {type_} for {path}.\")\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeBenchmarksReport.save_csv","title":"<code>save_csv(path)</code>","text":"<p>Save the report to a CSV file containing the summarized statistics and values for each report. Note, this data is not reloadable using the pydantic model. If the file is a directory, it will save the report to a file named benchmarks.csv under the directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>The path to save the report to.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>The path to the saved report.</p> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def save_csv(self, path: Union[str, Path]) -&gt; Path:\n    \"\"\"\n    Save the report to a CSV file containing the summarized statistics and values\n    for each report. Note, this data is not reloadable using the pydantic model.\n    If the file is a directory, it will save the report to a file named\n    benchmarks.csv under the directory.\n\n    :param path: The path to save the report to.\n    :return: The path to the saved report.\n    \"\"\"\n    path, type_ = GenerativeBenchmarksReport._file_setup(path, \"csv\")\n\n    if type_ != \"csv\":\n        raise ValueError(\n            f\"Unsupported file type for saving a CSV: {type_} for {path}.\"\n        )\n\n    with path.open(\"w\", newline=\"\") as file:\n        writer = csv.writer(file)\n        headers: list[str] = []\n        rows: list[list[Union[str, float, list[float]]]] = []\n\n        for benchmark in self.benchmarks:\n            benchmark_headers: list[str] = []\n            benchmark_values: list[Union[str, float, list[float]]] = []\n\n            desc_headers, desc_values = self._benchmark_desc_headers_and_values(\n                benchmark\n            )\n            benchmark_headers += desc_headers\n            benchmark_values += desc_values\n\n            for status in StatusDistributionSummary.model_fields:\n                status_headers, status_values = (\n                    self._benchmark_status_headers_and_values(benchmark, status)\n                )\n                benchmark_headers += status_headers\n                benchmark_values += status_values\n\n            benchmark_extra_headers, benchmark_extra_values = (\n                self._benchmark_extras_headers_and_values(benchmark)\n            )\n            benchmark_headers += benchmark_extra_headers\n            benchmark_values += benchmark_extra_values\n\n            if not headers:\n                headers = benchmark_headers\n            rows.append(benchmark_values)\n\n        writer.writerow(headers)\n        for row in rows:\n            writer.writerow(row)\n\n    return path\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeBenchmarksReport.save_file","title":"<code>save_file(path)</code>","text":"<p>Save the report to a file. The file type is determined by the file extension. If the file is a directory, it will save the report to a file named benchmarks.json under the directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>The path to save the report to.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>The path to the saved report.</p> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def save_file(self, path: Union[str, Path]) -&gt; Path:\n    \"\"\"\n    Save the report to a file. The file type is determined by the file extension.\n    If the file is a directory, it will save the report to a file named\n    benchmarks.json under the directory.\n\n    :param path: The path to save the report to.\n    :return: The path to the saved report.\n    \"\"\"\n    path, type_ = GenerativeBenchmarksReport._file_setup(path)\n\n    if type_ == \"json\":\n        return self.save_json(path)\n\n    if type_ == \"yaml\":\n        return self.save_yaml(path)\n\n    if type_ == \"csv\":\n        return self.save_csv(path)\n\n    raise ValueError(f\"Unsupported file type: {type_} for {path}.\")\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeBenchmarksReport.save_json","title":"<code>save_json(path)</code>","text":"<p>Save the report to a JSON file containing all of the report data which is reloadable using the pydantic model. If the file is a directory, it will save the report to a file named benchmarks.json under the directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>The path to save the report to.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>The path to the saved report.</p> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def save_json(self, path: Union[str, Path]) -&gt; Path:\n    \"\"\"\n    Save the report to a JSON file containing all of the report data which is\n    reloadable using the pydantic model. If the file is a directory, it will save\n    the report to a file named benchmarks.json under the directory.\n\n    :param path: The path to save the report to.\n    :return: The path to the saved report.\n    \"\"\"\n    path, type_ = GenerativeBenchmarksReport._file_setup(path, \"json\")\n\n    if type_ != \"json\":\n        raise ValueError(\n            f\"Unsupported file type for saving a JSON: {type_} for {path}.\"\n        )\n\n    model_dict = self.model_dump()\n    model_json = json.dumps(model_dict)\n\n    with path.open(\"w\") as file:\n        file.write(model_json)\n\n    return path\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeBenchmarksReport.save_yaml","title":"<code>save_yaml(path)</code>","text":"<p>Save the report to a YAML file containing all of the report data which is reloadable using the pydantic model. If the file is a directory, it will save the report to a file named benchmarks.yaml under the directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>The path to save the report to.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>The path to the saved report.</p> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def save_yaml(self, path: Union[str, Path]) -&gt; Path:\n    \"\"\"\n    Save the report to a YAML file containing all of the report data which is\n    reloadable using the pydantic model. If the file is a directory, it will save\n    the report to a file named benchmarks.yaml under the directory.\n\n    :param path: The path to save the report to.\n    :return: The path to the saved report.\n    \"\"\"\n\n    path, type_ = GenerativeBenchmarksReport._file_setup(path, \"yaml\")\n\n    if type_ != \"yaml\":\n        raise ValueError(\n            f\"Unsupported file type for saving a YAML: {type_} for {path}.\"\n        )\n\n    model_dict = self.model_dump()\n    model_yaml = yaml.dump(model_dict)\n\n    with path.open(\"w\") as file:\n        file.write(model_yaml)\n\n    return path\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeBenchmarksReport.set_sample_size","title":"<code>set_sample_size(sample_size)</code>","text":"<p>Set the sample size for each benchmark in the report. In doing this, it will reduce the contained requests of each benchmark to the sample size. If sample size is None, it will return the report as is.</p> <p>Parameters:</p> Name Type Description Default <code>sample_size</code> <code>Optional[int]</code> <p>The sample size to set for each benchmark. If None, the report will be returned as is.</p> required <p>Returns:</p> Type Description <code>GenerativeBenchmarksReport</code> <p>The report with the sample size set for each benchmark.</p> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def set_sample_size(\n    self, sample_size: Optional[int]\n) -&gt; \"GenerativeBenchmarksReport\":\n    \"\"\"\n    Set the sample size for each benchmark in the report. In doing this, it will\n    reduce the contained requests of each benchmark to the sample size.\n    If sample size is None, it will return the report as is.\n\n    :param sample_size: The sample size to set for each benchmark.\n        If None, the report will be returned as is.\n    :return: The report with the sample size set for each benchmark.\n    \"\"\"\n\n    if sample_size is not None:\n        for benchmark in self.benchmarks:\n            benchmark.set_sample_size(sample_size)\n\n    return self\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeMetrics","title":"<code>GenerativeMetrics</code>","text":"<p>               Bases: <code>BenchmarkMetrics</code></p> <p>A serializable model representing the metrics for a generative benchmark run.</p> Source code in <code>src/guidellm/benchmark/benchmark.py</code> <pre><code>class GenerativeMetrics(BenchmarkMetrics):\n    \"\"\"\n    A serializable model representing the metrics for a generative benchmark run.\n    \"\"\"\n\n    request_latency: StatusDistributionSummary = Field(\n        description=\"The distribution of latencies for the completed requests.\",\n    )\n    prompt_token_count: StatusDistributionSummary = Field(\n        description=(\n            \"The distribution of token counts in the prompts for completed, \"\n            \"errored, and all requests.\"\n        )\n    )\n    output_token_count: StatusDistributionSummary = Field(\n        description=(\n            \"The distribution of token counts in the outputs for completed, \"\n            \"errored, and all requests.\"\n        )\n    )\n    time_to_first_token_ms: StatusDistributionSummary = Field(\n        description=(\n            \"The distribution of latencies to receiving the first token in \"\n            \"milliseconds for completed, errored, and all requests.\"\n        ),\n    )\n    time_per_output_token_ms: StatusDistributionSummary = Field(\n        description=(\n            \"The distribution of latencies per output token in milliseconds for \"\n            \"completed, errored, and all requests. \"\n            \"This includes the time to generate the first token and all other tokens.\"\n        ),\n    )\n    inter_token_latency_ms: StatusDistributionSummary = Field(\n        description=(\n            \"The distribution of latencies between tokens in milliseconds for \"\n            \"completed, errored, and all requests.\"\n        ),\n    )\n    output_tokens_per_second: StatusDistributionSummary = Field(\n        description=(\n            \"The distribution of output tokens per second for completed, \"\n            \"errored, and all requests.\"\n        ),\n    )\n    tokens_per_second: StatusDistributionSummary = Field(\n        description=(\n            \"The distribution of tokens per second, including prompt and output tokens \"\n            \"for completed, errored, and all requests.\"\n        ),\n    )\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeTextErrorStats","title":"<code>GenerativeTextErrorStats</code>","text":"<p>               Bases: <code>GenerativeTextResponseStats</code></p> <p>A serializable model representing the request values, response values, and statistics for a generative text response that errored. Extends and overrides the GenerativeTextResponseStats model to include the error message and optional properties given the error occurred.</p> Source code in <code>src/guidellm/benchmark/benchmark.py</code> <pre><code>class GenerativeTextErrorStats(GenerativeTextResponseStats):\n    \"\"\"\n    A serializable model representing the request values, response values, and\n    statistics for a generative text response that errored.\n    Extends and overrides the GenerativeTextResponseStats model to include the\n    error message and optional properties given the error occurred.\n    \"\"\"\n\n    type_: Literal[\"generative_text_error\"] = \"generative_text_error\"  # type: ignore[assignment]\n    error: str = Field(\n        description=(\n            \"The error message for the error that occurred while making the request.\"\n        )\n    )\n    output: Optional[str] = Field(  # type: ignore[assignment]\n        default=None,\n        description=(\n            \"The generated text output from the generative request, if any, \"\n            \"before the error occurred.\"\n        ),\n    )\n    first_token_time: Optional[float] = Field(  # type: ignore[assignment]\n        default=None,\n        description=(\n            \"The time the first token was received, if any, before the error occurred.\"\n        ),\n    )\n    last_token_time: Optional[float] = Field(  # type: ignore[assignment]\n        default=None,\n        description=(\n            \"The time the last token was received, if any, before the error occurred.\"\n        ),\n    )\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def time_to_first_token_ms(self) -&gt; Optional[float]:  # type: ignore[override]\n        \"\"\"\n        :return: The time in milliseconds from the start of the request to the first\n            token received. None if the first token was not received.\n        \"\"\"\n        if self.first_token_time is None:\n            return None\n\n        return super().time_to_first_token_ms\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def time_per_output_token_ms(self) -&gt; Optional[float]:  # type: ignore[override]\n        \"\"\"\n        :return: The average time in milliseconds per output token generated.\n            This includes the time to generate the first token and all other tokens.\n            None if the output_tokens is None or 0.\n        \"\"\"\n        if (\n            self.output_tokens is None\n            or self.output_tokens == 0\n            or self.first_token_time is None\n            or self.last_token_time is None\n        ):\n            return None\n\n        return super().time_per_output_token_ms\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def inter_token_latency_ms(self) -&gt; Optional[float]:  # type: ignore[override]\n        \"\"\"\n        :return: The average time in milliseconds between generating tokens in the\n            output text. Note, does not include the time to generate the first token.\n            None if there were no output_tokens or the first token was not received.\n        \"\"\"\n        if (\n            self.output_tokens is None\n            or self.first_token_time is None\n            or self.last_token_time is None\n        ):\n            return None\n\n        return super().inter_token_latency_ms\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def output_tokens_per_second(self) -&gt; Optional[float]:  # type: ignore[override]\n        \"\"\"\n        :return: The average number of tokens generated per second in the output text.\n            Note, does not include the time to generate the first token. None if there\n            were no output_tokens or the first token was not received.\n        \"\"\"\n        if self.inter_token_latency_ms is None:\n            return None\n\n        return super().output_tokens_per_second\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeTextErrorStats.inter_token_latency_ms","title":"<code>inter_token_latency_ms</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>Optional[float]</code> <p>The average time in milliseconds between generating tokens in the output text. Note, does not include the time to generate the first token. None if there were no output_tokens or the first token was not received.</p>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeTextErrorStats.output_tokens_per_second","title":"<code>output_tokens_per_second</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>Optional[float]</code> <p>The average number of tokens generated per second in the output text. Note, does not include the time to generate the first token. None if there were no output_tokens or the first token was not received.</p>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeTextErrorStats.time_per_output_token_ms","title":"<code>time_per_output_token_ms</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>Optional[float]</code> <p>The average time in milliseconds per output token generated. This includes the time to generate the first token and all other tokens. None if the output_tokens is None or 0.</p>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeTextErrorStats.time_to_first_token_ms","title":"<code>time_to_first_token_ms</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>Optional[float]</code> <p>The time in milliseconds from the start of the request to the first token received. None if the first token was not received.</p>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeTextResponseStats","title":"<code>GenerativeTextResponseStats</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>A serializable model representing the request values, response values, and statistics for a generative text response.</p> Source code in <code>src/guidellm/benchmark/benchmark.py</code> <pre><code>class GenerativeTextResponseStats(StandardBaseModel):\n    \"\"\"\n    A serializable model representing the request values, response values, and\n    statistics for a generative text response.\n    \"\"\"\n\n    type_: Literal[\"generative_text_response\"] = \"generative_text_response\"\n    request_id: Optional[str] = Field(\n        description=\"The unique identifier for the request.\",\n    )\n    request_type: Literal[\"text_completions\", \"chat_completions\"] = Field(\n        description=\"The type of request made to the generative backend.\"\n    )\n    scheduler_info: SchedulerRequestInfo = Field(\n        description=(\n            \"The info about the request from the scheduler about how it was run.\"\n        ),\n    )\n    prompt: str = Field(\n        description=\"The text prompt used for the generative request.\",\n    )\n    output: str = Field(\n        description=\"The generated text output from the generative request.\",\n    )\n    prompt_tokens: int = Field(\n        description=\"The number of tokens in the prompt text.\",\n    )\n    output_tokens: int = Field(\n        description=\"The number of tokens in the generated output text.\",\n    )\n    start_time: float = Field(\n        description=\"The time the request started.\",\n    )\n    end_time: float = Field(\n        description=\"The time the request ended.\",\n    )\n    first_token_time: float = Field(\n        description=\"The time the first token was received.\",\n    )\n    last_token_time: float = Field(\n        description=\"The time the last token was received.\",\n    )\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def request_latency(self) -&gt; float:\n        \"\"\"\n        :return: The duration of the request in seconds from the start to the end.\n        \"\"\"\n        return self.end_time - self.start_time\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def time_to_first_token_ms(self) -&gt; float:\n        \"\"\"\n        :return: The time in milliseconds from the start of the request to the first\n            token received.\n        \"\"\"\n        return 1000 * (self.first_token_time - self.start_time)\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def time_per_output_token_ms(self) -&gt; float:\n        \"\"\"\n        :return: The average time in milliseconds per output token generated.\n            This includes the time to generate the first token and all other tokens.\n        \"\"\"\n        if self.output_tokens == 0:\n            return 0.0\n\n        return (\n            1000 * (self.last_token_time - self.first_token_time) / self.output_tokens\n        )\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def inter_token_latency_ms(self) -&gt; float:\n        \"\"\"\n        :return: The average time in milliseconds between generating tokens in the\n            output text. Note, does not include the time to generate the first token.\n        \"\"\"\n        if self.output_tokens &lt;= 1:\n            return 0.0\n\n        return (\n            1000\n            * (self.last_token_time - self.first_token_time)\n            / (self.output_tokens - 1)\n        )\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def tokens_per_second(self) -&gt; float:\n        \"\"\"\n        :return: The average number of tokens generated per second in the prompt and\n            output text.\n        \"\"\"\n        if (latency := self.request_latency) == 0.0:\n            return 0.0\n\n        return (self.prompt_tokens + self.output_tokens) / latency\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def output_tokens_per_second(self) -&gt; float:\n        \"\"\"\n        :return: The average number of output tokens generated per second.\n        \"\"\"\n        if (latency := self.request_latency) == 0.0:\n            return 0.0\n\n        return self.output_tokens / latency\n</code></pre>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeTextResponseStats.inter_token_latency_ms","title":"<code>inter_token_latency_ms</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>float</code> <p>The average time in milliseconds between generating tokens in the output text. Note, does not include the time to generate the first token.</p>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeTextResponseStats.output_tokens_per_second","title":"<code>output_tokens_per_second</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>float</code> <p>The average number of output tokens generated per second.</p>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeTextResponseStats.request_latency","title":"<code>request_latency</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>float</code> <p>The duration of the request in seconds from the start to the end.</p>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeTextResponseStats.time_per_output_token_ms","title":"<code>time_per_output_token_ms</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>float</code> <p>The average time in milliseconds per output token generated. This includes the time to generate the first token and all other tokens.</p>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeTextResponseStats.time_to_first_token_ms","title":"<code>time_to_first_token_ms</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>float</code> <p>The time in milliseconds from the start of the request to the first token received.</p>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.GenerativeTextResponseStats.tokens_per_second","title":"<code>tokens_per_second</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>float</code> <p>The average number of tokens generated per second in the prompt and output text.</p>"},{"location":"reference/guidellm/benchmark/#guidellm.benchmark.StatusBreakdown","title":"<code>StatusBreakdown</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[SuccessfulT, ErroredT, IncompleteT, TotalT]</code></p> <p>A base class for Pydantic models that are separated by statuses including successful, incomplete, and errored. It additionally enables the inclusion of total, which is intended as the combination of all statuses. Total may or may not be used depending on if it duplicates information.</p> Source code in <code>src/guidellm/objects/pydantic.py</code> <pre><code>class StatusBreakdown(BaseModel, Generic[SuccessfulT, ErroredT, IncompleteT, TotalT]):\n    \"\"\"\n    A base class for Pydantic models that are separated by statuses including\n    successful, incomplete, and errored. It additionally enables the inclusion\n    of total, which is intended as the combination of all statuses.\n    Total may or may not be used depending on if it duplicates information.\n    \"\"\"\n\n    successful: SuccessfulT = Field(\n        description=\"The results with a successful status.\",\n        default=None,  # type: ignore[assignment]\n    )\n    errored: ErroredT = Field(\n        description=\"The results with an errored status.\",\n        default=None,  # type: ignore[assignment]\n    )\n    incomplete: IncompleteT = Field(\n        description=\"The results with an incomplete status.\",\n        default=None,  # type: ignore[assignment]\n    )\n    total: TotalT = Field(\n        description=\"The combination of all statuses.\",\n        default=None,  # type: ignore[assignment]\n    )\n</code></pre>"},{"location":"reference/guidellm/benchmark/aggregator/","title":"guidellm.benchmark.aggregator","text":""},{"location":"reference/guidellm/benchmark/aggregator/#guidellm.benchmark.aggregator.BenchmarkAggregator","title":"<code>BenchmarkAggregator</code>","text":"<p>               Bases: <code>ABC</code>, <code>StandardBaseModel</code>, <code>Generic[BenchmarkT, RequestT, ResponseT]</code></p> <p>A pydantic base class representing the base class for aggregating benchmark results. The purpose is to receive and process results from a Benchmarker as it iterates through a Scheduler for an individual benchmark run. As results are added, lightweight statistics are updated and stored for immediate progress and informational updates to the caller. Once the benchmark run is complete, the <code>compile</code> method is called to finalize the benchmark and return a Benchmark object with all the results and statistics fully calculated.</p> Source code in <code>src/guidellm/benchmark/aggregator.py</code> <pre><code>class BenchmarkAggregator(\n    ABC, StandardBaseModel, Generic[BenchmarkT, RequestT, ResponseT]\n):\n    \"\"\"\n    A pydantic base class representing the base class for aggregating benchmark results.\n    The purpose is to receive and process results from a Benchmarker as it iterates\n    through a Scheduler for an individual benchmark run.\n    As results are added, lightweight statistics are updated and stored for immediate\n    progress and informational updates to the caller.\n    Once the benchmark run is complete, the `compile` method is called to finalize\n    the benchmark and return a Benchmark object with all the results and statistics\n    fully calculated.\n    \"\"\"\n\n    type_: Literal[\"benchmark_aggregator\"] = \"benchmark_aggregator\"\n    run_id: str = Field(\n        description=(\n            \"The unique identifier for the encompasing benchmark run that this \"\n            \"benchmark was a part of.\"\n        )\n    )\n    args: BenchmarkArgs = Field(\n        description=(\n            \"The arguments used to create the benchmark run that this benchmark was \"\n            \"a part of.\"\n        )\n    )\n    worker_description: Union[\n        GenerativeRequestsWorkerDescription, WorkerDescription\n    ] = Field(\n        description=(\n            \"The description and specifics for the worker used to resolve requests \"\n            \"for this benchmark.\"\n        ),\n        discriminator=\"type_\",\n    )\n    request_loader_description: Union[\n        GenerativeRequestLoaderDescription, RequestLoaderDescription\n    ] = Field(\n        description=(\n            \"The description and specifics for the request loader used to create \"\n            \"requests for this benchmark.\"\n        ),\n        discriminator=\"type_\",\n    )\n    extras: dict[str, Any] = Field(\n        description=(\n            \"Any additional information or metadata that was passed for this benchmark.\"\n        )\n    )\n    in_warmup: bool = Field(\n        description=(\n            \"A flag to indicate if the benchmark is currently in the warmup phase.\"\n        ),\n        default=False,\n        exclude=True,\n    )\n    in_cooldown: bool = Field(\n        description=(\n            \"A flag to indicate if the benchmark is currently in the cooldown phase.\"\n        ),\n        default=False,\n        exclude=True,\n    )\n    scheduler_stats: SchedulerRunningStats = Field(\n        description=(\n            \"The running statistics for the scheduler for this benchmark run. \"\n            \"This includes all requests created, regardless of their status.\"\n        ),\n        default_factory=SchedulerRunningStats,\n    )\n    requests_stats: RequestsRunningStats = Field(\n        description=(\n            \"The running statistics for the requests for this benchmark run. \"\n            \"This includes all requests created, regardless of their status.\"\n        ),\n        default_factory=RequestsRunningStats,\n    )\n    results: StatusBreakdown[\n        list[SchedulerRequestResult[RequestT, ResponseT]],\n        list[SchedulerRequestResult[RequestT, ResponseT]],\n        list[SchedulerRequestResult[RequestT, ResponseT]],\n        None,\n    ] = Field(\n        description=(\n            \"The completed requests for this benchmark run broken down by status\"\n            \"and excluding warmup and cooldown requests.\"\n        ),\n        default_factory=lambda: StatusBreakdown(  # type: ignore[arg-type]\n            successful=[],\n            errored=[],\n            incomplete=[],\n            total=None,\n        ),\n    )\n\n    def add_result(\n        self,\n        result: SchedulerRequestResult[RequestT, ResponseT],\n    ) -&gt; bool:\n        \"\"\"\n        Add a result to the aggregator. This will update the internal statistics\n        and add the result to the list of results if it is not within the warmup or\n        cooldown period.\n\n        :param result: The result to add to the aggregator.\n        :return: True if the result was added, False if it was added because it\n            did not fit within the warmup or cooldown period, was not requested,\n            or is not finished\n        \"\"\"\n        # Add scheduler statistics\n        self.scheduler_stats.created_requests += max(\n            0, result.run_info.created_requests\n        )\n        self.scheduler_stats.queued_requests += max(0, result.run_info.queued_requests)\n        self.scheduler_stats.scheduled_requests += max(\n            0, result.run_info.scheduled_requests\n        )\n        self.scheduler_stats.processing_requests += max(\n            0, result.run_info.processing_requests\n        )\n        self.scheduler_stats.completed_requests += max(\n            0, result.run_info.completed_requests\n        )\n\n        if result.type_ != \"request_complete\" or (\n            result.request_info.canceled and not result.request_info.requested\n        ):\n            # If the result is not completed yet, don't add to the results\n            # If the result was canceled and not started, ignore it\n            return False\n\n        # Add request statistics\n        self.requests_stats.totals.total += 1\n        if result.request_info.canceled:\n            self.requests_stats.totals.incomplete += 1\n        elif result.request_info.errored:\n            self.requests_stats.totals.errored += 1\n        elif result.request_info.completed:\n            self.requests_stats.totals.successful += 1\n        else:\n            raise ValueError(\n                \"Unexpected state: request_info must be either \"\n                \"completed, canceled, or errored. \"\n                f\"Got {result.request_info}\"\n            )\n\n        self.requests_stats.queued_time.update(\n            result.request_info.dequeued_time - result.request_info.queued_time\n        )\n        self.requests_stats.scheduled_time_delay.update(\n            result.request_info.scheduled_time - result.request_info.dequeued_time\n        )\n        sleep_time = max(\n            0.0,\n            result.request_info.targeted_start_time\n            - result.request_info.scheduled_time,\n        )\n        self.requests_stats.scheduled_time_sleep.update(sleep_time)\n        time_to_worker_start = (\n            result.request_info.worker_start - result.request_info.scheduled_time\n        )\n        self.requests_stats.worker_start_delay.update(time_to_worker_start - sleep_time)\n        self.requests_stats.worker_time.update(\n            result.request_info.worker_end - result.request_info.worker_start\n        )\n        self.requests_stats.worker_start_time_targeted_delay.update(\n            result.request_info.worker_start - result.request_info.targeted_start_time\n        )\n        self.requests_stats.request_start_time_delay.update(\n            result.request_info.worker_start - result.request_info.targeted_start_time\n        )\n        self.requests_stats.request_start_time_targeted_delay.update(\n            result.request_info.worker_start - result.request_info.targeted_start_time\n        )\n        self.requests_stats.request_time_delay.update(\n            (result.request_info.worker_end - result.request_info.worker_start)\n            - (result.request_info.worker_end - result.request_info.worker_start)\n        )\n        self.requests_stats.request_time.update(\n            result.request_info.worker_end - result.request_info.worker_start\n        )\n\n        # Add result to the list of results provided we are not in warmup or cooldown\n        total_completed = self.requests_stats.totals.total.total\n        global_start_time = self.requests_stats.totals.total.start_time\n\n        in_warmup_number = (\n            self.args.warmup_number and total_completed &lt;= self.args.warmup_number\n        )\n        in_warmup_duration = (\n            self.args.warmup_duration\n            and result.request_info.worker_start\n            &lt;= (global_start_time - self.args.warmup_duration)\n        )\n\n        if in_warmup_number or in_warmup_duration:\n            self.in_warmup = True\n            return True\n\n        self.in_warmup = False\n        in_cooldown_number = (\n            self.args.cooldown_number\n            and self.args.max_number\n            and total_completed &gt; self.args.max_number - self.args.cooldown_number\n        )\n        in_cooldown_duration = (\n            self.args.cooldown_duration\n            and self.args.max_duration\n            and result.request_info.worker_start\n            &gt; global_start_time + self.args.max_duration - self.args.cooldown_duration\n        )\n\n        if in_cooldown_number or in_cooldown_duration:\n            self.in_cooldown = True\n            return True\n\n        self.in_cooldown = False\n\n        if result.request_info.canceled:\n            self.results.incomplete.append(result)\n        elif result.request_info.errored:\n            self.results.errored.append(result)\n        elif result.request_info.completed:\n            self.results.successful.append(result)\n        else:\n            raise ValueError(\n                \"Unexpected state: request_info must be either \"\n                \"completed, canceled, or errored. \"\n                f\"Got {result.request_info}\"\n            )\n\n        return True\n\n    @abstractmethod\n    def compile(self) -&gt; BenchmarkT:\n        \"\"\"\n        Compile the benchmark results and statistics into a Benchmark object.\n        This is required to be implemented by subclasses to finalize the benchmark\n        and return the compiled object.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/guidellm/benchmark/aggregator/#guidellm.benchmark.aggregator.BenchmarkAggregator.add_result","title":"<code>add_result(result)</code>","text":"<p>Add a result to the aggregator. This will update the internal statistics and add the result to the list of results if it is not within the warmup or cooldown period.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>SchedulerRequestResult[RequestT, ResponseT]</code> <p>The result to add to the aggregator.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the result was added, False if it was added because it did not fit within the warmup or cooldown period, was not requested, or is not finished</p> Source code in <code>src/guidellm/benchmark/aggregator.py</code> <pre><code>def add_result(\n    self,\n    result: SchedulerRequestResult[RequestT, ResponseT],\n) -&gt; bool:\n    \"\"\"\n    Add a result to the aggregator. This will update the internal statistics\n    and add the result to the list of results if it is not within the warmup or\n    cooldown period.\n\n    :param result: The result to add to the aggregator.\n    :return: True if the result was added, False if it was added because it\n        did not fit within the warmup or cooldown period, was not requested,\n        or is not finished\n    \"\"\"\n    # Add scheduler statistics\n    self.scheduler_stats.created_requests += max(\n        0, result.run_info.created_requests\n    )\n    self.scheduler_stats.queued_requests += max(0, result.run_info.queued_requests)\n    self.scheduler_stats.scheduled_requests += max(\n        0, result.run_info.scheduled_requests\n    )\n    self.scheduler_stats.processing_requests += max(\n        0, result.run_info.processing_requests\n    )\n    self.scheduler_stats.completed_requests += max(\n        0, result.run_info.completed_requests\n    )\n\n    if result.type_ != \"request_complete\" or (\n        result.request_info.canceled and not result.request_info.requested\n    ):\n        # If the result is not completed yet, don't add to the results\n        # If the result was canceled and not started, ignore it\n        return False\n\n    # Add request statistics\n    self.requests_stats.totals.total += 1\n    if result.request_info.canceled:\n        self.requests_stats.totals.incomplete += 1\n    elif result.request_info.errored:\n        self.requests_stats.totals.errored += 1\n    elif result.request_info.completed:\n        self.requests_stats.totals.successful += 1\n    else:\n        raise ValueError(\n            \"Unexpected state: request_info must be either \"\n            \"completed, canceled, or errored. \"\n            f\"Got {result.request_info}\"\n        )\n\n    self.requests_stats.queued_time.update(\n        result.request_info.dequeued_time - result.request_info.queued_time\n    )\n    self.requests_stats.scheduled_time_delay.update(\n        result.request_info.scheduled_time - result.request_info.dequeued_time\n    )\n    sleep_time = max(\n        0.0,\n        result.request_info.targeted_start_time\n        - result.request_info.scheduled_time,\n    )\n    self.requests_stats.scheduled_time_sleep.update(sleep_time)\n    time_to_worker_start = (\n        result.request_info.worker_start - result.request_info.scheduled_time\n    )\n    self.requests_stats.worker_start_delay.update(time_to_worker_start - sleep_time)\n    self.requests_stats.worker_time.update(\n        result.request_info.worker_end - result.request_info.worker_start\n    )\n    self.requests_stats.worker_start_time_targeted_delay.update(\n        result.request_info.worker_start - result.request_info.targeted_start_time\n    )\n    self.requests_stats.request_start_time_delay.update(\n        result.request_info.worker_start - result.request_info.targeted_start_time\n    )\n    self.requests_stats.request_start_time_targeted_delay.update(\n        result.request_info.worker_start - result.request_info.targeted_start_time\n    )\n    self.requests_stats.request_time_delay.update(\n        (result.request_info.worker_end - result.request_info.worker_start)\n        - (result.request_info.worker_end - result.request_info.worker_start)\n    )\n    self.requests_stats.request_time.update(\n        result.request_info.worker_end - result.request_info.worker_start\n    )\n\n    # Add result to the list of results provided we are not in warmup or cooldown\n    total_completed = self.requests_stats.totals.total.total\n    global_start_time = self.requests_stats.totals.total.start_time\n\n    in_warmup_number = (\n        self.args.warmup_number and total_completed &lt;= self.args.warmup_number\n    )\n    in_warmup_duration = (\n        self.args.warmup_duration\n        and result.request_info.worker_start\n        &lt;= (global_start_time - self.args.warmup_duration)\n    )\n\n    if in_warmup_number or in_warmup_duration:\n        self.in_warmup = True\n        return True\n\n    self.in_warmup = False\n    in_cooldown_number = (\n        self.args.cooldown_number\n        and self.args.max_number\n        and total_completed &gt; self.args.max_number - self.args.cooldown_number\n    )\n    in_cooldown_duration = (\n        self.args.cooldown_duration\n        and self.args.max_duration\n        and result.request_info.worker_start\n        &gt; global_start_time + self.args.max_duration - self.args.cooldown_duration\n    )\n\n    if in_cooldown_number or in_cooldown_duration:\n        self.in_cooldown = True\n        return True\n\n    self.in_cooldown = False\n\n    if result.request_info.canceled:\n        self.results.incomplete.append(result)\n    elif result.request_info.errored:\n        self.results.errored.append(result)\n    elif result.request_info.completed:\n        self.results.successful.append(result)\n    else:\n        raise ValueError(\n            \"Unexpected state: request_info must be either \"\n            \"completed, canceled, or errored. \"\n            f\"Got {result.request_info}\"\n        )\n\n    return True\n</code></pre>"},{"location":"reference/guidellm/benchmark/aggregator/#guidellm.benchmark.aggregator.BenchmarkAggregator.compile","title":"<code>compile()</code>  <code>abstractmethod</code>","text":"<p>Compile the benchmark results and statistics into a Benchmark object. This is required to be implemented by subclasses to finalize the benchmark and return the compiled object.</p> Source code in <code>src/guidellm/benchmark/aggregator.py</code> <pre><code>@abstractmethod\ndef compile(self) -&gt; BenchmarkT:\n    \"\"\"\n    Compile the benchmark results and statistics into a Benchmark object.\n    This is required to be implemented by subclasses to finalize the benchmark\n    and return the compiled object.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/guidellm/benchmark/aggregator/#guidellm.benchmark.aggregator.GenerativeBenchmarkAggregator","title":"<code>GenerativeBenchmarkAggregator</code>","text":"<p>               Bases: <code>BenchmarkAggregator[GenerativeBenchmark, GenerationRequest, ResponseSummary]</code></p> Source code in <code>src/guidellm/benchmark/aggregator.py</code> <pre><code>class GenerativeBenchmarkAggregator(\n    BenchmarkAggregator[GenerativeBenchmark, GenerationRequest, ResponseSummary]\n):\n    type_: Literal[\"generative_benchmark_aggregator\"] = (\n        \"generative_benchmark_aggregator\"  # type: ignore[assignment]\n    )\n    processor: Optional[Union[str, Path, Any]] = Field(\n        description=(\n            \"The tokenizer to use for calculating token counts when none are \"\n            \"avaiable that match the preferred source.\"\n        )\n    )\n    processor_args: Optional[dict[str, Any]] = Field(\n        description=(\n            \"Additional arguments to pass to the tokenizer if it requires \"\n            \"any specific configuration for loading or processing.\"\n        ),\n    )\n    worker_description: GenerativeRequestsWorkerDescription = Field(\n        description=(\n            \"The description and specifics for the worker used to resolve requests \"\n            \"for this benchmark.\"\n        ),\n        discriminator=\"type_\",\n    )\n    request_loader_description: GenerativeRequestLoaderDescription = Field(\n        description=(\n            \"The description and specifics for the request loader used to create \"\n            \"requests for this benchmark.\"\n        ),\n        discriminator=\"type_\",\n    )\n    requests_stats: GenerativeRequestsRunningStats = Field(\n        description=(\n            \"The running statistics for the requests for this benchmark run. \"\n            \"This includes all requests created, regardless of their status.\"\n        ),\n        default_factory=GenerativeRequestsRunningStats,\n    )\n\n    def add_result(\n        self, result: SchedulerRequestResult[GenerationRequest, ResponseSummary]\n    ) -&gt; bool:\n        \"\"\"\n        Add a result to the aggregator. This will update the internal statistics\n        and add the result to the list of results if it is not within the warmup or\n        cooldown period.\n\n        :param result: The result to add to the aggregator.\n        \"\"\"\n        if not super().add_result(result):\n            return False\n\n        if result.request is None:\n            raise ValueError(\"Request is None, cannot add result.\")\n\n        if result.response is None:\n            raise ValueError(\"Response is None, cannot add result.\")\n\n        self.requests_stats.request_start_time_delay.update(\n            result.response.start_time - result.request_info.worker_start\n        )\n        self.requests_stats.request_start_time_targeted_delay.update(\n            result.response.start_time - result.request_info.targeted_start_time\n        )\n        self.requests_stats.request_time_delay.update(\n            (result.response.start_time - result.request_info.worker_start)\n            + result.request_info.worker_end\n            - result.response.end_time\n        )\n        self.requests_stats.request_time.update(\n            result.response.end_time - result.response.start_time\n        )\n        if result.response.first_iter_time:\n            self.requests_stats.time_to_first_token.update(\n                result.response.first_iter_time - result.response.start_time\n            )\n        if result.response.last_iter_time and result.response.first_iter_time:\n            self.requests_stats.inter_token_latency.update(\n                result.response.last_iter_time - result.response.first_iter_time,\n                count=(result.response.output_tokens or 1) - 1,\n            )\n        self.requests_stats.prompt_tokens += result.response.request_prompt_tokens or 0\n        self.requests_stats.output_tokens += result.response.request_output_tokens or 0\n        total_tokens = (result.response.request_prompt_tokens or 0) + (\n            result.response.request_output_tokens or 0\n        )\n        self.requests_stats.total_tokens += total_tokens\n\n        return True\n\n    def compile(self) -&gt; GenerativeBenchmark:\n        \"\"\"\n        Compile the benchmark results and statistics into a GenerativeBenchmark object.\n        This is required to be implemented by subclasses to finalize the benchmark\n        and return the compiled object.\n        \"\"\"\n        successful, incomplete, errored = self._compile_results()\n\n        return GenerativeBenchmark.from_stats(\n            run_id=self.run_id,\n            successful=successful,\n            incomplete=incomplete,\n            errored=errored,\n            args=self.args,\n            run_stats=BenchmarkRunStats(\n                start_time=self.requests_stats.totals.total.start_time,\n                end_time=time.time(),\n                requests_made=StatusBreakdown(\n                    successful=int(self.requests_stats.totals.successful.total),\n                    errored=int(self.requests_stats.totals.errored.total),\n                    incomplete=int(self.requests_stats.totals.incomplete.total),\n                    total=int(self.requests_stats.totals.total.total),\n                ),\n                queued_time_avg=self.requests_stats.queued_time.mean,\n                scheduled_time_delay_avg=self.requests_stats.scheduled_time_delay.mean,\n                scheduled_time_sleep_avg=self.requests_stats.scheduled_time_sleep.mean,\n                worker_start_delay_avg=self.requests_stats.worker_start_delay.mean,\n                worker_time_avg=self.requests_stats.worker_time.mean,\n                worker_start_time_targeted_delay_avg=self.requests_stats.worker_start_time_targeted_delay.mean,\n                request_start_time_delay_avg=self.requests_stats.request_start_time_delay.mean,\n                request_start_time_targeted_delay_avg=self.requests_stats.request_start_time_targeted_delay.mean,\n                request_time_delay_avg=self.requests_stats.request_time_delay.mean,\n                request_time_avg=self.requests_stats.request_time.mean,\n            ),\n            worker=self.worker_description,\n            requests_loader=self.request_loader_description,\n            extras=self.extras,\n        )\n\n    def _compile_results(\n        self,\n    ) -&gt; tuple[\n        list[GenerativeTextResponseStats],\n        list[GenerativeTextErrorStats],\n        list[GenerativeTextErrorStats],\n    ]:\n        successful: list[GenerativeTextResponseStats] = [\n            GenerativeTextResponseStats(\n                request_id=result.request.request_id,\n                request_type=result.request.request_type,\n                scheduler_info=result.request_info,\n                prompt=str(result.request.content),\n                prompt_tokens=self._compile_tokens_count(\n                    value=str(result.request.content),\n                    requests_tokens=result.response.request_prompt_tokens,\n                    response_tokens=result.response.response_prompt_tokens,\n                    preferred_tokens_source=settings.preferred_prompt_tokens_source,\n                    errored=False,\n                ),\n                output=result.response.value,\n                output_tokens=self._compile_tokens_count(\n                    value=result.response.value,\n                    requests_tokens=result.response.request_output_tokens,\n                    response_tokens=result.response.response_output_tokens,\n                    preferred_tokens_source=settings.preferred_output_tokens_source,\n                    errored=False,\n                ),\n                start_time=result.response.start_time,\n                end_time=result.response.end_time,\n                first_token_time=result.response.first_iter_time or -1.0,\n                last_token_time=result.response.last_iter_time or -1.0,\n            )\n            for result in self.results.successful\n            if result.request and result.response\n        ]\n        incomplete: list[GenerativeTextErrorStats] = [\n            GenerativeTextErrorStats(\n                error=result.response.error or \"\",\n                request_id=result.request.request_id,\n                request_type=result.request.request_type,\n                scheduler_info=result.request_info,\n                prompt=str(result.request.content),\n                prompt_tokens=self._compile_tokens_count(\n                    value=str(result.request.content),\n                    requests_tokens=result.response.request_prompt_tokens,\n                    response_tokens=result.response.response_prompt_tokens,\n                    preferred_tokens_source=settings.preferred_prompt_tokens_source,\n                    errored=True,\n                ),\n                output=result.response.value,\n                output_tokens=self._compile_tokens_count(\n                    value=result.response.value,\n                    requests_tokens=result.response.request_output_tokens,\n                    response_tokens=result.response.response_output_tokens,\n                    preferred_tokens_source=settings.preferred_output_tokens_source,\n                    errored=True,\n                ),\n                start_time=result.response.start_time,\n                end_time=result.response.end_time,\n                first_token_time=result.response.first_iter_time,\n                last_token_time=result.response.last_iter_time,\n            )\n            for result in self.results.incomplete\n            if result.request and result.response\n        ]\n        error: list[GenerativeTextErrorStats] = [\n            GenerativeTextErrorStats(\n                error=result.response.error or \"\",\n                request_id=result.request.request_id,\n                request_type=result.request.request_type,\n                scheduler_info=result.request_info,\n                prompt=str(result.request.content),\n                prompt_tokens=self._compile_tokens_count(\n                    value=str(result.request.content),\n                    requests_tokens=result.response.request_prompt_tokens,\n                    response_tokens=result.response.response_prompt_tokens,\n                    preferred_tokens_source=settings.preferred_prompt_tokens_source,\n                    errored=True,\n                ),\n                output=result.response.value,\n                output_tokens=self._compile_tokens_count(\n                    value=result.response.value,\n                    requests_tokens=result.response.request_output_tokens,\n                    response_tokens=result.response.response_output_tokens,\n                    preferred_tokens_source=settings.preferred_output_tokens_source,\n                    errored=True,\n                ),\n                start_time=result.response.start_time,\n                end_time=result.response.end_time,\n                first_token_time=result.response.first_iter_time,\n                last_token_time=result.response.last_iter_time,\n            )\n            for result in self.results.errored\n            if result.request and result.response\n        ]\n\n        return successful, incomplete, error\n\n    def _compile_tokens_count(\n        self,\n        value: str,\n        requests_tokens: Optional[int],\n        response_tokens: Optional[int],\n        preferred_tokens_source: Optional[Literal[\"request\", \"response\", \"local\"]],\n        errored: bool,\n    ) -&gt; int:\n        if not errored and preferred_tokens_source == \"response\" and response_tokens:\n            return response_tokens or 0\n\n        if not errored and preferred_tokens_source == \"request\" and requests_tokens:\n            return requests_tokens or 0\n\n        if preferred_tokens_source in {\"response\", \"request\"} and (\n            self.processor is None or errored or response_tokens or requests_tokens\n        ):\n            # we had a preferred tokens source that isn't local and we either\n            # have the data to return something or we don't have the ability\n            # to calculate locally\n            return response_tokens or requests_tokens or 0\n\n        self.processor = check_load_processor(\n            self.processor,\n            processor_args=self.processor_args,\n            error_msg=\"Processor/Tokenizer is required for calculating token counts.\",\n        )\n        return len(self.processor.tokenize(value))\n</code></pre>"},{"location":"reference/guidellm/benchmark/aggregator/#guidellm.benchmark.aggregator.GenerativeBenchmarkAggregator.add_result","title":"<code>add_result(result)</code>","text":"<p>Add a result to the aggregator. This will update the internal statistics and add the result to the list of results if it is not within the warmup or cooldown period.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>SchedulerRequestResult[GenerationRequest, ResponseSummary]</code> <p>The result to add to the aggregator.</p> required Source code in <code>src/guidellm/benchmark/aggregator.py</code> <pre><code>def add_result(\n    self, result: SchedulerRequestResult[GenerationRequest, ResponseSummary]\n) -&gt; bool:\n    \"\"\"\n    Add a result to the aggregator. This will update the internal statistics\n    and add the result to the list of results if it is not within the warmup or\n    cooldown period.\n\n    :param result: The result to add to the aggregator.\n    \"\"\"\n    if not super().add_result(result):\n        return False\n\n    if result.request is None:\n        raise ValueError(\"Request is None, cannot add result.\")\n\n    if result.response is None:\n        raise ValueError(\"Response is None, cannot add result.\")\n\n    self.requests_stats.request_start_time_delay.update(\n        result.response.start_time - result.request_info.worker_start\n    )\n    self.requests_stats.request_start_time_targeted_delay.update(\n        result.response.start_time - result.request_info.targeted_start_time\n    )\n    self.requests_stats.request_time_delay.update(\n        (result.response.start_time - result.request_info.worker_start)\n        + result.request_info.worker_end\n        - result.response.end_time\n    )\n    self.requests_stats.request_time.update(\n        result.response.end_time - result.response.start_time\n    )\n    if result.response.first_iter_time:\n        self.requests_stats.time_to_first_token.update(\n            result.response.first_iter_time - result.response.start_time\n        )\n    if result.response.last_iter_time and result.response.first_iter_time:\n        self.requests_stats.inter_token_latency.update(\n            result.response.last_iter_time - result.response.first_iter_time,\n            count=(result.response.output_tokens or 1) - 1,\n        )\n    self.requests_stats.prompt_tokens += result.response.request_prompt_tokens or 0\n    self.requests_stats.output_tokens += result.response.request_output_tokens or 0\n    total_tokens = (result.response.request_prompt_tokens or 0) + (\n        result.response.request_output_tokens or 0\n    )\n    self.requests_stats.total_tokens += total_tokens\n\n    return True\n</code></pre>"},{"location":"reference/guidellm/benchmark/aggregator/#guidellm.benchmark.aggregator.GenerativeBenchmarkAggregator.compile","title":"<code>compile()</code>","text":"<p>Compile the benchmark results and statistics into a GenerativeBenchmark object. This is required to be implemented by subclasses to finalize the benchmark and return the compiled object.</p> Source code in <code>src/guidellm/benchmark/aggregator.py</code> <pre><code>def compile(self) -&gt; GenerativeBenchmark:\n    \"\"\"\n    Compile the benchmark results and statistics into a GenerativeBenchmark object.\n    This is required to be implemented by subclasses to finalize the benchmark\n    and return the compiled object.\n    \"\"\"\n    successful, incomplete, errored = self._compile_results()\n\n    return GenerativeBenchmark.from_stats(\n        run_id=self.run_id,\n        successful=successful,\n        incomplete=incomplete,\n        errored=errored,\n        args=self.args,\n        run_stats=BenchmarkRunStats(\n            start_time=self.requests_stats.totals.total.start_time,\n            end_time=time.time(),\n            requests_made=StatusBreakdown(\n                successful=int(self.requests_stats.totals.successful.total),\n                errored=int(self.requests_stats.totals.errored.total),\n                incomplete=int(self.requests_stats.totals.incomplete.total),\n                total=int(self.requests_stats.totals.total.total),\n            ),\n            queued_time_avg=self.requests_stats.queued_time.mean,\n            scheduled_time_delay_avg=self.requests_stats.scheduled_time_delay.mean,\n            scheduled_time_sleep_avg=self.requests_stats.scheduled_time_sleep.mean,\n            worker_start_delay_avg=self.requests_stats.worker_start_delay.mean,\n            worker_time_avg=self.requests_stats.worker_time.mean,\n            worker_start_time_targeted_delay_avg=self.requests_stats.worker_start_time_targeted_delay.mean,\n            request_start_time_delay_avg=self.requests_stats.request_start_time_delay.mean,\n            request_start_time_targeted_delay_avg=self.requests_stats.request_start_time_targeted_delay.mean,\n            request_time_delay_avg=self.requests_stats.request_time_delay.mean,\n            request_time_avg=self.requests_stats.request_time.mean,\n        ),\n        worker=self.worker_description,\n        requests_loader=self.request_loader_description,\n        extras=self.extras,\n    )\n</code></pre>"},{"location":"reference/guidellm/benchmark/aggregator/#guidellm.benchmark.aggregator.GenerativeRequestsRunningStats","title":"<code>GenerativeRequestsRunningStats</code>","text":"<p>               Bases: <code>RequestsRunningStats</code></p> <p>The metrics for generative requests that have succeeded, been canceled, or errored stored as running statistics for easy calculations of rates, averages, totals, etc.</p> Source code in <code>src/guidellm/benchmark/aggregator.py</code> <pre><code>class GenerativeRequestsRunningStats(RequestsRunningStats):\n    \"\"\"\n    The metrics for generative requests that have succeeded, been canceled, or errored\n    stored as running statistics for easy calculations of rates, averages, totals, etc.\n    \"\"\"\n\n    time_to_first_token: TimeRunningStats = Field(\n        description=(\n            \"The running statistics for the time from the start of the request to the \"\n            \"first token being generated for all requests that completed within the \"\n            \"benchmark run.\"\n        ),\n        default_factory=TimeRunningStats,\n    )\n    inter_token_latency: TimeRunningStats = Field(\n        description=(\n            \"The running statistics for the time between each token being generated \"\n            \"for all requests that completed within the benchmark run.\"\n        ),\n        default_factory=TimeRunningStats,\n    )\n    prompt_tokens: RunningStats = Field(\n        description=(\n            \"The running statistics for the token count for the prompt for all \"\n            \"requests that completed, if available in the response.\"\n        ),\n        default_factory=RunningStats,\n    )\n    output_tokens: RunningStats = Field(\n        description=(\n            \"The running statistics for the token count for the output for all \"\n            \"requests that completed, if available in the response.\"\n        ),\n        default_factory=RunningStats,\n    )\n    total_tokens: RunningStats = Field(\n        description=(\n            \"The running statistics for the total token count for all requests that \"\n            \"completed, if available in the response.\"\n        ),\n        default_factory=RunningStats,\n    )\n</code></pre>"},{"location":"reference/guidellm/benchmark/aggregator/#guidellm.benchmark.aggregator.RequestsRunningStats","title":"<code>RequestsRunningStats</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>The metrics for requests that have succeeded, been canceled, or errored stored as running statistics for easy calculations of rates, averages, totals, etc.</p> Source code in <code>src/guidellm/benchmark/aggregator.py</code> <pre><code>class RequestsRunningStats(StandardBaseModel):\n    \"\"\"\n    The metrics for requests that have succeeded, been canceled, or errored stored\n    as running statistics for easy calculations of rates, averages, totals, etc.\n    \"\"\"\n\n    totals: StatusBreakdown[RunningStats, RunningStats, RunningStats, RunningStats] = (\n        Field(\n            description=(\n                \"The running statistics for the total number of requests that \"\n                \"completed within the benchmark run.\"\n            ),\n            default_factory=lambda: StatusBreakdown(\n                successful=RunningStats(),\n                errored=RunningStats(),\n                incomplete=RunningStats(),\n                total=RunningStats(),\n            ),\n        )\n    )\n    queued_time: TimeRunningStats = Field(\n        description=(\n            \"The running statistics for the time spent in queue for all requests that \"\n            \"completed within the benchmark run. This is the time from when the \"\n            \"request was created to when it was dequeued by the worker.\"\n        ),\n        default_factory=TimeRunningStats,\n    )\n    scheduled_time_delay: TimeRunningStats = Field(\n        description=(\n            \"The running statistics for the time spent from when a request was \"\n            \"dequeued by the worker to when it was actually scheduled by the worker\"\n            \"for all requests that completed within the benchmark run. \"\n            \"This should be as close to 0 as possible, any additional time is \"\n            \"overheads from the system or the worker.\"\n        ),\n        default_factory=TimeRunningStats,\n    )\n    scheduled_time_sleep: TimeRunningStats = Field(\n        description=(\n            \"The running statistics for the time for each request spent sleeping til \"\n            \"the desired start time was reached for all requests that completed within \"\n            \"the benchmark run. This is the time from when the request was scheduled \"\n            \"to when the desired start time was reached. \"\n        ),\n        default_factory=TimeRunningStats,\n    )\n    worker_start_delay: TimeRunningStats = Field(\n        description=(\n            \"The running statistics for the time delay between when the request was \"\n            \"scheduled and when the worker actually started processing subtracting any \"\n            \"sleep time for all requests that completed within the benchmark run. \"\n            \"This should be as close to 0 as possible, any additional time is \"\n            \"overheads from the system or the worker.\"\n        ),\n        default_factory=TimeRunningStats,\n    )\n    worker_time: TimeRunningStats = Field(\n        description=(\n            \"The running statistics for the time spent processing all requests that \"\n            \"completed within the benchmark run. This is the time from when the \"\n            \"request was started to when it was completed.\"\n        ),\n        default_factory=TimeRunningStats,\n    )\n    worker_start_time_targeted_delay: TimeRunningStats = Field(\n        description=(\n            \"The running statistics for the delay between the targeted start time and \"\n            \"the actual start time for requests that completed within the benchmark \"\n            \"run. This represents delays from the best case desired start time. \"\n            \"For async strategies, this represents delays from the ideal system. \"\n            \"For sync strategies, since those are doubled in queue, this should be \"\n            \"as close to the time for a request to be processed as possible.\"\n        ),\n        default_factory=TimeRunningStats,\n    )\n    request_start_time_delay: TimeRunningStats = Field(\n        description=(\n            \"The running statistics for the delay between the actual request being \"\n            \"made and the time the worker started on the request for all requests \"\n            \"that completed within the benchmark run. This time should be as close to \"\n            \"0 as possible, any additional time is overhead from the system or \"\n            \"the worker.\"\n        ),\n        default_factory=TimeRunningStats,\n    )\n    request_start_time_targeted_delay: TimeRunningStats = Field(\n        description=(\n            \"The running statistics for the delay between the targeted start time and \"\n            \"the actual start time for all requests that completed within the \"\n            \"benchmark run. This represents delays from the best case desired start \"\n            \"time. For async strategies, this represents delays from the ideal system. \"\n            \"For sync strategies, since those are duplicated in queue, this should be \"\n            \"as close to the time for a request to be processed.\"\n        ),\n        default_factory=TimeRunningStats,\n    )\n    request_time_delay: TimeRunningStats = Field(\n        description=(\n            \"The running statistics for the delay in time between the total request \"\n            \"time and the worker time. This should be as close to 0 as possible, any \"\n            \"additional time is overhead from the system or the worker. \"\n        ),\n        default_factory=TimeRunningStats,\n    )\n    request_time: TimeRunningStats = Field(\n        description=(\n            \"The running statistics for the time spent processing all requests that \"\n            \"completed within the benchmark run. This is the time from when the \"\n            \"request was created to when it was completed.\"\n        ),\n        default_factory=TimeRunningStats,\n    )\n</code></pre>"},{"location":"reference/guidellm/benchmark/aggregator/#guidellm.benchmark.aggregator.SchedulerRunningStats","title":"<code>SchedulerRunningStats</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>The metrics for the scheduler stored as running statistics for easy calculations of rates, averages, totals, etc.</p> Source code in <code>src/guidellm/benchmark/aggregator.py</code> <pre><code>class SchedulerRunningStats(StandardBaseModel):\n    \"\"\"\n    The metrics for the scheduler stored as running statistics for easy calculations\n    of rates, averages, totals, etc.\n    \"\"\"\n\n    created_requests: RunningStats = Field(\n        description=(\n            \"The running statistics for the number of requests created for this \"\n            \"benchmark run. This includes all requests created, regardless of \"\n            \"their status.\"\n        ),\n        default_factory=RunningStats,\n    )\n    queued_requests: RunningStats = Field(\n        description=(\n            \"The running statistics for the number of requests pending in queue \"\n            \"for this benchmark run. This includes requests that are waiting to \"\n            \"be scheduled.\"\n        ),\n        default_factory=RunningStats,\n    )\n    scheduled_requests: RunningStats = Field(\n        description=(\n            \"The running statistics for the number of requests scheduled (actively \"\n            \"running but waiting for the desired start time) for this benchmark run.\"\n        ),\n        default_factory=RunningStats,\n    )\n    processing_requests: RunningStats = Field(\n        description=(\n            \"The running statistics for the number of requests actively being \"\n            \"processed by the worker for this benchmark run.\"\n        ),\n        default_factory=RunningStats,\n    )\n    completed_requests: RunningStats = Field(\n        description=(\n            \"The running statistics for the number of requests completed for this \"\n            \"benchmark run. This includes requests within the warmup and cooldown \"\n            \"period, if any, along with the final results.\"\n        ),\n        default_factory=RunningStats,\n    )\n</code></pre>"},{"location":"reference/guidellm/benchmark/benchmark/","title":"guidellm.benchmark.benchmark","text":""},{"location":"reference/guidellm/benchmark/benchmark/#guidellm.benchmark.benchmark.Benchmark","title":"<code>Benchmark</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>The base serializable model representing a benchmark run and its results. Specific benchmarker implementations should extend this model to include additional information or metadata as needed.</p> <p>Note, requests_per_second and request_concurrency are kept at this level and are expected to be populated by the subclass implementation to ensure the logic for Profiles can include more complicated logic for determining what rates and concurrency values to use for subsequent strategies.</p> Source code in <code>src/guidellm/benchmark/benchmark.py</code> <pre><code>class Benchmark(StandardBaseModel):\n    \"\"\"\n    The base serializable model representing a benchmark run and its results.\n    Specific benchmarker implementations should extend this model to include\n    additional information or metadata as needed.\n\n    Note, requests_per_second and request_concurrency are kept at this level\n    and are expected to be populated by the subclass implementation to ensure\n    the logic for Profiles can include more complicated logic for determining\n    what rates and concurrency values to use for subsequent strategies.\n    \"\"\"\n\n    type_: Literal[\"benchmark\"] = \"benchmark\"\n    id_: str = Field(\n        default_factory=lambda: str(uuid.uuid4()),\n        description=\"The unique identifier for the benchmark.\",\n    )\n    run_id: str = Field(\n        description=(\n            \"The unique identifier for the encompasing benchmark run that this \"\n            \"benchmark was a part of.\"\n        )\n    )\n    args: BenchmarkArgs = Field(\n        description=(\n            \"The arguments used to specify how to run the benchmark and collect data.\"\n        )\n    )\n    run_stats: BenchmarkRunStats = Field(\n        description=(\n            \"The process statistics for the entire benchmark run across all requests.\"\n        )\n    )\n    worker: Union[WorkerDescription] = Field(\n        description=(\n            \"The description and specifics for the worker used to resolve requests \"\n            \"for this benchmark.\"\n        ),\n    )\n    request_loader: Union[RequestLoaderDescription] = Field(\n        description=(\n            \"The description and specifics for the request loader used to create \"\n            \"requests for this benchmark.\"\n        ),\n    )\n    extras: dict[str, Any] = Field(\n        description=(\n            \"Any additional information or metadata that was passed for this benchmark.\"\n        )\n    )\n    metrics: BenchmarkMetrics = Field(\n        description=(\n            \"The metrics for the benchmark run represented as a distribution of \"\n            \"various per-request statistics.\"\n        ),\n    )\n</code></pre>"},{"location":"reference/guidellm/benchmark/benchmark/#guidellm.benchmark.benchmark.BenchmarkArgs","title":"<code>BenchmarkArgs</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>A serializable model representing the arguments used to specify a benchmark run and how data was collected for it.</p> Source code in <code>src/guidellm/benchmark/benchmark.py</code> <pre><code>class BenchmarkArgs(StandardBaseModel):\n    \"\"\"\n    A serializable model representing the arguments used to specify a benchmark run\n    and how data was collected for it.\n    \"\"\"\n\n    profile: Union[\n        AsyncProfile,\n        SweepProfile,\n        ConcurrentProfile,\n        ThroughputProfile,\n        SynchronousProfile,\n        Profile,\n    ] = Field(\n        description=(\n            \"The profile used for the entire benchmark run that the strategy for \"\n            \"this benchmark was pulled from.\"\n        ),\n        discriminator=\"type_\",\n    )\n    strategy_index: int = Field(\n        description=(\n            \"The index of the strategy in the profile that was used for this benchmark.\"\n        )\n    )\n    strategy: Union[\n        ConcurrentStrategy,\n        SchedulingStrategy,\n        ThroughputStrategy,\n        SynchronousStrategy,\n        AsyncPoissonStrategy,\n        AsyncConstantStrategy,\n        SchedulingStrategy,\n    ] = Field(\n        description=\"The scheduling strategy used to run this benchmark. \",\n        discriminator=\"type_\",\n    )\n    max_number: Optional[int] = Field(\n        description=\"The maximum number of requests to run for this benchmark, if any.\"\n    )\n    max_duration: Optional[float] = Field(\n        description=\"The maximum duration in seconds to run this benchmark, if any.\"\n    )\n    warmup_number: Optional[int] = Field(\n        description=(\n            \"The number of requests to run for the warmup phase of this benchmark, \"\n            \"if any. These are requests that were not included in the final results.\"\n        )\n    )\n    warmup_duration: Optional[float] = Field(\n        description=(\n            \"The duration in seconds to run for the warmup phase of this benchmark, \"\n            \"if any. These are requests that were not included in the final results.\"\n        )\n    )\n    cooldown_number: Optional[int] = Field(\n        description=(\n            \"The number of requests to run for the cooldown phase of this benchmark, \"\n            \"if any. These are requests that were not included in the final results.\"\n        )\n    )\n    cooldown_duration: Optional[float] = Field(\n        description=(\n            \"The duration in seconds to run for the cooldown phase of this benchmark, \"\n            \"if any. These are requests that were not included in the final results.\"\n        )\n    )\n</code></pre>"},{"location":"reference/guidellm/benchmark/benchmark/#guidellm.benchmark.benchmark.BenchmarkMetrics","title":"<code>BenchmarkMetrics</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>A serializable model representing the metrics for a benchmark run.</p> Source code in <code>src/guidellm/benchmark/benchmark.py</code> <pre><code>class BenchmarkMetrics(StandardBaseModel):\n    \"\"\"\n    A serializable model representing the metrics for a benchmark run.\n    \"\"\"\n\n    requests_per_second: StatusDistributionSummary = Field(\n        description=\"The distribution of requests per second for the benchmark.\",\n    )\n    request_concurrency: StatusDistributionSummary = Field(\n        description=\"The distribution of requests concurrency for the benchmark.\",\n    )\n</code></pre>"},{"location":"reference/guidellm/benchmark/benchmark/#guidellm.benchmark.benchmark.BenchmarkRunStats","title":"<code>BenchmarkRunStats</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>A serializable model representing the run process statistics for the entire benchmark run across all requests including warmup and cooldown.</p> Source code in <code>src/guidellm/benchmark/benchmark.py</code> <pre><code>class BenchmarkRunStats(StandardBaseModel):\n    \"\"\"\n    A serializable model representing the run process statistics for the\n    entire benchmark run across all requests including warmup and cooldown.\n    \"\"\"\n\n    start_time: float = Field(\n        description=\"The start time of the benchmark run.\",\n    )\n    end_time: float = Field(\n        description=\"The end time of the benchmark run.\",\n    )\n    requests_made: StatusBreakdown[int, int, int, int] = Field(\n        description=(\n            \"The number of requests made for the benchmark run broken down by \"\n            \"status including successful, incomplete, errored, and the sum of all three\"\n        )\n    )\n    queued_time_avg: float = Field(\n        description=(\n            \"The average time spent in the queue for each request in the benchmark \"\n            \"run until it was dequeued by a worker.\"\n        )\n    )\n    scheduled_time_delay_avg: float = Field(\n        description=(\n            \"The average time delay between when a request was dequeued and when it \"\n            \"was scheduled to be processed by a worker in the benchmark run. \"\n            \"This should be as close to 0 as possible, any additional time is \"\n            \"overheads from the system or the worker.\"\n        )\n    )\n    scheduled_time_sleep_avg: float = Field(\n        description=(\n            \"The average time spent sleeping til the desired start time was reached \"\n            \"after being scheduled by the worker in the benchmark run.\"\n        )\n    )\n    worker_start_delay_avg: float = Field(\n        description=(\n            \"The average time delay between when a request was scheduled and when \"\n            \"the worker started processing it in the benchmark run. \"\n            \"This should be as close to 0 as possible, any additional time is \"\n            \"overheads from the system or the worker.\"\n        )\n    )\n    worker_time_avg: float = Field(\n        description=(\n            \"The average time taken by the worker to process each request in the \"\n            \"benchmark run. This includes the time to generate the response and \"\n            \"any additional processing time.\"\n        )\n    )\n    worker_start_time_targeted_delay_avg: float = Field(\n        description=(\n            \"The average time delay between when a request was targeted to start \"\n            \"and when the worker actually started processing it in the benchmark \"\n            \"run. For async strategies, this represents delays from the ideal \"\n            \"system. For sync strategies, since those are doubled in queue, \"\n            \"this should be as close to the time for a request to be processed \"\n            \"as possible. Any additional time is overhead from the system or \"\n            \"the worker.\"\n        )\n    )\n    request_start_time_delay_avg: float = Field(\n        description=(\n            \"The average time delay between the actual request being made \"\n            \"and the time the worker started on the request for all requests \"\n            \"that completed within the benchmark run. This time should be as close \"\n            \"to 0 as possible, any additional time is overhead from the system or \"\n            \"the worker.\"\n        )\n    )\n    request_start_time_targeted_delay_avg: float = Field(\n        description=(\n            \"The average time delay between when the targeted start time and \"\n            \"the actual start time for each request in the benchmark run. \"\n            \"For async strategies, this represents delays from the ideal \"\n            \"system. For sync strategies, this should be as close to the \"\n            \"time for a request to be processed as possible. Any additional \"\n            \"time is overhead from the system or the worker.\"\n        )\n    )\n    request_time_delay_avg: float = Field(\n        description=(\n            \"The average time delay between the total request time and the \"\n            \"worker time. This should be as close to 0 as possible, any additional \"\n            \"time is overhead from the system or the worker. \"\n        )\n    )\n    request_time_avg: float = Field(\n        description=(\n            \"The average time spent processing all requests in the benchmark run. \"\n            \"This is the time from when the actual request was started to when \"\n            \"it was completed.\"\n        )\n    )\n</code></pre>"},{"location":"reference/guidellm/benchmark/benchmark/#guidellm.benchmark.benchmark.GenerativeBenchmark","title":"<code>GenerativeBenchmark</code>","text":"<p>               Bases: <code>Benchmark</code></p> <p>A serializable model representing a benchmark run and its results for generative requests and responses. Includes the completed and errored requests, the start and end times for the benchmark, and the statistics for the requests and responses.</p> Source code in <code>src/guidellm/benchmark/benchmark.py</code> <pre><code>class GenerativeBenchmark(Benchmark):\n    \"\"\"\n    A serializable model representing a benchmark run and its results for generative\n    requests and responses. Includes the completed and errored requests, the start\n    and end times for the benchmark, and the statistics for the requests and responses.\n    \"\"\"\n\n    type_: Literal[\"generative_benchmark\"] = \"generative_benchmark\"  # type: ignore[assignment]\n    start_time: float = Field(\n        description=\"The start time of the first request for the benchmark.\",\n    )\n    end_time: float = Field(\n        description=\"The end time of the last request for the benchmark.\",\n    )\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def duration(self) -&gt; float:\n        \"\"\"\n        :return: The duration of the benchmark in seconds from the start of the\n            first request to the end of the last request.\n        \"\"\"\n        return self.end_time - self.start_time\n\n    worker: GenerativeRequestsWorkerDescription = Field(\n        description=(\n            \"The description and specifics for the worker used to resolve requests \"\n            \"for this benchmark.\"\n        ),\n    )\n    request_loader: GenerativeRequestLoaderDescription = Field(\n        description=(\n            \"The description and specifics for the request loader used to create \"\n            \"requests for this benchmark.\"\n        ),\n    )\n    metrics: GenerativeMetrics = Field(\n        description=(\n            \"The metrics for the benchmark run represented as a distribution of \"\n            \"various per-request statistics.\"\n        ),\n    )\n    # Output is ordered so keep the requests at the end for better readability in files\n    request_totals: StatusBreakdown[int, int, int, int] = Field(\n        description=(\n            \"The number of requests made for the benchmark broken down by status \"\n            \"including successful, incomplete, errored, and the sum of all three\"\n        )\n    )\n    request_samples: Optional[StatusBreakdown[int, int, int, None]] = Field(\n        description=(\n            \"The number of requests that were randomly sampled for \"\n            \"the benchmark. None if no sampling was applied.\"\n        ),\n        default=None,\n    )\n    requests: StatusBreakdown[\n        list[GenerativeTextResponseStats],\n        list[GenerativeTextErrorStats],\n        list[GenerativeTextErrorStats],\n        None,\n    ] = Field(\n        description=(\n            \"The breakdown of requests for the benchmark run including successful, \"\n            \"incomplete, and errored requests.\"\n        ),\n    )\n\n    def set_sample_size(self, sample_size: Optional[int]) -&gt; \"GenerativeBenchmark\":\n        \"\"\"\n        Set the sample size for the benchmark. This will randomly sample the\n        requests for each status type to the given sample size or the maximum\n        number of requests for that status type, whichever is smaller.\n        This is applied to requests.successful, requests.errored, and\n        requests.incomplete.\n        If None, no sampling is applied and the state is kept.\n\n        :param sample_size: The number of requests to sample for each status type.\n        :return: The benchmark with the sampled requests.\n        :raises ValueError: If the sample size is invalid.\n        \"\"\"\n\n        if sample_size is not None:\n            if sample_size &lt; 0 or not isinstance(sample_size, int):\n                raise ValueError(\n                    f\"Sample size must be non-negative integer, given {sample_size}\"\n                )\n\n            sample_size = min(sample_size, len(self.requests.successful))\n            error_sample_size = min(sample_size, len(self.requests.errored))\n            incomplete_sample_size = min(sample_size, len(self.requests.incomplete))\n\n            self.requests.successful = random.sample(\n                self.requests.successful, sample_size\n            )\n            self.requests.errored = random.sample(\n                self.requests.errored, error_sample_size\n            )\n            self.requests.incomplete = random.sample(\n                self.requests.incomplete, incomplete_sample_size\n            )\n            self.request_samples = StatusBreakdown(\n                successful=len(self.requests.successful),\n                incomplete=len(self.requests.incomplete),\n                errored=len(self.requests.errored),\n            )\n\n        return self\n\n    @staticmethod\n    def from_stats(\n        run_id: str,\n        successful: list[GenerativeTextResponseStats],\n        incomplete: list[GenerativeTextErrorStats],\n        errored: list[GenerativeTextErrorStats],\n        args: BenchmarkArgs,\n        run_stats: BenchmarkRunStats,\n        worker: GenerativeRequestsWorkerDescription,\n        requests_loader: GenerativeRequestLoaderDescription,\n        extras: Optional[dict[str, Any]],\n    ) -&gt; \"GenerativeBenchmark\":\n        \"\"\"\n        Create a GenerativeBenchmark instance from the given statistics and metadata.\n        Given the completed and errored requests, the benchmark will fill in the\n        remaining statistics for the various metrics required for a benchmark.\n        This is the preferred method for creating a GenerativeBenchmark instance\n        to ensure all statistics are properly calculated and populated.\n\n        :param run_id: The unique identifier for the benchmark run.\n        :param completed: The list of completed requests.\n        :param errored: The list of errored requests.\n        :param args: The arguments used to specify how to run the benchmark\n            and collect data.\n        :param run_stats: The process statistics for the entire benchmark run across\n            all requests.\n        :param worker: The description and specifics for the worker used to resolve\n            requests.\n        :param requests_loader: The description and specifics for the request loader\n            used to create requests.\n        :param extras: Any additional information or metadata that was passed for\n            this benchmark.\n        :return: A GenerativeBenchmark instance with the given statistics and metadata\n            populated and calculated\n        \"\"\"\n        total = successful + incomplete + errored\n        total_types: list[Literal[\"successful\", \"incomplete\", \"error\"]] = [\n            *[\"successful\"] * len(successful),  # type: ignore[list-item]\n            *[\"incomplete\"] * len(incomplete),  # type: ignore[list-item]\n            *[\"error\"] * len(errored),  # type: ignore[list-item]\n        ]\n        start_time = min(req.start_time for req in total)\n        end_time = max(req.end_time for req in total)\n\n        total_with_prompt, total_types_with_prompt = (\n            zip(*filtered)\n            if (\n                filtered := list(\n                    filter(lambda val: bool(val[0].prompt), zip(total, total_types))\n                )\n            )\n            else ([], [])\n        )\n        total_with_output_first, total_types_with_output_first = (\n            zip(*filtered)\n            if (\n                filtered := list(\n                    filter(\n                        lambda val: bool(val[0].output_tokens &gt; 0),\n                        zip(total, total_types),\n                    )\n                )\n            )\n            else ([], [])\n        )\n        total_with_output_multi, total_types_with_output_multi = (\n            zip(*filtered)\n            if (\n                filtered := list(\n                    filter(\n                        lambda val: bool(val[0].output_tokens &gt; 1),\n                        zip(total, total_types),\n                    )\n                )\n            )\n            else ([], [])\n        )\n\n        return GenerativeBenchmark(\n            run_id=run_id,\n            args=args,\n            run_stats=run_stats,\n            extras=extras or {},\n            start_time=start_time,\n            end_time=end_time,\n            worker=worker,\n            request_loader=requests_loader,\n            metrics=GenerativeMetrics(\n                requests_per_second=StatusDistributionSummary.from_request_times(\n                    request_types=total_types,\n                    requests=[(req.start_time, req.end_time) for req in total],\n                    distribution_type=\"rate\",\n                ),\n                request_concurrency=StatusDistributionSummary.from_request_times(\n                    request_types=total_types,\n                    requests=[(req.start_time, req.end_time) for req in total],\n                    distribution_type=\"concurrency\",\n                ),\n                request_latency=StatusDistributionSummary.from_values(\n                    value_types=total_types,\n                    values=[req.request_latency for req in total],\n                ),\n                prompt_token_count=StatusDistributionSummary.from_values(\n                    value_types=list(total_types_with_prompt),\n                    values=[req.prompt_tokens for req in total_with_prompt],\n                ),\n                output_token_count=StatusDistributionSummary.from_values(\n                    value_types=list(total_types_with_output_first),\n                    values=[req.output_tokens for req in total_with_output_first],\n                ),\n                time_to_first_token_ms=StatusDistributionSummary.from_values(\n                    value_types=list(total_types_with_output_first),\n                    values=[\n                        req.time_to_first_token_ms or 0\n                        for req in total_with_output_first\n                    ],\n                ),\n                time_per_output_token_ms=StatusDistributionSummary.from_values(\n                    value_types=list(total_types_with_output_first),\n                    values=[\n                        req.time_per_output_token_ms or 0\n                        for req in total_with_output_first\n                    ],\n                    weights=[req.output_tokens for req in total_with_output_first],\n                ),\n                inter_token_latency_ms=StatusDistributionSummary.from_values(\n                    value_types=list(total_types_with_output_multi),\n                    values=[\n                        req.inter_token_latency_ms or 0\n                        for req in total_with_output_multi\n                    ],\n                    weights=[req.output_tokens - 1 for req in total_with_output_multi],\n                ),\n                output_tokens_per_second=StatusDistributionSummary.from_iterable_request_times(\n                    request_types=list(total_types_with_output_first),\n                    requests=[\n                        (req.start_time, req.end_time)\n                        for req in total_with_output_first\n                    ],\n                    first_iter_times=[\n                        req.first_token_time or req.start_time\n                        for req in total_with_output_first\n                    ],\n                    iter_counts=[req.output_tokens for req in total_with_output_first],\n                ),\n                tokens_per_second=StatusDistributionSummary.from_iterable_request_times(\n                    request_types=list(total_types_with_output_first),\n                    requests=[\n                        (req.start_time, req.end_time)\n                        for req in total_with_output_first\n                    ],\n                    first_iter_times=[\n                        req.first_token_time or req.start_time\n                        for req in total_with_output_first\n                    ],\n                    iter_counts=[\n                        req.prompt_tokens + req.output_tokens\n                        for req in total_with_output_first\n                    ],\n                    first_iter_counts=[\n                        req.prompt_tokens for req in total_with_output_first\n                    ],\n                ),\n            ),\n            request_totals=StatusBreakdown(\n                successful=len(successful),\n                incomplete=len(incomplete),\n                errored=len(errored),\n                total=len(total),\n            ),\n            requests=StatusBreakdown(\n                successful=successful,\n                incomplete=incomplete,\n                errored=errored,\n            ),\n        )\n</code></pre>"},{"location":"reference/guidellm/benchmark/benchmark/#guidellm.benchmark.benchmark.GenerativeBenchmark.duration","title":"<code>duration</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>float</code> <p>The duration of the benchmark in seconds from the start of the first request to the end of the last request.</p>"},{"location":"reference/guidellm/benchmark/benchmark/#guidellm.benchmark.benchmark.GenerativeBenchmark.from_stats","title":"<code>from_stats(run_id, successful, incomplete, errored, args, run_stats, worker, requests_loader, extras)</code>  <code>staticmethod</code>","text":"<p>Create a GenerativeBenchmark instance from the given statistics and metadata. Given the completed and errored requests, the benchmark will fill in the remaining statistics for the various metrics required for a benchmark. This is the preferred method for creating a GenerativeBenchmark instance to ensure all statistics are properly calculated and populated.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>The unique identifier for the benchmark run.</p> required <code>completed</code> <p>The list of completed requests.</p> required <code>errored</code> <code>list[GenerativeTextErrorStats]</code> <p>The list of errored requests.</p> required <code>args</code> <code>BenchmarkArgs</code> <p>The arguments used to specify how to run the benchmark and collect data.</p> required <code>run_stats</code> <code>BenchmarkRunStats</code> <p>The process statistics for the entire benchmark run across all requests.</p> required <code>worker</code> <code>GenerativeRequestsWorkerDescription</code> <p>The description and specifics for the worker used to resolve requests.</p> required <code>requests_loader</code> <code>GenerativeRequestLoaderDescription</code> <p>The description and specifics for the request loader used to create requests.</p> required <code>extras</code> <code>Optional[dict[str, Any]]</code> <p>Any additional information or metadata that was passed for this benchmark.</p> required <p>Returns:</p> Type Description <code>GenerativeBenchmark</code> <p>A GenerativeBenchmark instance with the given statistics and metadata populated and calculated</p> Source code in <code>src/guidellm/benchmark/benchmark.py</code> <pre><code>@staticmethod\ndef from_stats(\n    run_id: str,\n    successful: list[GenerativeTextResponseStats],\n    incomplete: list[GenerativeTextErrorStats],\n    errored: list[GenerativeTextErrorStats],\n    args: BenchmarkArgs,\n    run_stats: BenchmarkRunStats,\n    worker: GenerativeRequestsWorkerDescription,\n    requests_loader: GenerativeRequestLoaderDescription,\n    extras: Optional[dict[str, Any]],\n) -&gt; \"GenerativeBenchmark\":\n    \"\"\"\n    Create a GenerativeBenchmark instance from the given statistics and metadata.\n    Given the completed and errored requests, the benchmark will fill in the\n    remaining statistics for the various metrics required for a benchmark.\n    This is the preferred method for creating a GenerativeBenchmark instance\n    to ensure all statistics are properly calculated and populated.\n\n    :param run_id: The unique identifier for the benchmark run.\n    :param completed: The list of completed requests.\n    :param errored: The list of errored requests.\n    :param args: The arguments used to specify how to run the benchmark\n        and collect data.\n    :param run_stats: The process statistics for the entire benchmark run across\n        all requests.\n    :param worker: The description and specifics for the worker used to resolve\n        requests.\n    :param requests_loader: The description and specifics for the request loader\n        used to create requests.\n    :param extras: Any additional information or metadata that was passed for\n        this benchmark.\n    :return: A GenerativeBenchmark instance with the given statistics and metadata\n        populated and calculated\n    \"\"\"\n    total = successful + incomplete + errored\n    total_types: list[Literal[\"successful\", \"incomplete\", \"error\"]] = [\n        *[\"successful\"] * len(successful),  # type: ignore[list-item]\n        *[\"incomplete\"] * len(incomplete),  # type: ignore[list-item]\n        *[\"error\"] * len(errored),  # type: ignore[list-item]\n    ]\n    start_time = min(req.start_time for req in total)\n    end_time = max(req.end_time for req in total)\n\n    total_with_prompt, total_types_with_prompt = (\n        zip(*filtered)\n        if (\n            filtered := list(\n                filter(lambda val: bool(val[0].prompt), zip(total, total_types))\n            )\n        )\n        else ([], [])\n    )\n    total_with_output_first, total_types_with_output_first = (\n        zip(*filtered)\n        if (\n            filtered := list(\n                filter(\n                    lambda val: bool(val[0].output_tokens &gt; 0),\n                    zip(total, total_types),\n                )\n            )\n        )\n        else ([], [])\n    )\n    total_with_output_multi, total_types_with_output_multi = (\n        zip(*filtered)\n        if (\n            filtered := list(\n                filter(\n                    lambda val: bool(val[0].output_tokens &gt; 1),\n                    zip(total, total_types),\n                )\n            )\n        )\n        else ([], [])\n    )\n\n    return GenerativeBenchmark(\n        run_id=run_id,\n        args=args,\n        run_stats=run_stats,\n        extras=extras or {},\n        start_time=start_time,\n        end_time=end_time,\n        worker=worker,\n        request_loader=requests_loader,\n        metrics=GenerativeMetrics(\n            requests_per_second=StatusDistributionSummary.from_request_times(\n                request_types=total_types,\n                requests=[(req.start_time, req.end_time) for req in total],\n                distribution_type=\"rate\",\n            ),\n            request_concurrency=StatusDistributionSummary.from_request_times(\n                request_types=total_types,\n                requests=[(req.start_time, req.end_time) for req in total],\n                distribution_type=\"concurrency\",\n            ),\n            request_latency=StatusDistributionSummary.from_values(\n                value_types=total_types,\n                values=[req.request_latency for req in total],\n            ),\n            prompt_token_count=StatusDistributionSummary.from_values(\n                value_types=list(total_types_with_prompt),\n                values=[req.prompt_tokens for req in total_with_prompt],\n            ),\n            output_token_count=StatusDistributionSummary.from_values(\n                value_types=list(total_types_with_output_first),\n                values=[req.output_tokens for req in total_with_output_first],\n            ),\n            time_to_first_token_ms=StatusDistributionSummary.from_values(\n                value_types=list(total_types_with_output_first),\n                values=[\n                    req.time_to_first_token_ms or 0\n                    for req in total_with_output_first\n                ],\n            ),\n            time_per_output_token_ms=StatusDistributionSummary.from_values(\n                value_types=list(total_types_with_output_first),\n                values=[\n                    req.time_per_output_token_ms or 0\n                    for req in total_with_output_first\n                ],\n                weights=[req.output_tokens for req in total_with_output_first],\n            ),\n            inter_token_latency_ms=StatusDistributionSummary.from_values(\n                value_types=list(total_types_with_output_multi),\n                values=[\n                    req.inter_token_latency_ms or 0\n                    for req in total_with_output_multi\n                ],\n                weights=[req.output_tokens - 1 for req in total_with_output_multi],\n            ),\n            output_tokens_per_second=StatusDistributionSummary.from_iterable_request_times(\n                request_types=list(total_types_with_output_first),\n                requests=[\n                    (req.start_time, req.end_time)\n                    for req in total_with_output_first\n                ],\n                first_iter_times=[\n                    req.first_token_time or req.start_time\n                    for req in total_with_output_first\n                ],\n                iter_counts=[req.output_tokens for req in total_with_output_first],\n            ),\n            tokens_per_second=StatusDistributionSummary.from_iterable_request_times(\n                request_types=list(total_types_with_output_first),\n                requests=[\n                    (req.start_time, req.end_time)\n                    for req in total_with_output_first\n                ],\n                first_iter_times=[\n                    req.first_token_time or req.start_time\n                    for req in total_with_output_first\n                ],\n                iter_counts=[\n                    req.prompt_tokens + req.output_tokens\n                    for req in total_with_output_first\n                ],\n                first_iter_counts=[\n                    req.prompt_tokens for req in total_with_output_first\n                ],\n            ),\n        ),\n        request_totals=StatusBreakdown(\n            successful=len(successful),\n            incomplete=len(incomplete),\n            errored=len(errored),\n            total=len(total),\n        ),\n        requests=StatusBreakdown(\n            successful=successful,\n            incomplete=incomplete,\n            errored=errored,\n        ),\n    )\n</code></pre>"},{"location":"reference/guidellm/benchmark/benchmark/#guidellm.benchmark.benchmark.GenerativeBenchmark.set_sample_size","title":"<code>set_sample_size(sample_size)</code>","text":"<p>Set the sample size for the benchmark. This will randomly sample the requests for each status type to the given sample size or the maximum number of requests for that status type, whichever is smaller. This is applied to requests.successful, requests.errored, and requests.incomplete. If None, no sampling is applied and the state is kept.</p> <p>Parameters:</p> Name Type Description Default <code>sample_size</code> <code>Optional[int]</code> <p>The number of requests to sample for each status type.</p> required <p>Returns:</p> Type Description <code>GenerativeBenchmark</code> <p>The benchmark with the sampled requests.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the sample size is invalid.</p> Source code in <code>src/guidellm/benchmark/benchmark.py</code> <pre><code>def set_sample_size(self, sample_size: Optional[int]) -&gt; \"GenerativeBenchmark\":\n    \"\"\"\n    Set the sample size for the benchmark. This will randomly sample the\n    requests for each status type to the given sample size or the maximum\n    number of requests for that status type, whichever is smaller.\n    This is applied to requests.successful, requests.errored, and\n    requests.incomplete.\n    If None, no sampling is applied and the state is kept.\n\n    :param sample_size: The number of requests to sample for each status type.\n    :return: The benchmark with the sampled requests.\n    :raises ValueError: If the sample size is invalid.\n    \"\"\"\n\n    if sample_size is not None:\n        if sample_size &lt; 0 or not isinstance(sample_size, int):\n            raise ValueError(\n                f\"Sample size must be non-negative integer, given {sample_size}\"\n            )\n\n        sample_size = min(sample_size, len(self.requests.successful))\n        error_sample_size = min(sample_size, len(self.requests.errored))\n        incomplete_sample_size = min(sample_size, len(self.requests.incomplete))\n\n        self.requests.successful = random.sample(\n            self.requests.successful, sample_size\n        )\n        self.requests.errored = random.sample(\n            self.requests.errored, error_sample_size\n        )\n        self.requests.incomplete = random.sample(\n            self.requests.incomplete, incomplete_sample_size\n        )\n        self.request_samples = StatusBreakdown(\n            successful=len(self.requests.successful),\n            incomplete=len(self.requests.incomplete),\n            errored=len(self.requests.errored),\n        )\n\n    return self\n</code></pre>"},{"location":"reference/guidellm/benchmark/benchmark/#guidellm.benchmark.benchmark.GenerativeMetrics","title":"<code>GenerativeMetrics</code>","text":"<p>               Bases: <code>BenchmarkMetrics</code></p> <p>A serializable model representing the metrics for a generative benchmark run.</p> Source code in <code>src/guidellm/benchmark/benchmark.py</code> <pre><code>class GenerativeMetrics(BenchmarkMetrics):\n    \"\"\"\n    A serializable model representing the metrics for a generative benchmark run.\n    \"\"\"\n\n    request_latency: StatusDistributionSummary = Field(\n        description=\"The distribution of latencies for the completed requests.\",\n    )\n    prompt_token_count: StatusDistributionSummary = Field(\n        description=(\n            \"The distribution of token counts in the prompts for completed, \"\n            \"errored, and all requests.\"\n        )\n    )\n    output_token_count: StatusDistributionSummary = Field(\n        description=(\n            \"The distribution of token counts in the outputs for completed, \"\n            \"errored, and all requests.\"\n        )\n    )\n    time_to_first_token_ms: StatusDistributionSummary = Field(\n        description=(\n            \"The distribution of latencies to receiving the first token in \"\n            \"milliseconds for completed, errored, and all requests.\"\n        ),\n    )\n    time_per_output_token_ms: StatusDistributionSummary = Field(\n        description=(\n            \"The distribution of latencies per output token in milliseconds for \"\n            \"completed, errored, and all requests. \"\n            \"This includes the time to generate the first token and all other tokens.\"\n        ),\n    )\n    inter_token_latency_ms: StatusDistributionSummary = Field(\n        description=(\n            \"The distribution of latencies between tokens in milliseconds for \"\n            \"completed, errored, and all requests.\"\n        ),\n    )\n    output_tokens_per_second: StatusDistributionSummary = Field(\n        description=(\n            \"The distribution of output tokens per second for completed, \"\n            \"errored, and all requests.\"\n        ),\n    )\n    tokens_per_second: StatusDistributionSummary = Field(\n        description=(\n            \"The distribution of tokens per second, including prompt and output tokens \"\n            \"for completed, errored, and all requests.\"\n        ),\n    )\n</code></pre>"},{"location":"reference/guidellm/benchmark/benchmark/#guidellm.benchmark.benchmark.GenerativeTextErrorStats","title":"<code>GenerativeTextErrorStats</code>","text":"<p>               Bases: <code>GenerativeTextResponseStats</code></p> <p>A serializable model representing the request values, response values, and statistics for a generative text response that errored. Extends and overrides the GenerativeTextResponseStats model to include the error message and optional properties given the error occurred.</p> Source code in <code>src/guidellm/benchmark/benchmark.py</code> <pre><code>class GenerativeTextErrorStats(GenerativeTextResponseStats):\n    \"\"\"\n    A serializable model representing the request values, response values, and\n    statistics for a generative text response that errored.\n    Extends and overrides the GenerativeTextResponseStats model to include the\n    error message and optional properties given the error occurred.\n    \"\"\"\n\n    type_: Literal[\"generative_text_error\"] = \"generative_text_error\"  # type: ignore[assignment]\n    error: str = Field(\n        description=(\n            \"The error message for the error that occurred while making the request.\"\n        )\n    )\n    output: Optional[str] = Field(  # type: ignore[assignment]\n        default=None,\n        description=(\n            \"The generated text output from the generative request, if any, \"\n            \"before the error occurred.\"\n        ),\n    )\n    first_token_time: Optional[float] = Field(  # type: ignore[assignment]\n        default=None,\n        description=(\n            \"The time the first token was received, if any, before the error occurred.\"\n        ),\n    )\n    last_token_time: Optional[float] = Field(  # type: ignore[assignment]\n        default=None,\n        description=(\n            \"The time the last token was received, if any, before the error occurred.\"\n        ),\n    )\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def time_to_first_token_ms(self) -&gt; Optional[float]:  # type: ignore[override]\n        \"\"\"\n        :return: The time in milliseconds from the start of the request to the first\n            token received. None if the first token was not received.\n        \"\"\"\n        if self.first_token_time is None:\n            return None\n\n        return super().time_to_first_token_ms\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def time_per_output_token_ms(self) -&gt; Optional[float]:  # type: ignore[override]\n        \"\"\"\n        :return: The average time in milliseconds per output token generated.\n            This includes the time to generate the first token and all other tokens.\n            None if the output_tokens is None or 0.\n        \"\"\"\n        if (\n            self.output_tokens is None\n            or self.output_tokens == 0\n            or self.first_token_time is None\n            or self.last_token_time is None\n        ):\n            return None\n\n        return super().time_per_output_token_ms\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def inter_token_latency_ms(self) -&gt; Optional[float]:  # type: ignore[override]\n        \"\"\"\n        :return: The average time in milliseconds between generating tokens in the\n            output text. Note, does not include the time to generate the first token.\n            None if there were no output_tokens or the first token was not received.\n        \"\"\"\n        if (\n            self.output_tokens is None\n            or self.first_token_time is None\n            or self.last_token_time is None\n        ):\n            return None\n\n        return super().inter_token_latency_ms\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def output_tokens_per_second(self) -&gt; Optional[float]:  # type: ignore[override]\n        \"\"\"\n        :return: The average number of tokens generated per second in the output text.\n            Note, does not include the time to generate the first token. None if there\n            were no output_tokens or the first token was not received.\n        \"\"\"\n        if self.inter_token_latency_ms is None:\n            return None\n\n        return super().output_tokens_per_second\n</code></pre>"},{"location":"reference/guidellm/benchmark/benchmark/#guidellm.benchmark.benchmark.GenerativeTextErrorStats.inter_token_latency_ms","title":"<code>inter_token_latency_ms</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>Optional[float]</code> <p>The average time in milliseconds between generating tokens in the output text. Note, does not include the time to generate the first token. None if there were no output_tokens or the first token was not received.</p>"},{"location":"reference/guidellm/benchmark/benchmark/#guidellm.benchmark.benchmark.GenerativeTextErrorStats.output_tokens_per_second","title":"<code>output_tokens_per_second</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>Optional[float]</code> <p>The average number of tokens generated per second in the output text. Note, does not include the time to generate the first token. None if there were no output_tokens or the first token was not received.</p>"},{"location":"reference/guidellm/benchmark/benchmark/#guidellm.benchmark.benchmark.GenerativeTextErrorStats.time_per_output_token_ms","title":"<code>time_per_output_token_ms</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>Optional[float]</code> <p>The average time in milliseconds per output token generated. This includes the time to generate the first token and all other tokens. None if the output_tokens is None or 0.</p>"},{"location":"reference/guidellm/benchmark/benchmark/#guidellm.benchmark.benchmark.GenerativeTextErrorStats.time_to_first_token_ms","title":"<code>time_to_first_token_ms</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>Optional[float]</code> <p>The time in milliseconds from the start of the request to the first token received. None if the first token was not received.</p>"},{"location":"reference/guidellm/benchmark/benchmark/#guidellm.benchmark.benchmark.GenerativeTextResponseStats","title":"<code>GenerativeTextResponseStats</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>A serializable model representing the request values, response values, and statistics for a generative text response.</p> Source code in <code>src/guidellm/benchmark/benchmark.py</code> <pre><code>class GenerativeTextResponseStats(StandardBaseModel):\n    \"\"\"\n    A serializable model representing the request values, response values, and\n    statistics for a generative text response.\n    \"\"\"\n\n    type_: Literal[\"generative_text_response\"] = \"generative_text_response\"\n    request_id: Optional[str] = Field(\n        description=\"The unique identifier for the request.\",\n    )\n    request_type: Literal[\"text_completions\", \"chat_completions\"] = Field(\n        description=\"The type of request made to the generative backend.\"\n    )\n    scheduler_info: SchedulerRequestInfo = Field(\n        description=(\n            \"The info about the request from the scheduler about how it was run.\"\n        ),\n    )\n    prompt: str = Field(\n        description=\"The text prompt used for the generative request.\",\n    )\n    output: str = Field(\n        description=\"The generated text output from the generative request.\",\n    )\n    prompt_tokens: int = Field(\n        description=\"The number of tokens in the prompt text.\",\n    )\n    output_tokens: int = Field(\n        description=\"The number of tokens in the generated output text.\",\n    )\n    start_time: float = Field(\n        description=\"The time the request started.\",\n    )\n    end_time: float = Field(\n        description=\"The time the request ended.\",\n    )\n    first_token_time: float = Field(\n        description=\"The time the first token was received.\",\n    )\n    last_token_time: float = Field(\n        description=\"The time the last token was received.\",\n    )\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def request_latency(self) -&gt; float:\n        \"\"\"\n        :return: The duration of the request in seconds from the start to the end.\n        \"\"\"\n        return self.end_time - self.start_time\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def time_to_first_token_ms(self) -&gt; float:\n        \"\"\"\n        :return: The time in milliseconds from the start of the request to the first\n            token received.\n        \"\"\"\n        return 1000 * (self.first_token_time - self.start_time)\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def time_per_output_token_ms(self) -&gt; float:\n        \"\"\"\n        :return: The average time in milliseconds per output token generated.\n            This includes the time to generate the first token and all other tokens.\n        \"\"\"\n        if self.output_tokens == 0:\n            return 0.0\n\n        return (\n            1000 * (self.last_token_time - self.first_token_time) / self.output_tokens\n        )\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def inter_token_latency_ms(self) -&gt; float:\n        \"\"\"\n        :return: The average time in milliseconds between generating tokens in the\n            output text. Note, does not include the time to generate the first token.\n        \"\"\"\n        if self.output_tokens &lt;= 1:\n            return 0.0\n\n        return (\n            1000\n            * (self.last_token_time - self.first_token_time)\n            / (self.output_tokens - 1)\n        )\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def tokens_per_second(self) -&gt; float:\n        \"\"\"\n        :return: The average number of tokens generated per second in the prompt and\n            output text.\n        \"\"\"\n        if (latency := self.request_latency) == 0.0:\n            return 0.0\n\n        return (self.prompt_tokens + self.output_tokens) / latency\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def output_tokens_per_second(self) -&gt; float:\n        \"\"\"\n        :return: The average number of output tokens generated per second.\n        \"\"\"\n        if (latency := self.request_latency) == 0.0:\n            return 0.0\n\n        return self.output_tokens / latency\n</code></pre>"},{"location":"reference/guidellm/benchmark/benchmark/#guidellm.benchmark.benchmark.GenerativeTextResponseStats.inter_token_latency_ms","title":"<code>inter_token_latency_ms</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>float</code> <p>The average time in milliseconds between generating tokens in the output text. Note, does not include the time to generate the first token.</p>"},{"location":"reference/guidellm/benchmark/benchmark/#guidellm.benchmark.benchmark.GenerativeTextResponseStats.output_tokens_per_second","title":"<code>output_tokens_per_second</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>float</code> <p>The average number of output tokens generated per second.</p>"},{"location":"reference/guidellm/benchmark/benchmark/#guidellm.benchmark.benchmark.GenerativeTextResponseStats.request_latency","title":"<code>request_latency</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>float</code> <p>The duration of the request in seconds from the start to the end.</p>"},{"location":"reference/guidellm/benchmark/benchmark/#guidellm.benchmark.benchmark.GenerativeTextResponseStats.time_per_output_token_ms","title":"<code>time_per_output_token_ms</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>float</code> <p>The average time in milliseconds per output token generated. This includes the time to generate the first token and all other tokens.</p>"},{"location":"reference/guidellm/benchmark/benchmark/#guidellm.benchmark.benchmark.GenerativeTextResponseStats.time_to_first_token_ms","title":"<code>time_to_first_token_ms</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>float</code> <p>The time in milliseconds from the start of the request to the first token received.</p>"},{"location":"reference/guidellm/benchmark/benchmark/#guidellm.benchmark.benchmark.GenerativeTextResponseStats.tokens_per_second","title":"<code>tokens_per_second</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>float</code> <p>The average number of tokens generated per second in the prompt and output text.</p>"},{"location":"reference/guidellm/benchmark/benchmark/#guidellm.benchmark.benchmark.StatusBreakdown","title":"<code>StatusBreakdown</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[SuccessfulT, ErroredT, IncompleteT, TotalT]</code></p> <p>A base class for Pydantic models that are separated by statuses including successful, incomplete, and errored. It additionally enables the inclusion of total, which is intended as the combination of all statuses. Total may or may not be used depending on if it duplicates information.</p> Source code in <code>src/guidellm/objects/pydantic.py</code> <pre><code>class StatusBreakdown(BaseModel, Generic[SuccessfulT, ErroredT, IncompleteT, TotalT]):\n    \"\"\"\n    A base class for Pydantic models that are separated by statuses including\n    successful, incomplete, and errored. It additionally enables the inclusion\n    of total, which is intended as the combination of all statuses.\n    Total may or may not be used depending on if it duplicates information.\n    \"\"\"\n\n    successful: SuccessfulT = Field(\n        description=\"The results with a successful status.\",\n        default=None,  # type: ignore[assignment]\n    )\n    errored: ErroredT = Field(\n        description=\"The results with an errored status.\",\n        default=None,  # type: ignore[assignment]\n    )\n    incomplete: IncompleteT = Field(\n        description=\"The results with an incomplete status.\",\n        default=None,  # type: ignore[assignment]\n    )\n    total: TotalT = Field(\n        description=\"The combination of all statuses.\",\n        default=None,  # type: ignore[assignment]\n    )\n</code></pre>"},{"location":"reference/guidellm/benchmark/benchmarker/","title":"guidellm.benchmark.benchmarker","text":""},{"location":"reference/guidellm/benchmark/entrypoints/","title":"guidellm.benchmark.entrypoints","text":""},{"location":"reference/guidellm/benchmark/output/","title":"guidellm.benchmark.output","text":""},{"location":"reference/guidellm/benchmark/output/#guidellm.benchmark.output.GenerativeBenchmarksConsole","title":"<code>GenerativeBenchmarksConsole</code>","text":"<p>A class for outputting progress and benchmark results to the console. Utilizes the rich library for formatting, enabling colored and styled output.</p> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>class GenerativeBenchmarksConsole:\n    \"\"\"\n    A class for outputting progress and benchmark results to the console.\n    Utilizes the rich library for formatting, enabling colored and styled output.\n    \"\"\"\n\n    def __init__(self, enabled: bool = True):\n        \"\"\"\n        :param enabled: Whether to enable console output. Defaults to True.\n            If False, all console output will be suppressed.\n        \"\"\"\n        self.enabled = enabled\n        self.benchmarks: Optional[list[GenerativeBenchmark]] = None\n        self.console = Console()\n\n    @property\n    def benchmarks_profile_str(self) -&gt; str:\n        \"\"\"\n        :return: A string representation of the profile used for the benchmarks.\n        \"\"\"\n        profile = self.benchmarks[0].args.profile if self.benchmarks else None\n\n        if profile is None:\n            return \"None\"\n\n        profile_args = OrderedDict(\n            {\n                \"type\": profile.type_,\n                \"strategies\": profile.strategy_types,\n            }\n        )\n\n        if isinstance(profile, ConcurrentProfile):\n            profile_args[\"streams\"] = str(profile.streams)\n        elif isinstance(profile, ThroughputProfile):\n            profile_args[\"max_concurrency\"] = str(profile.max_concurrency)\n        elif isinstance(profile, AsyncProfile):\n            profile_args[\"max_concurrency\"] = str(profile.max_concurrency)\n            profile_args[\"rate\"] = str(profile.rate)\n            profile_args[\"initial_burst\"] = str(profile.initial_burst)\n        elif isinstance(profile, SweepProfile):\n            profile_args[\"sweep_size\"] = str(profile.sweep_size)\n\n        return \", \".join(f\"{key}={value}\" for key, value in profile_args.items())\n\n    @property\n    def benchmarks_args_str(self) -&gt; str:\n        \"\"\"\n        :return: A string representation of the arguments used for the benchmarks.\n        \"\"\"\n        args = self.benchmarks[0].args if self.benchmarks else None\n\n        if args is None:\n            return \"None\"\n\n        args_dict = OrderedDict(\n            {\n                \"max_number\": args.max_number,\n                \"max_duration\": args.max_duration,\n                \"warmup_number\": args.warmup_number,\n                \"warmup_duration\": args.warmup_duration,\n                \"cooldown_number\": args.cooldown_number,\n                \"cooldown_duration\": args.cooldown_duration,\n            }\n        )\n\n        return \", \".join(f\"{key}={value}\" for key, value in args_dict.items())\n\n    @property\n    def benchmarks_worker_desc_str(self) -&gt; str:\n        \"\"\"\n        :return: A string representation of the worker used for the benchmarks.\n        \"\"\"\n        return str(self.benchmarks[0].worker) if self.benchmarks else \"None\"\n\n    @property\n    def benchmarks_request_loader_desc_str(self) -&gt; str:\n        \"\"\"\n        :return: A string representation of the request loader used for the benchmarks.\n        \"\"\"\n        return str(self.benchmarks[0].request_loader) if self.benchmarks else \"None\"\n\n    @property\n    def benchmarks_extras_str(self) -&gt; str:\n        \"\"\"\n        :return: A string representation of the extras used for the benchmarks.\n        \"\"\"\n        extras = self.benchmarks[0].extras if self.benchmarks else None\n\n        if not extras:\n            return \"None\"\n\n        return \", \".join(f\"{key}={value}\" for key, value in extras.items())\n\n    def print_section_header(self, title: str, indent: int = 0, new_lines: int = 2):\n        \"\"\"\n        Print out a styled section header to the console.\n        The title is underlined, bolded, and colored with the INFO color.\n\n        :param title: The title of the section.\n        :param indent: The number of spaces to indent the title.\n            Defaults to 0.\n        :param new_lines: The number of new lines to print before the title.\n            Defaults to 2.\n        \"\"\"\n        self.print_line(\n            value=f\"{title}:\",\n            style=f\"bold underline {Colors.INFO}\",\n            indent=indent,\n            new_lines=new_lines,\n        )\n\n    def print_labeled_line(\n        self, label: str, value: str, indent: int = 4, new_lines: int = 0\n    ):\n        \"\"\"\n        Print out a styled, labeled line (label: value) to the console.\n        The label is bolded and colored with the INFO color,\n        and the value is italicized.\n\n        :param label: The label of the line.\n        :param value: The value of the line.\n        :param indent: The number of spaces to indent the line.\n            Defaults to 4.\n        :param new_lines: The number of new lines to print before the line.\n            Defaults to 0.\n        \"\"\"\n        self.print_line(\n            value=[label + \":\", value],\n            style=[\"bold \" + Colors.INFO, \"italic\"],\n            new_lines=new_lines,\n            indent=indent,\n        )\n\n    def print_line(\n        self,\n        value: Union[str, list[str]],\n        style: Union[str, list[str]] = \"\",\n        indent: int = 0,\n        new_lines: int = 0,\n    ):\n        \"\"\"\n        Print out a a value to the console as a line with optional indentation.\n\n        :param value: The value to print.\n        :param style: The style to apply to the value.\n            Defaults to none.\n        :param indent: The number of spaces to indent the line.\n            Defaults to 0.\n        :param new_lines: The number of new lines to print before the value.\n            Defaults to 0.\n        \"\"\"\n        if not self.enabled:\n            return\n\n        text = Text()\n\n        for _ in range(new_lines):\n            text.append(\"\\n\")\n\n        if not isinstance(value, list):\n            value = [value]\n\n        if not isinstance(style, list):\n            style = [style for _ in range(len(value))]\n\n        if len(value) != len(style):\n            raise ValueError(\n                f\"Value and style length mismatch. Value length: {len(value)}, \"\n                f\"Style length: {len(style)}.\"\n            )\n\n        for val, sty in zip(value, style):\n            text.append(val, style=sty)\n\n        self.console.print(Padding.indent(text, indent))\n\n    def print_table(\n        self,\n        headers: list[str],\n        rows: list[list[Any]],\n        title: str,\n        sections: Optional[dict[str, tuple[int, int]]] = None,\n        max_char_per_col: int = 2**10,\n        indent: int = 0,\n        new_lines: int = 2,\n    ):\n        \"\"\"\n        Print a table to the console with the given headers and rows.\n\n        :param headers: The headers of the table.\n        :param rows: The rows of the table.\n        :param title: The title of the table.\n        :param sections: The sections of the table grouping columns together.\n            This is a mapping of the section display name to a tuple of the start and\n            end column indices. If None, no sections are added (default).\n        :param max_char_per_col: The maximum number of characters per column.\n        :param indent: The number of spaces to indent the table.\n            Defaults to 0.\n        :param new_lines: The number of new lines to print before the table.\n            Defaults to 0.\n        \"\"\"\n\n        if rows and any(len(row) != len(headers) for row in rows):\n            raise ValueError(\n                f\"Headers and rows length mismatch. Headers length: {len(headers)}, \"\n                f\"Row length: {len(rows[0]) if rows else 'N/A'}.\"\n            )\n\n        max_characters_per_column = self.calculate_max_chars_per_column(\n            headers, rows, sections, max_char_per_col\n        )\n\n        self.print_section_header(title, indent=indent, new_lines=new_lines)\n        self.print_table_divider(\n            max_characters_per_column, include_separators=False, indent=indent\n        )\n        if sections:\n            self.print_table_sections(\n                sections, max_characters_per_column, indent=indent\n            )\n        self.print_table_row(\n            split_text_list_by_length(headers, max_characters_per_column),\n            style=f\"bold {Colors.INFO}\",\n            indent=indent,\n        )\n        self.print_table_divider(\n            max_characters_per_column, include_separators=True, indent=indent\n        )\n        for row in rows:\n            self.print_table_row(\n                split_text_list_by_length(row, max_characters_per_column),\n                style=\"italic\",\n                indent=indent,\n            )\n        self.print_table_divider(\n            max_characters_per_column, include_separators=False, indent=indent\n        )\n\n    def calculate_max_chars_per_column(\n        self,\n        headers: list[str],\n        rows: list[list[Any]],\n        sections: Optional[dict[str, tuple[int, int]]],\n        max_char_per_col: int,\n    ) -&gt; list[int]:\n        \"\"\"\n        Calculate the maximum number of characters per column in the table.\n        This is done by checking the length of the headers, rows, and optional sections\n        to ensure all columns are accounted for and spaced correctly.\n\n        :param headers: The headers of the table.\n        :param rows: The rows of the table.\n        :param sections: The sections of the table grouping columns together.\n            This is a mapping of the section display name to a tuple of the start and\n            end column indices. If None, no sections are added (default).\n        :param max_char_per_col: The maximum number of characters per column.\n        :return: A list of the maximum number of characters per column.\n        \"\"\"\n        max_characters_per_column = []\n        for ind in range(len(headers)):\n            max_characters_per_column.append(min(len(headers[ind]), max_char_per_col))\n\n            for row in rows:\n                max_characters_per_column[ind] = max(\n                    max_characters_per_column[ind], len(str(row[ind]))\n                )\n\n        if not sections:\n            return max_characters_per_column\n\n        for section in sections:\n            start_col, end_col = sections[section]\n            min_section_len = len(section) + (\n                end_col - start_col\n            )  # ensure we have enough space for separators\n            chars_in_columns = sum(\n                max_characters_per_column[start_col : end_col + 1]\n            ) + 2 * (end_col - start_col)\n            if min_section_len &gt; chars_in_columns:\n                add_chars_per_col = math.ceil(\n                    (min_section_len - chars_in_columns) / (end_col - start_col + 1)\n                )\n                for col in range(start_col, end_col + 1):\n                    max_characters_per_column[col] += add_chars_per_col\n\n        return max_characters_per_column\n\n    def print_table_divider(\n        self, max_chars_per_column: list[int], include_separators: bool, indent: int = 0\n    ):\n        \"\"\"\n        Print a divider line for the table (top and bottom of table with '=' characters)\n\n        :param max_chars_per_column: The maximum number of characters per column.\n        :param include_separators: Whether to include separators between columns.\n        :param indent: The number of spaces to indent the line.\n            Defaults to 0.\n        \"\"\"\n        if include_separators:\n            columns = [\n                settings.table_headers_border_char * max_chars\n                + settings.table_column_separator_char\n                + settings.table_headers_border_char\n                for max_chars in max_chars_per_column\n            ]\n        else:\n            columns = [\n                settings.table_border_char * (max_chars + 2)\n                for max_chars in max_chars_per_column\n            ]\n\n        columns[-1] = columns[-1][:-2]\n        self.print_line(value=columns, style=Colors.INFO, indent=indent)\n\n    def print_table_sections(\n        self,\n        sections: dict[str, tuple[int, int]],\n        max_chars_per_column: list[int],\n        indent: int = 0,\n    ):\n        \"\"\"\n        Print the sections of the table with corresponding separators to the columns\n        the sections are mapped to to ensure it is compliant with a CSV format.\n        For example, a section named \"Metadata\" with columns 0-3 will print this:\n        Metadata               ,,,,\n        Where the spaces plus the separators at the end will span the columns 0-3.\n        All columns must be accounted for in the sections.\n\n        :param sections: The sections of the table.\n        :param max_chars_per_column: The maximum number of characters per column.\n        :param indent: The number of spaces to indent the line.\n            Defaults to 0.\n        \"\"\"\n        section_tuples = [(start, end, name) for name, (start, end) in sections.items()]\n        section_tuples.sort(key=lambda x: x[0])\n\n        if any(start &gt; end for start, end, _ in section_tuples):\n            raise ValueError(f\"Invalid section ranges: {section_tuples}\")\n\n        if (\n            any(\n                section_tuples[ind][1] + 1 != section_tuples[ind + 1][0]\n                for ind in range(len(section_tuples) - 1)\n            )\n            or section_tuples[0][0] != 0\n            or section_tuples[-1][1] != len(max_chars_per_column) - 1\n        ):\n            raise ValueError(f\"Invalid section ranges: {section_tuples}\")\n\n        line_values = []\n        line_styles = []\n        for section, (start_col, end_col) in sections.items():\n            section_length = sum(max_chars_per_column[start_col : end_col + 1]) + 2 * (\n                end_col - start_col + 1\n            )\n            num_separators = end_col - start_col\n            line_values.append(section)\n            line_styles.append(\"bold \" + Colors.INFO)\n            line_values.append(\n                \" \" * (section_length - len(section) - num_separators - 2)\n            )\n            line_styles.append(\"\")\n            line_values.append(settings.table_column_separator_char * num_separators)\n            line_styles.append(\"\")\n            line_values.append(settings.table_column_separator_char + \" \")\n            line_styles.append(Colors.INFO)\n        line_values = line_values[:-1]\n        line_styles = line_styles[:-1]\n        self.print_line(value=line_values, style=line_styles, indent=indent)\n\n    def print_table_row(\n        self, column_lines: list[list[str]], style: str, indent: int = 0\n    ):\n        \"\"\"\n        Print a single row of a table to the console.\n\n        :param column_lines: The lines of text to print for each column.\n        :param indent: The number of spaces to indent the line.\n            Defaults to 0.\n        \"\"\"\n        for row in range(len(column_lines[0])):\n            print_line = []\n            print_styles = []\n            for column in range(len(column_lines)):\n                print_line.extend(\n                    [\n                        column_lines[column][row],\n                        settings.table_column_separator_char,\n                        \" \",\n                    ]\n                )\n                print_styles.extend([style, Colors.INFO, \"\"])\n            print_line = print_line[:-2]\n            print_styles = print_styles[:-2]\n            self.print_line(value=print_line, style=print_styles, indent=indent)\n\n    def print_benchmarks_metadata(self):\n        \"\"\"\n        Print out the metadata of the benchmarks to the console including the run id,\n        duration, profile, args, worker, request loader, and extras.\n        \"\"\"\n\n        if not self.benchmarks:\n            raise ValueError(\n                \"No benchmarks to print metadata for. Please set benchmarks first.\"\n            )\n\n        start_time = self.benchmarks[0].run_stats.start_time\n        end_time = self.benchmarks[-1].run_stats.end_time\n        duration = end_time - start_time\n\n        self.print_section_header(title=\"Benchmarks Metadata\")\n        self.print_labeled_line(\n            label=\"Run id\",\n            value=str(self.benchmarks[0].run_id),\n        )\n        self.print_labeled_line(\n            label=\"Duration\",\n            value=f\"{duration:.1f} seconds\",\n        )\n        self.print_labeled_line(\n            label=\"Profile\",\n            value=self.benchmarks_profile_str,\n        )\n        self.print_labeled_line(\n            label=\"Args\",\n            value=self.benchmarks_args_str,\n        )\n        self.print_labeled_line(\n            label=\"Worker\",\n            value=self.benchmarks_worker_desc_str,\n        )\n        self.print_labeled_line(\n            label=\"Request Loader\",\n            value=self.benchmarks_request_loader_desc_str,\n        )\n        self.print_labeled_line(\n            label=\"Extras\",\n            value=self.benchmarks_extras_str,\n        )\n\n    def print_benchmarks_info(self):\n        \"\"\"\n        Print out the benchmark information to the console including the start time,\n        end time, duration, request totals, and token totals for each benchmark.\n        \"\"\"\n        if not self.benchmarks:\n            raise ValueError(\n                \"No benchmarks to print info for. Please set benchmarks first.\"\n            )\n\n        sections = {\n            \"Metadata\": (0, 3),\n            \"Requests Made\": (4, 6),\n            \"Prompt Tok/Req\": (7, 9),\n            \"Output Tok/Req\": (10, 12),\n            \"Prompt Tok Total\": (13, 15),\n            \"Output Tok Total\": (16, 18),\n        }\n        headers = [\n            \"Benchmark\",\n            \"Start Time\",\n            \"End Time\",\n            \"Duration (s)\",\n            \"Comp\",\n            \"Inc\",\n            \"Err\",\n            \"Comp\",\n            \"Inc\",\n            \"Err\",\n            \"Comp\",\n            \"Inc\",\n            \"Err\",\n            \"Comp\",\n            \"Inc\",\n            \"Err\",\n            \"Comp\",\n            \"Inc\",\n            \"Err\",\n        ]\n        rows = []\n\n        for benchmark in self.benchmarks:\n            rows.append(\n                [\n                    strategy_display_str(benchmark.args.strategy),\n                    f\"{datetime.fromtimestamp(benchmark.start_time).strftime('%H:%M:%S')}\",\n                    f\"{datetime.fromtimestamp(benchmark.end_time).strftime('%H:%M:%S')}\",\n                    f\"{(benchmark.end_time - benchmark.start_time):.1f}\",\n                    f\"{benchmark.request_totals.successful:.0f}\",\n                    f\"{benchmark.request_totals.incomplete:.0f}\",\n                    f\"{benchmark.request_totals.errored:.0f}\",\n                    f\"{benchmark.metrics.prompt_token_count.successful.mean:.1f}\",\n                    f\"{benchmark.metrics.prompt_token_count.incomplete.mean:.1f}\",\n                    f\"{benchmark.metrics.prompt_token_count.errored.mean:.1f}\",\n                    f\"{benchmark.metrics.output_token_count.successful.mean:.1f}\",\n                    f\"{benchmark.metrics.output_token_count.incomplete.mean:.1f}\",\n                    f\"{benchmark.metrics.output_token_count.errored.mean:.1f}\",\n                    f\"{benchmark.metrics.prompt_token_count.successful.total_sum:.0f}\",\n                    f\"{benchmark.metrics.prompt_token_count.incomplete.total_sum:.0f}\",\n                    f\"{benchmark.metrics.prompt_token_count.errored.total_sum:.0f}\",\n                    f\"{benchmark.metrics.output_token_count.successful.total_sum:.0f}\",\n                    f\"{benchmark.metrics.output_token_count.incomplete.total_sum:.0f}\",\n                    f\"{benchmark.metrics.output_token_count.errored.total_sum:.0f}\",\n                ]\n            )\n\n        self.print_table(\n            headers=headers, rows=rows, title=\"Benchmarks Info\", sections=sections\n        )\n\n    def print_benchmarks_stats(self):\n        \"\"\"\n        Print out the benchmark statistics to the console including the requests per\n        second, request concurrency, output tokens per second, total tokens per second,\n        request latency, time to first token, inter token latency, and time per output\n        token for each benchmark.\n        \"\"\"\n        if not self.benchmarks:\n            raise ValueError(\n                \"No benchmarks to print stats for. Please set benchmarks first.\"\n            )\n\n        sections = {\n            \"Metadata\": (0, 0),\n            \"Request Stats\": (1, 2),\n            \"Out Tok/sec\": (3, 3),\n            \"Tot Tok/sec\": (4, 4),\n            \"Req Latency (sec)\": (5, 7),\n            \"TTFT (ms)\": (8, 10),\n            \"ITL (ms)\": (11, 13),\n            \"TPOT (ms)\": (14, 16),\n        }\n        headers = [\n            \"Benchmark\",\n            \"Per Second\",\n            \"Concurrency\",\n            \"mean\",\n            \"mean\",\n            \"mean\",\n            \"median\",\n            \"p99\",\n            \"mean\",\n            \"median\",\n            \"p99\",\n            \"mean\",\n            \"median\",\n            \"p99\",\n            \"mean\",\n            \"median\",\n            \"p99\",\n        ]\n        rows = []\n\n        for benchmark in self.benchmarks:\n            rows.append(\n                [\n                    strategy_display_str(benchmark.args.strategy),\n                    f\"{benchmark.metrics.requests_per_second.successful.mean:.2f}\",\n                    f\"{benchmark.metrics.request_concurrency.successful.mean:.2f}\",\n                    f\"{benchmark.metrics.output_tokens_per_second.successful.mean:.1f}\",\n                    f\"{benchmark.metrics.tokens_per_second.successful.mean:.1f}\",\n                    f\"{benchmark.metrics.request_latency.successful.mean:.2f}\",\n                    f\"{benchmark.metrics.request_latency.successful.median:.2f}\",\n                    f\"{benchmark.metrics.request_latency.successful.percentiles.p99:.2f}\",\n                    f\"{benchmark.metrics.time_to_first_token_ms.successful.mean:.1f}\",\n                    f\"{benchmark.metrics.time_to_first_token_ms.successful.median:.1f}\",\n                    f\"{benchmark.metrics.time_to_first_token_ms.successful.percentiles.p99:.1f}\",\n                    f\"{benchmark.metrics.inter_token_latency_ms.successful.mean:.1f}\",\n                    f\"{benchmark.metrics.inter_token_latency_ms.successful.median:.1f}\",\n                    f\"{benchmark.metrics.inter_token_latency_ms.successful.percentiles.p99:.1f}\",\n                    f\"{benchmark.metrics.time_per_output_token_ms.successful.mean:.1f}\",\n                    f\"{benchmark.metrics.time_per_output_token_ms.successful.median:.1f}\",\n                    f\"{benchmark.metrics.time_per_output_token_ms.successful.percentiles.p99:.1f}\",\n                ]\n            )\n\n        self.print_table(\n            headers=headers,\n            rows=rows,\n            title=\"Benchmarks Stats\",\n            sections=sections,\n        )\n</code></pre>"},{"location":"reference/guidellm/benchmark/output/#guidellm.benchmark.output.GenerativeBenchmarksConsole.benchmarks_args_str","title":"<code>benchmarks_args_str</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>str</code> <p>A string representation of the arguments used for the benchmarks.</p>"},{"location":"reference/guidellm/benchmark/output/#guidellm.benchmark.output.GenerativeBenchmarksConsole.benchmarks_extras_str","title":"<code>benchmarks_extras_str</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>str</code> <p>A string representation of the extras used for the benchmarks.</p>"},{"location":"reference/guidellm/benchmark/output/#guidellm.benchmark.output.GenerativeBenchmarksConsole.benchmarks_profile_str","title":"<code>benchmarks_profile_str</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>str</code> <p>A string representation of the profile used for the benchmarks.</p>"},{"location":"reference/guidellm/benchmark/output/#guidellm.benchmark.output.GenerativeBenchmarksConsole.benchmarks_request_loader_desc_str","title":"<code>benchmarks_request_loader_desc_str</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>str</code> <p>A string representation of the request loader used for the benchmarks.</p>"},{"location":"reference/guidellm/benchmark/output/#guidellm.benchmark.output.GenerativeBenchmarksConsole.benchmarks_worker_desc_str","title":"<code>benchmarks_worker_desc_str</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>str</code> <p>A string representation of the worker used for the benchmarks.</p>"},{"location":"reference/guidellm/benchmark/output/#guidellm.benchmark.output.GenerativeBenchmarksConsole.__init__","title":"<code>__init__(enabled=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>enabled</code> <code>bool</code> <p>Whether to enable console output. Defaults to True. If False, all console output will be suppressed.</p> <code>True</code> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def __init__(self, enabled: bool = True):\n    \"\"\"\n    :param enabled: Whether to enable console output. Defaults to True.\n        If False, all console output will be suppressed.\n    \"\"\"\n    self.enabled = enabled\n    self.benchmarks: Optional[list[GenerativeBenchmark]] = None\n    self.console = Console()\n</code></pre>"},{"location":"reference/guidellm/benchmark/output/#guidellm.benchmark.output.GenerativeBenchmarksConsole.calculate_max_chars_per_column","title":"<code>calculate_max_chars_per_column(headers, rows, sections, max_char_per_col)</code>","text":"<p>Calculate the maximum number of characters per column in the table. This is done by checking the length of the headers, rows, and optional sections to ensure all columns are accounted for and spaced correctly.</p> <p>Parameters:</p> Name Type Description Default <code>headers</code> <code>list[str]</code> <p>The headers of the table.</p> required <code>rows</code> <code>list[list[Any]]</code> <p>The rows of the table.</p> required <code>sections</code> <code>Optional[dict[str, tuple[int, int]]]</code> <p>The sections of the table grouping columns together. This is a mapping of the section display name to a tuple of the start and end column indices. If None, no sections are added (default).</p> required <code>max_char_per_col</code> <code>int</code> <p>The maximum number of characters per column.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>A list of the maximum number of characters per column.</p> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def calculate_max_chars_per_column(\n    self,\n    headers: list[str],\n    rows: list[list[Any]],\n    sections: Optional[dict[str, tuple[int, int]]],\n    max_char_per_col: int,\n) -&gt; list[int]:\n    \"\"\"\n    Calculate the maximum number of characters per column in the table.\n    This is done by checking the length of the headers, rows, and optional sections\n    to ensure all columns are accounted for and spaced correctly.\n\n    :param headers: The headers of the table.\n    :param rows: The rows of the table.\n    :param sections: The sections of the table grouping columns together.\n        This is a mapping of the section display name to a tuple of the start and\n        end column indices. If None, no sections are added (default).\n    :param max_char_per_col: The maximum number of characters per column.\n    :return: A list of the maximum number of characters per column.\n    \"\"\"\n    max_characters_per_column = []\n    for ind in range(len(headers)):\n        max_characters_per_column.append(min(len(headers[ind]), max_char_per_col))\n\n        for row in rows:\n            max_characters_per_column[ind] = max(\n                max_characters_per_column[ind], len(str(row[ind]))\n            )\n\n    if not sections:\n        return max_characters_per_column\n\n    for section in sections:\n        start_col, end_col = sections[section]\n        min_section_len = len(section) + (\n            end_col - start_col\n        )  # ensure we have enough space for separators\n        chars_in_columns = sum(\n            max_characters_per_column[start_col : end_col + 1]\n        ) + 2 * (end_col - start_col)\n        if min_section_len &gt; chars_in_columns:\n            add_chars_per_col = math.ceil(\n                (min_section_len - chars_in_columns) / (end_col - start_col + 1)\n            )\n            for col in range(start_col, end_col + 1):\n                max_characters_per_column[col] += add_chars_per_col\n\n    return max_characters_per_column\n</code></pre>"},{"location":"reference/guidellm/benchmark/output/#guidellm.benchmark.output.GenerativeBenchmarksConsole.print_benchmarks_info","title":"<code>print_benchmarks_info()</code>","text":"<p>Print out the benchmark information to the console including the start time, end time, duration, request totals, and token totals for each benchmark.</p> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def print_benchmarks_info(self):\n    \"\"\"\n    Print out the benchmark information to the console including the start time,\n    end time, duration, request totals, and token totals for each benchmark.\n    \"\"\"\n    if not self.benchmarks:\n        raise ValueError(\n            \"No benchmarks to print info for. Please set benchmarks first.\"\n        )\n\n    sections = {\n        \"Metadata\": (0, 3),\n        \"Requests Made\": (4, 6),\n        \"Prompt Tok/Req\": (7, 9),\n        \"Output Tok/Req\": (10, 12),\n        \"Prompt Tok Total\": (13, 15),\n        \"Output Tok Total\": (16, 18),\n    }\n    headers = [\n        \"Benchmark\",\n        \"Start Time\",\n        \"End Time\",\n        \"Duration (s)\",\n        \"Comp\",\n        \"Inc\",\n        \"Err\",\n        \"Comp\",\n        \"Inc\",\n        \"Err\",\n        \"Comp\",\n        \"Inc\",\n        \"Err\",\n        \"Comp\",\n        \"Inc\",\n        \"Err\",\n        \"Comp\",\n        \"Inc\",\n        \"Err\",\n    ]\n    rows = []\n\n    for benchmark in self.benchmarks:\n        rows.append(\n            [\n                strategy_display_str(benchmark.args.strategy),\n                f\"{datetime.fromtimestamp(benchmark.start_time).strftime('%H:%M:%S')}\",\n                f\"{datetime.fromtimestamp(benchmark.end_time).strftime('%H:%M:%S')}\",\n                f\"{(benchmark.end_time - benchmark.start_time):.1f}\",\n                f\"{benchmark.request_totals.successful:.0f}\",\n                f\"{benchmark.request_totals.incomplete:.0f}\",\n                f\"{benchmark.request_totals.errored:.0f}\",\n                f\"{benchmark.metrics.prompt_token_count.successful.mean:.1f}\",\n                f\"{benchmark.metrics.prompt_token_count.incomplete.mean:.1f}\",\n                f\"{benchmark.metrics.prompt_token_count.errored.mean:.1f}\",\n                f\"{benchmark.metrics.output_token_count.successful.mean:.1f}\",\n                f\"{benchmark.metrics.output_token_count.incomplete.mean:.1f}\",\n                f\"{benchmark.metrics.output_token_count.errored.mean:.1f}\",\n                f\"{benchmark.metrics.prompt_token_count.successful.total_sum:.0f}\",\n                f\"{benchmark.metrics.prompt_token_count.incomplete.total_sum:.0f}\",\n                f\"{benchmark.metrics.prompt_token_count.errored.total_sum:.0f}\",\n                f\"{benchmark.metrics.output_token_count.successful.total_sum:.0f}\",\n                f\"{benchmark.metrics.output_token_count.incomplete.total_sum:.0f}\",\n                f\"{benchmark.metrics.output_token_count.errored.total_sum:.0f}\",\n            ]\n        )\n\n    self.print_table(\n        headers=headers, rows=rows, title=\"Benchmarks Info\", sections=sections\n    )\n</code></pre>"},{"location":"reference/guidellm/benchmark/output/#guidellm.benchmark.output.GenerativeBenchmarksConsole.print_benchmarks_metadata","title":"<code>print_benchmarks_metadata()</code>","text":"<p>Print out the metadata of the benchmarks to the console including the run id, duration, profile, args, worker, request loader, and extras.</p> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def print_benchmarks_metadata(self):\n    \"\"\"\n    Print out the metadata of the benchmarks to the console including the run id,\n    duration, profile, args, worker, request loader, and extras.\n    \"\"\"\n\n    if not self.benchmarks:\n        raise ValueError(\n            \"No benchmarks to print metadata for. Please set benchmarks first.\"\n        )\n\n    start_time = self.benchmarks[0].run_stats.start_time\n    end_time = self.benchmarks[-1].run_stats.end_time\n    duration = end_time - start_time\n\n    self.print_section_header(title=\"Benchmarks Metadata\")\n    self.print_labeled_line(\n        label=\"Run id\",\n        value=str(self.benchmarks[0].run_id),\n    )\n    self.print_labeled_line(\n        label=\"Duration\",\n        value=f\"{duration:.1f} seconds\",\n    )\n    self.print_labeled_line(\n        label=\"Profile\",\n        value=self.benchmarks_profile_str,\n    )\n    self.print_labeled_line(\n        label=\"Args\",\n        value=self.benchmarks_args_str,\n    )\n    self.print_labeled_line(\n        label=\"Worker\",\n        value=self.benchmarks_worker_desc_str,\n    )\n    self.print_labeled_line(\n        label=\"Request Loader\",\n        value=self.benchmarks_request_loader_desc_str,\n    )\n    self.print_labeled_line(\n        label=\"Extras\",\n        value=self.benchmarks_extras_str,\n    )\n</code></pre>"},{"location":"reference/guidellm/benchmark/output/#guidellm.benchmark.output.GenerativeBenchmarksConsole.print_benchmarks_stats","title":"<code>print_benchmarks_stats()</code>","text":"<p>Print out the benchmark statistics to the console including the requests per second, request concurrency, output tokens per second, total tokens per second, request latency, time to first token, inter token latency, and time per output token for each benchmark.</p> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def print_benchmarks_stats(self):\n    \"\"\"\n    Print out the benchmark statistics to the console including the requests per\n    second, request concurrency, output tokens per second, total tokens per second,\n    request latency, time to first token, inter token latency, and time per output\n    token for each benchmark.\n    \"\"\"\n    if not self.benchmarks:\n        raise ValueError(\n            \"No benchmarks to print stats for. Please set benchmarks first.\"\n        )\n\n    sections = {\n        \"Metadata\": (0, 0),\n        \"Request Stats\": (1, 2),\n        \"Out Tok/sec\": (3, 3),\n        \"Tot Tok/sec\": (4, 4),\n        \"Req Latency (sec)\": (5, 7),\n        \"TTFT (ms)\": (8, 10),\n        \"ITL (ms)\": (11, 13),\n        \"TPOT (ms)\": (14, 16),\n    }\n    headers = [\n        \"Benchmark\",\n        \"Per Second\",\n        \"Concurrency\",\n        \"mean\",\n        \"mean\",\n        \"mean\",\n        \"median\",\n        \"p99\",\n        \"mean\",\n        \"median\",\n        \"p99\",\n        \"mean\",\n        \"median\",\n        \"p99\",\n        \"mean\",\n        \"median\",\n        \"p99\",\n    ]\n    rows = []\n\n    for benchmark in self.benchmarks:\n        rows.append(\n            [\n                strategy_display_str(benchmark.args.strategy),\n                f\"{benchmark.metrics.requests_per_second.successful.mean:.2f}\",\n                f\"{benchmark.metrics.request_concurrency.successful.mean:.2f}\",\n                f\"{benchmark.metrics.output_tokens_per_second.successful.mean:.1f}\",\n                f\"{benchmark.metrics.tokens_per_second.successful.mean:.1f}\",\n                f\"{benchmark.metrics.request_latency.successful.mean:.2f}\",\n                f\"{benchmark.metrics.request_latency.successful.median:.2f}\",\n                f\"{benchmark.metrics.request_latency.successful.percentiles.p99:.2f}\",\n                f\"{benchmark.metrics.time_to_first_token_ms.successful.mean:.1f}\",\n                f\"{benchmark.metrics.time_to_first_token_ms.successful.median:.1f}\",\n                f\"{benchmark.metrics.time_to_first_token_ms.successful.percentiles.p99:.1f}\",\n                f\"{benchmark.metrics.inter_token_latency_ms.successful.mean:.1f}\",\n                f\"{benchmark.metrics.inter_token_latency_ms.successful.median:.1f}\",\n                f\"{benchmark.metrics.inter_token_latency_ms.successful.percentiles.p99:.1f}\",\n                f\"{benchmark.metrics.time_per_output_token_ms.successful.mean:.1f}\",\n                f\"{benchmark.metrics.time_per_output_token_ms.successful.median:.1f}\",\n                f\"{benchmark.metrics.time_per_output_token_ms.successful.percentiles.p99:.1f}\",\n            ]\n        )\n\n    self.print_table(\n        headers=headers,\n        rows=rows,\n        title=\"Benchmarks Stats\",\n        sections=sections,\n    )\n</code></pre>"},{"location":"reference/guidellm/benchmark/output/#guidellm.benchmark.output.GenerativeBenchmarksConsole.print_labeled_line","title":"<code>print_labeled_line(label, value, indent=4, new_lines=0)</code>","text":"<p>Print out a styled, labeled line (label: value) to the console. The label is bolded and colored with the INFO color, and the value is italicized.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>str</code> <p>The label of the line.</p> required <code>value</code> <code>str</code> <p>The value of the line.</p> required <code>indent</code> <code>int</code> <p>The number of spaces to indent the line. Defaults to 4.</p> <code>4</code> <code>new_lines</code> <code>int</code> <p>The number of new lines to print before the line. Defaults to 0.</p> <code>0</code> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def print_labeled_line(\n    self, label: str, value: str, indent: int = 4, new_lines: int = 0\n):\n    \"\"\"\n    Print out a styled, labeled line (label: value) to the console.\n    The label is bolded and colored with the INFO color,\n    and the value is italicized.\n\n    :param label: The label of the line.\n    :param value: The value of the line.\n    :param indent: The number of spaces to indent the line.\n        Defaults to 4.\n    :param new_lines: The number of new lines to print before the line.\n        Defaults to 0.\n    \"\"\"\n    self.print_line(\n        value=[label + \":\", value],\n        style=[\"bold \" + Colors.INFO, \"italic\"],\n        new_lines=new_lines,\n        indent=indent,\n    )\n</code></pre>"},{"location":"reference/guidellm/benchmark/output/#guidellm.benchmark.output.GenerativeBenchmarksConsole.print_line","title":"<code>print_line(value, style='', indent=0, new_lines=0)</code>","text":"<p>Print out a a value to the console as a line with optional indentation.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, list[str]]</code> <p>The value to print.</p> required <code>style</code> <code>Union[str, list[str]]</code> <p>The style to apply to the value. Defaults to none.</p> <code>''</code> <code>indent</code> <code>int</code> <p>The number of spaces to indent the line. Defaults to 0.</p> <code>0</code> <code>new_lines</code> <code>int</code> <p>The number of new lines to print before the value. Defaults to 0.</p> <code>0</code> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def print_line(\n    self,\n    value: Union[str, list[str]],\n    style: Union[str, list[str]] = \"\",\n    indent: int = 0,\n    new_lines: int = 0,\n):\n    \"\"\"\n    Print out a a value to the console as a line with optional indentation.\n\n    :param value: The value to print.\n    :param style: The style to apply to the value.\n        Defaults to none.\n    :param indent: The number of spaces to indent the line.\n        Defaults to 0.\n    :param new_lines: The number of new lines to print before the value.\n        Defaults to 0.\n    \"\"\"\n    if not self.enabled:\n        return\n\n    text = Text()\n\n    for _ in range(new_lines):\n        text.append(\"\\n\")\n\n    if not isinstance(value, list):\n        value = [value]\n\n    if not isinstance(style, list):\n        style = [style for _ in range(len(value))]\n\n    if len(value) != len(style):\n        raise ValueError(\n            f\"Value and style length mismatch. Value length: {len(value)}, \"\n            f\"Style length: {len(style)}.\"\n        )\n\n    for val, sty in zip(value, style):\n        text.append(val, style=sty)\n\n    self.console.print(Padding.indent(text, indent))\n</code></pre>"},{"location":"reference/guidellm/benchmark/output/#guidellm.benchmark.output.GenerativeBenchmarksConsole.print_section_header","title":"<code>print_section_header(title, indent=0, new_lines=2)</code>","text":"<p>Print out a styled section header to the console. The title is underlined, bolded, and colored with the INFO color.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>The title of the section.</p> required <code>indent</code> <code>int</code> <p>The number of spaces to indent the title. Defaults to 0.</p> <code>0</code> <code>new_lines</code> <code>int</code> <p>The number of new lines to print before the title. Defaults to 2.</p> <code>2</code> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def print_section_header(self, title: str, indent: int = 0, new_lines: int = 2):\n    \"\"\"\n    Print out a styled section header to the console.\n    The title is underlined, bolded, and colored with the INFO color.\n\n    :param title: The title of the section.\n    :param indent: The number of spaces to indent the title.\n        Defaults to 0.\n    :param new_lines: The number of new lines to print before the title.\n        Defaults to 2.\n    \"\"\"\n    self.print_line(\n        value=f\"{title}:\",\n        style=f\"bold underline {Colors.INFO}\",\n        indent=indent,\n        new_lines=new_lines,\n    )\n</code></pre>"},{"location":"reference/guidellm/benchmark/output/#guidellm.benchmark.output.GenerativeBenchmarksConsole.print_table","title":"<code>print_table(headers, rows, title, sections=None, max_char_per_col=2 ** 10, indent=0, new_lines=2)</code>","text":"<p>Print a table to the console with the given headers and rows.</p> <p>Parameters:</p> Name Type Description Default <code>headers</code> <code>list[str]</code> <p>The headers of the table.</p> required <code>rows</code> <code>list[list[Any]]</code> <p>The rows of the table.</p> required <code>title</code> <code>str</code> <p>The title of the table.</p> required <code>sections</code> <code>Optional[dict[str, tuple[int, int]]]</code> <p>The sections of the table grouping columns together. This is a mapping of the section display name to a tuple of the start and end column indices. If None, no sections are added (default).</p> <code>None</code> <code>max_char_per_col</code> <code>int</code> <p>The maximum number of characters per column.</p> <code>2 ** 10</code> <code>indent</code> <code>int</code> <p>The number of spaces to indent the table. Defaults to 0.</p> <code>0</code> <code>new_lines</code> <code>int</code> <p>The number of new lines to print before the table. Defaults to 0.</p> <code>2</code> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def print_table(\n    self,\n    headers: list[str],\n    rows: list[list[Any]],\n    title: str,\n    sections: Optional[dict[str, tuple[int, int]]] = None,\n    max_char_per_col: int = 2**10,\n    indent: int = 0,\n    new_lines: int = 2,\n):\n    \"\"\"\n    Print a table to the console with the given headers and rows.\n\n    :param headers: The headers of the table.\n    :param rows: The rows of the table.\n    :param title: The title of the table.\n    :param sections: The sections of the table grouping columns together.\n        This is a mapping of the section display name to a tuple of the start and\n        end column indices. If None, no sections are added (default).\n    :param max_char_per_col: The maximum number of characters per column.\n    :param indent: The number of spaces to indent the table.\n        Defaults to 0.\n    :param new_lines: The number of new lines to print before the table.\n        Defaults to 0.\n    \"\"\"\n\n    if rows and any(len(row) != len(headers) for row in rows):\n        raise ValueError(\n            f\"Headers and rows length mismatch. Headers length: {len(headers)}, \"\n            f\"Row length: {len(rows[0]) if rows else 'N/A'}.\"\n        )\n\n    max_characters_per_column = self.calculate_max_chars_per_column(\n        headers, rows, sections, max_char_per_col\n    )\n\n    self.print_section_header(title, indent=indent, new_lines=new_lines)\n    self.print_table_divider(\n        max_characters_per_column, include_separators=False, indent=indent\n    )\n    if sections:\n        self.print_table_sections(\n            sections, max_characters_per_column, indent=indent\n        )\n    self.print_table_row(\n        split_text_list_by_length(headers, max_characters_per_column),\n        style=f\"bold {Colors.INFO}\",\n        indent=indent,\n    )\n    self.print_table_divider(\n        max_characters_per_column, include_separators=True, indent=indent\n    )\n    for row in rows:\n        self.print_table_row(\n            split_text_list_by_length(row, max_characters_per_column),\n            style=\"italic\",\n            indent=indent,\n        )\n    self.print_table_divider(\n        max_characters_per_column, include_separators=False, indent=indent\n    )\n</code></pre>"},{"location":"reference/guidellm/benchmark/output/#guidellm.benchmark.output.GenerativeBenchmarksConsole.print_table_divider","title":"<code>print_table_divider(max_chars_per_column, include_separators, indent=0)</code>","text":"<p>Print a divider line for the table (top and bottom of table with '=' characters)</p> <p>Parameters:</p> Name Type Description Default <code>max_chars_per_column</code> <code>list[int]</code> <p>The maximum number of characters per column.</p> required <code>include_separators</code> <code>bool</code> <p>Whether to include separators between columns.</p> required <code>indent</code> <code>int</code> <p>The number of spaces to indent the line. Defaults to 0.</p> <code>0</code> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def print_table_divider(\n    self, max_chars_per_column: list[int], include_separators: bool, indent: int = 0\n):\n    \"\"\"\n    Print a divider line for the table (top and bottom of table with '=' characters)\n\n    :param max_chars_per_column: The maximum number of characters per column.\n    :param include_separators: Whether to include separators between columns.\n    :param indent: The number of spaces to indent the line.\n        Defaults to 0.\n    \"\"\"\n    if include_separators:\n        columns = [\n            settings.table_headers_border_char * max_chars\n            + settings.table_column_separator_char\n            + settings.table_headers_border_char\n            for max_chars in max_chars_per_column\n        ]\n    else:\n        columns = [\n            settings.table_border_char * (max_chars + 2)\n            for max_chars in max_chars_per_column\n        ]\n\n    columns[-1] = columns[-1][:-2]\n    self.print_line(value=columns, style=Colors.INFO, indent=indent)\n</code></pre>"},{"location":"reference/guidellm/benchmark/output/#guidellm.benchmark.output.GenerativeBenchmarksConsole.print_table_row","title":"<code>print_table_row(column_lines, style, indent=0)</code>","text":"<p>Print a single row of a table to the console.</p> <p>Parameters:</p> Name Type Description Default <code>column_lines</code> <code>list[list[str]]</code> <p>The lines of text to print for each column.</p> required <code>indent</code> <code>int</code> <p>The number of spaces to indent the line. Defaults to 0.</p> <code>0</code> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def print_table_row(\n    self, column_lines: list[list[str]], style: str, indent: int = 0\n):\n    \"\"\"\n    Print a single row of a table to the console.\n\n    :param column_lines: The lines of text to print for each column.\n    :param indent: The number of spaces to indent the line.\n        Defaults to 0.\n    \"\"\"\n    for row in range(len(column_lines[0])):\n        print_line = []\n        print_styles = []\n        for column in range(len(column_lines)):\n            print_line.extend(\n                [\n                    column_lines[column][row],\n                    settings.table_column_separator_char,\n                    \" \",\n                ]\n            )\n            print_styles.extend([style, Colors.INFO, \"\"])\n        print_line = print_line[:-2]\n        print_styles = print_styles[:-2]\n        self.print_line(value=print_line, style=print_styles, indent=indent)\n</code></pre>"},{"location":"reference/guidellm/benchmark/output/#guidellm.benchmark.output.GenerativeBenchmarksConsole.print_table_sections","title":"<code>print_table_sections(sections, max_chars_per_column, indent=0)</code>","text":"<p>Print the sections of the table with corresponding separators to the columns the sections are mapped to to ensure it is compliant with a CSV format. For example, a section named \"Metadata\" with columns 0-3 will print this: Metadata               ,,,, Where the spaces plus the separators at the end will span the columns 0-3. All columns must be accounted for in the sections.</p> <p>Parameters:</p> Name Type Description Default <code>sections</code> <code>dict[str, tuple[int, int]]</code> <p>The sections of the table.</p> required <code>max_chars_per_column</code> <code>list[int]</code> <p>The maximum number of characters per column.</p> required <code>indent</code> <code>int</code> <p>The number of spaces to indent the line. Defaults to 0.</p> <code>0</code> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def print_table_sections(\n    self,\n    sections: dict[str, tuple[int, int]],\n    max_chars_per_column: list[int],\n    indent: int = 0,\n):\n    \"\"\"\n    Print the sections of the table with corresponding separators to the columns\n    the sections are mapped to to ensure it is compliant with a CSV format.\n    For example, a section named \"Metadata\" with columns 0-3 will print this:\n    Metadata               ,,,,\n    Where the spaces plus the separators at the end will span the columns 0-3.\n    All columns must be accounted for in the sections.\n\n    :param sections: The sections of the table.\n    :param max_chars_per_column: The maximum number of characters per column.\n    :param indent: The number of spaces to indent the line.\n        Defaults to 0.\n    \"\"\"\n    section_tuples = [(start, end, name) for name, (start, end) in sections.items()]\n    section_tuples.sort(key=lambda x: x[0])\n\n    if any(start &gt; end for start, end, _ in section_tuples):\n        raise ValueError(f\"Invalid section ranges: {section_tuples}\")\n\n    if (\n        any(\n            section_tuples[ind][1] + 1 != section_tuples[ind + 1][0]\n            for ind in range(len(section_tuples) - 1)\n        )\n        or section_tuples[0][0] != 0\n        or section_tuples[-1][1] != len(max_chars_per_column) - 1\n    ):\n        raise ValueError(f\"Invalid section ranges: {section_tuples}\")\n\n    line_values = []\n    line_styles = []\n    for section, (start_col, end_col) in sections.items():\n        section_length = sum(max_chars_per_column[start_col : end_col + 1]) + 2 * (\n            end_col - start_col + 1\n        )\n        num_separators = end_col - start_col\n        line_values.append(section)\n        line_styles.append(\"bold \" + Colors.INFO)\n        line_values.append(\n            \" \" * (section_length - len(section) - num_separators - 2)\n        )\n        line_styles.append(\"\")\n        line_values.append(settings.table_column_separator_char * num_separators)\n        line_styles.append(\"\")\n        line_values.append(settings.table_column_separator_char + \" \")\n        line_styles.append(Colors.INFO)\n    line_values = line_values[:-1]\n    line_styles = line_styles[:-1]\n    self.print_line(value=line_values, style=line_styles, indent=indent)\n</code></pre>"},{"location":"reference/guidellm/benchmark/output/#guidellm.benchmark.output.GenerativeBenchmarksReport","title":"<code>GenerativeBenchmarksReport</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>A pydantic model representing a completed benchmark report. Contains a list of benchmarks along with convenience methods for finalizing and saving the report.</p> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>class GenerativeBenchmarksReport(StandardBaseModel):\n    \"\"\"\n    A pydantic model representing a completed benchmark report.\n    Contains a list of benchmarks along with convenience methods for finalizing\n    and saving the report.\n    \"\"\"\n\n    @staticmethod\n    def load_file(path: Union[str, Path]) -&gt; \"GenerativeBenchmarksReport\":\n        \"\"\"\n        Load a report from a file. The file type is determined by the file extension.\n        If the file is a directory, it expects a file named benchmarks.json under the\n        directory.\n\n        :param path: The path to load the report from.\n        :return: The loaded report.\n        \"\"\"\n        path, type_ = GenerativeBenchmarksReport._file_setup(path)\n\n        if type_ == \"json\":\n            with path.open(\"r\") as file:\n                model_dict = json.load(file)\n\n            return GenerativeBenchmarksReport.model_validate(model_dict)\n\n        if type_ == \"yaml\":\n            with path.open(\"r\") as file:\n                model_dict = yaml.safe_load(file)\n\n            return GenerativeBenchmarksReport.model_validate(model_dict)\n\n        if type_ == \"csv\":\n            raise ValueError(f\"CSV file type is not supported for loading: {path}.\")\n\n        raise ValueError(f\"Unsupported file type: {type_} for {path}.\")\n\n    benchmarks: list[GenerativeBenchmark] = Field(\n        description=\"The list of completed benchmarks contained within the report.\",\n        default_factory=list,\n    )\n\n    def set_sample_size(\n        self, sample_size: Optional[int]\n    ) -&gt; \"GenerativeBenchmarksReport\":\n        \"\"\"\n        Set the sample size for each benchmark in the report. In doing this, it will\n        reduce the contained requests of each benchmark to the sample size.\n        If sample size is None, it will return the report as is.\n\n        :param sample_size: The sample size to set for each benchmark.\n            If None, the report will be returned as is.\n        :return: The report with the sample size set for each benchmark.\n        \"\"\"\n\n        if sample_size is not None:\n            for benchmark in self.benchmarks:\n                benchmark.set_sample_size(sample_size)\n\n        return self\n\n    def save_file(self, path: Union[str, Path]) -&gt; Path:\n        \"\"\"\n        Save the report to a file. The file type is determined by the file extension.\n        If the file is a directory, it will save the report to a file named\n        benchmarks.json under the directory.\n\n        :param path: The path to save the report to.\n        :return: The path to the saved report.\n        \"\"\"\n        path, type_ = GenerativeBenchmarksReport._file_setup(path)\n\n        if type_ == \"json\":\n            return self.save_json(path)\n\n        if type_ == \"yaml\":\n            return self.save_yaml(path)\n\n        if type_ == \"csv\":\n            return self.save_csv(path)\n\n        raise ValueError(f\"Unsupported file type: {type_} for {path}.\")\n\n    def save_json(self, path: Union[str, Path]) -&gt; Path:\n        \"\"\"\n        Save the report to a JSON file containing all of the report data which is\n        reloadable using the pydantic model. If the file is a directory, it will save\n        the report to a file named benchmarks.json under the directory.\n\n        :param path: The path to save the report to.\n        :return: The path to the saved report.\n        \"\"\"\n        path, type_ = GenerativeBenchmarksReport._file_setup(path, \"json\")\n\n        if type_ != \"json\":\n            raise ValueError(\n                f\"Unsupported file type for saving a JSON: {type_} for {path}.\"\n            )\n\n        model_dict = self.model_dump()\n        model_json = json.dumps(model_dict)\n\n        with path.open(\"w\") as file:\n            file.write(model_json)\n\n        return path\n\n    def save_yaml(self, path: Union[str, Path]) -&gt; Path:\n        \"\"\"\n        Save the report to a YAML file containing all of the report data which is\n        reloadable using the pydantic model. If the file is a directory, it will save\n        the report to a file named benchmarks.yaml under the directory.\n\n        :param path: The path to save the report to.\n        :return: The path to the saved report.\n        \"\"\"\n\n        path, type_ = GenerativeBenchmarksReport._file_setup(path, \"yaml\")\n\n        if type_ != \"yaml\":\n            raise ValueError(\n                f\"Unsupported file type for saving a YAML: {type_} for {path}.\"\n            )\n\n        model_dict = self.model_dump()\n        model_yaml = yaml.dump(model_dict)\n\n        with path.open(\"w\") as file:\n            file.write(model_yaml)\n\n        return path\n\n    def save_csv(self, path: Union[str, Path]) -&gt; Path:\n        \"\"\"\n        Save the report to a CSV file containing the summarized statistics and values\n        for each report. Note, this data is not reloadable using the pydantic model.\n        If the file is a directory, it will save the report to a file named\n        benchmarks.csv under the directory.\n\n        :param path: The path to save the report to.\n        :return: The path to the saved report.\n        \"\"\"\n        path, type_ = GenerativeBenchmarksReport._file_setup(path, \"csv\")\n\n        if type_ != \"csv\":\n            raise ValueError(\n                f\"Unsupported file type for saving a CSV: {type_} for {path}.\"\n            )\n\n        with path.open(\"w\", newline=\"\") as file:\n            writer = csv.writer(file)\n            headers: list[str] = []\n            rows: list[list[Union[str, float, list[float]]]] = []\n\n            for benchmark in self.benchmarks:\n                benchmark_headers: list[str] = []\n                benchmark_values: list[Union[str, float, list[float]]] = []\n\n                desc_headers, desc_values = self._benchmark_desc_headers_and_values(\n                    benchmark\n                )\n                benchmark_headers += desc_headers\n                benchmark_values += desc_values\n\n                for status in StatusDistributionSummary.model_fields:\n                    status_headers, status_values = (\n                        self._benchmark_status_headers_and_values(benchmark, status)\n                    )\n                    benchmark_headers += status_headers\n                    benchmark_values += status_values\n\n                benchmark_extra_headers, benchmark_extra_values = (\n                    self._benchmark_extras_headers_and_values(benchmark)\n                )\n                benchmark_headers += benchmark_extra_headers\n                benchmark_values += benchmark_extra_values\n\n                if not headers:\n                    headers = benchmark_headers\n                rows.append(benchmark_values)\n\n            writer.writerow(headers)\n            for row in rows:\n                writer.writerow(row)\n\n        return path\n\n    @staticmethod\n    def _file_setup(\n        path: Union[str, Path],\n        default_file_type: Literal[\"json\", \"yaml\", \"csv\"] = \"json\",\n    ) -&gt; tuple[Path, Literal[\"json\", \"yaml\", \"csv\"]]:\n        path = Path(path) if not isinstance(path, Path) else path\n\n        if path.is_dir():\n            path = path / f\"benchmarks.{default_file_type}\"\n\n        path.parent.mkdir(parents=True, exist_ok=True)\n        path_suffix = path.suffix.lower()\n\n        if path_suffix == \".json\":\n            return path, \"json\"\n\n        if path_suffix in [\".yaml\", \".yml\"]:\n            return path, \"yaml\"\n\n        if path_suffix in [\".csv\"]:\n            return path, \"csv\"\n\n        raise ValueError(f\"Unsupported file extension: {path_suffix} for {path}.\")\n\n    @staticmethod\n    def _benchmark_desc_headers_and_values(\n        benchmark: GenerativeBenchmark,\n    ) -&gt; tuple[list[str], list[Union[str, float]]]:\n        headers = [\n            \"Type\",\n            \"Run Id\",\n            \"Id\",\n            \"Name\",\n            \"Start Time\",\n            \"End Time\",\n            \"Duration\",\n        ]\n        values: list[Union[str, float]] = [\n            benchmark.type_,\n            benchmark.run_id,\n            benchmark.id_,\n            strategy_display_str(benchmark.args.strategy),\n            datetime.fromtimestamp(benchmark.start_time).strftime(\"%Y-%m-%d %H:%M:%S\"),\n            datetime.fromtimestamp(benchmark.end_time).strftime(\"%Y-%m-%d %H:%M:%S\"),\n            benchmark.duration,\n        ]\n\n        if len(headers) != len(values):\n            raise ValueError(\"Headers and values length mismatch.\")\n\n        return headers, values\n\n    @staticmethod\n    def _benchmark_extras_headers_and_values(\n        benchmark: GenerativeBenchmark,\n    ) -&gt; tuple[list[str], list[str]]:\n        headers = [\"Args\", \"Worker\", \"Request Loader\", \"Extras\"]\n        values: list[str] = [\n            json.dumps(benchmark.args.model_dump()),\n            json.dumps(benchmark.worker.model_dump()),\n            json.dumps(benchmark.request_loader.model_dump()),\n            json.dumps(benchmark.extras),\n        ]\n\n        if len(headers) != len(values):\n            raise ValueError(\"Headers and values length mismatch.\")\n\n        return headers, values\n\n    @staticmethod\n    def _benchmark_status_headers_and_values(\n        benchmark: GenerativeBenchmark, status: str\n    ) -&gt; tuple[list[str], list[Union[float, list[float]]]]:\n        headers = [\n            f\"{status.capitalize()} Requests\",\n        ]\n        values = [\n            getattr(benchmark.request_totals, status),\n        ]\n\n        for metric in GenerativeMetrics.model_fields:\n            metric_headers, metric_values = (\n                GenerativeBenchmarksReport._benchmark_status_metrics_stats(\n                    benchmark, status, metric\n                )\n            )\n            headers += metric_headers\n            values += metric_values\n\n        if len(headers) != len(values):\n            raise ValueError(\"Headers and values length mismatch.\")\n\n        return headers, values\n\n    @staticmethod\n    def _benchmark_status_metrics_stats(\n        benchmark: GenerativeBenchmark,\n        status: str,\n        metric: str,\n    ) -&gt; tuple[list[str], list[Union[float, list[float]]]]:\n        status_display = status.capitalize()\n        metric_display = metric.replace(\"_\", \" \").capitalize()\n        status_dist_summary: StatusDistributionSummary = getattr(\n            benchmark.metrics, metric\n        )\n        dist_summary: DistributionSummary = getattr(status_dist_summary, status)\n        headers = [\n            f\"{status_display} {metric_display} mean\",\n            f\"{status_display} {metric_display} median\",\n            f\"{status_display} {metric_display} std dev\",\n            (\n                f\"{status_display} {metric_display} \"\n                \"[min, 0.1, 1, 5, 10, 25, 75, 90, 95, 99, max]\"\n            ),\n        ]\n        values: list[Union[float, list[float]]] = [\n            dist_summary.mean,\n            dist_summary.median,\n            dist_summary.std_dev,\n            [\n                dist_summary.min,\n                dist_summary.percentiles.p001,\n                dist_summary.percentiles.p01,\n                dist_summary.percentiles.p05,\n                dist_summary.percentiles.p10,\n                dist_summary.percentiles.p25,\n                dist_summary.percentiles.p75,\n                dist_summary.percentiles.p90,\n                dist_summary.percentiles.p95,\n                dist_summary.percentiles.p99,\n                dist_summary.max,\n            ],\n        ]\n\n        if len(headers) != len(values):\n            raise ValueError(\"Headers and values length mismatch.\")\n\n        return headers, values\n</code></pre>"},{"location":"reference/guidellm/benchmark/output/#guidellm.benchmark.output.GenerativeBenchmarksReport.load_file","title":"<code>load_file(path)</code>  <code>staticmethod</code>","text":"<p>Load a report from a file. The file type is determined by the file extension. If the file is a directory, it expects a file named benchmarks.json under the directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>The path to load the report from.</p> required <p>Returns:</p> Type Description <code>GenerativeBenchmarksReport</code> <p>The loaded report.</p> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>@staticmethod\ndef load_file(path: Union[str, Path]) -&gt; \"GenerativeBenchmarksReport\":\n    \"\"\"\n    Load a report from a file. The file type is determined by the file extension.\n    If the file is a directory, it expects a file named benchmarks.json under the\n    directory.\n\n    :param path: The path to load the report from.\n    :return: The loaded report.\n    \"\"\"\n    path, type_ = GenerativeBenchmarksReport._file_setup(path)\n\n    if type_ == \"json\":\n        with path.open(\"r\") as file:\n            model_dict = json.load(file)\n\n        return GenerativeBenchmarksReport.model_validate(model_dict)\n\n    if type_ == \"yaml\":\n        with path.open(\"r\") as file:\n            model_dict = yaml.safe_load(file)\n\n        return GenerativeBenchmarksReport.model_validate(model_dict)\n\n    if type_ == \"csv\":\n        raise ValueError(f\"CSV file type is not supported for loading: {path}.\")\n\n    raise ValueError(f\"Unsupported file type: {type_} for {path}.\")\n</code></pre>"},{"location":"reference/guidellm/benchmark/output/#guidellm.benchmark.output.GenerativeBenchmarksReport.save_csv","title":"<code>save_csv(path)</code>","text":"<p>Save the report to a CSV file containing the summarized statistics and values for each report. Note, this data is not reloadable using the pydantic model. If the file is a directory, it will save the report to a file named benchmarks.csv under the directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>The path to save the report to.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>The path to the saved report.</p> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def save_csv(self, path: Union[str, Path]) -&gt; Path:\n    \"\"\"\n    Save the report to a CSV file containing the summarized statistics and values\n    for each report. Note, this data is not reloadable using the pydantic model.\n    If the file is a directory, it will save the report to a file named\n    benchmarks.csv under the directory.\n\n    :param path: The path to save the report to.\n    :return: The path to the saved report.\n    \"\"\"\n    path, type_ = GenerativeBenchmarksReport._file_setup(path, \"csv\")\n\n    if type_ != \"csv\":\n        raise ValueError(\n            f\"Unsupported file type for saving a CSV: {type_} for {path}.\"\n        )\n\n    with path.open(\"w\", newline=\"\") as file:\n        writer = csv.writer(file)\n        headers: list[str] = []\n        rows: list[list[Union[str, float, list[float]]]] = []\n\n        for benchmark in self.benchmarks:\n            benchmark_headers: list[str] = []\n            benchmark_values: list[Union[str, float, list[float]]] = []\n\n            desc_headers, desc_values = self._benchmark_desc_headers_and_values(\n                benchmark\n            )\n            benchmark_headers += desc_headers\n            benchmark_values += desc_values\n\n            for status in StatusDistributionSummary.model_fields:\n                status_headers, status_values = (\n                    self._benchmark_status_headers_and_values(benchmark, status)\n                )\n                benchmark_headers += status_headers\n                benchmark_values += status_values\n\n            benchmark_extra_headers, benchmark_extra_values = (\n                self._benchmark_extras_headers_and_values(benchmark)\n            )\n            benchmark_headers += benchmark_extra_headers\n            benchmark_values += benchmark_extra_values\n\n            if not headers:\n                headers = benchmark_headers\n            rows.append(benchmark_values)\n\n        writer.writerow(headers)\n        for row in rows:\n            writer.writerow(row)\n\n    return path\n</code></pre>"},{"location":"reference/guidellm/benchmark/output/#guidellm.benchmark.output.GenerativeBenchmarksReport.save_file","title":"<code>save_file(path)</code>","text":"<p>Save the report to a file. The file type is determined by the file extension. If the file is a directory, it will save the report to a file named benchmarks.json under the directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>The path to save the report to.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>The path to the saved report.</p> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def save_file(self, path: Union[str, Path]) -&gt; Path:\n    \"\"\"\n    Save the report to a file. The file type is determined by the file extension.\n    If the file is a directory, it will save the report to a file named\n    benchmarks.json under the directory.\n\n    :param path: The path to save the report to.\n    :return: The path to the saved report.\n    \"\"\"\n    path, type_ = GenerativeBenchmarksReport._file_setup(path)\n\n    if type_ == \"json\":\n        return self.save_json(path)\n\n    if type_ == \"yaml\":\n        return self.save_yaml(path)\n\n    if type_ == \"csv\":\n        return self.save_csv(path)\n\n    raise ValueError(f\"Unsupported file type: {type_} for {path}.\")\n</code></pre>"},{"location":"reference/guidellm/benchmark/output/#guidellm.benchmark.output.GenerativeBenchmarksReport.save_json","title":"<code>save_json(path)</code>","text":"<p>Save the report to a JSON file containing all of the report data which is reloadable using the pydantic model. If the file is a directory, it will save the report to a file named benchmarks.json under the directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>The path to save the report to.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>The path to the saved report.</p> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def save_json(self, path: Union[str, Path]) -&gt; Path:\n    \"\"\"\n    Save the report to a JSON file containing all of the report data which is\n    reloadable using the pydantic model. If the file is a directory, it will save\n    the report to a file named benchmarks.json under the directory.\n\n    :param path: The path to save the report to.\n    :return: The path to the saved report.\n    \"\"\"\n    path, type_ = GenerativeBenchmarksReport._file_setup(path, \"json\")\n\n    if type_ != \"json\":\n        raise ValueError(\n            f\"Unsupported file type for saving a JSON: {type_} for {path}.\"\n        )\n\n    model_dict = self.model_dump()\n    model_json = json.dumps(model_dict)\n\n    with path.open(\"w\") as file:\n        file.write(model_json)\n\n    return path\n</code></pre>"},{"location":"reference/guidellm/benchmark/output/#guidellm.benchmark.output.GenerativeBenchmarksReport.save_yaml","title":"<code>save_yaml(path)</code>","text":"<p>Save the report to a YAML file containing all of the report data which is reloadable using the pydantic model. If the file is a directory, it will save the report to a file named benchmarks.yaml under the directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>The path to save the report to.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>The path to the saved report.</p> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def save_yaml(self, path: Union[str, Path]) -&gt; Path:\n    \"\"\"\n    Save the report to a YAML file containing all of the report data which is\n    reloadable using the pydantic model. If the file is a directory, it will save\n    the report to a file named benchmarks.yaml under the directory.\n\n    :param path: The path to save the report to.\n    :return: The path to the saved report.\n    \"\"\"\n\n    path, type_ = GenerativeBenchmarksReport._file_setup(path, \"yaml\")\n\n    if type_ != \"yaml\":\n        raise ValueError(\n            f\"Unsupported file type for saving a YAML: {type_} for {path}.\"\n        )\n\n    model_dict = self.model_dump()\n    model_yaml = yaml.dump(model_dict)\n\n    with path.open(\"w\") as file:\n        file.write(model_yaml)\n\n    return path\n</code></pre>"},{"location":"reference/guidellm/benchmark/output/#guidellm.benchmark.output.GenerativeBenchmarksReport.set_sample_size","title":"<code>set_sample_size(sample_size)</code>","text":"<p>Set the sample size for each benchmark in the report. In doing this, it will reduce the contained requests of each benchmark to the sample size. If sample size is None, it will return the report as is.</p> <p>Parameters:</p> Name Type Description Default <code>sample_size</code> <code>Optional[int]</code> <p>The sample size to set for each benchmark. If None, the report will be returned as is.</p> required <p>Returns:</p> Type Description <code>GenerativeBenchmarksReport</code> <p>The report with the sample size set for each benchmark.</p> Source code in <code>src/guidellm/benchmark/output.py</code> <pre><code>def set_sample_size(\n    self, sample_size: Optional[int]\n) -&gt; \"GenerativeBenchmarksReport\":\n    \"\"\"\n    Set the sample size for each benchmark in the report. In doing this, it will\n    reduce the contained requests of each benchmark to the sample size.\n    If sample size is None, it will return the report as is.\n\n    :param sample_size: The sample size to set for each benchmark.\n        If None, the report will be returned as is.\n    :return: The report with the sample size set for each benchmark.\n    \"\"\"\n\n    if sample_size is not None:\n        for benchmark in self.benchmarks:\n            benchmark.set_sample_size(sample_size)\n\n    return self\n</code></pre>"},{"location":"reference/guidellm/benchmark/profile/","title":"guidellm.benchmark.profile","text":""},{"location":"reference/guidellm/benchmark/progress/","title":"guidellm.benchmark.progress","text":""},{"location":"reference/guidellm/data/","title":"guidellm.data","text":"<p>Required for python &lt; 3.12 https://docs.python.org/3/library/importlib.resources.html#importlib.resources.files</p>"},{"location":"reference/guidellm/dataset/","title":"guidellm.dataset","text":""},{"location":"reference/guidellm/dataset/creator/","title":"guidellm.dataset.creator","text":""},{"location":"reference/guidellm/dataset/entrypoints/","title":"guidellm.dataset.entrypoints","text":""},{"location":"reference/guidellm/dataset/file/","title":"guidellm.dataset.file","text":""},{"location":"reference/guidellm/dataset/hf_datasets/","title":"guidellm.dataset.hf_datasets","text":""},{"location":"reference/guidellm/dataset/in_memory/","title":"guidellm.dataset.in_memory","text":""},{"location":"reference/guidellm/dataset/synthetic/","title":"guidellm.dataset.synthetic","text":""},{"location":"reference/guidellm/objects/","title":"guidellm.objects","text":""},{"location":"reference/guidellm/objects/#guidellm.objects.DistributionSummary","title":"<code>DistributionSummary</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>A pydantic model representing a statistical summary for a given distribution of numerical values.</p> Source code in <code>src/guidellm/objects/statistics.py</code> <pre><code>class DistributionSummary(StandardBaseModel):\n    \"\"\"\n    A pydantic model representing a statistical summary for a given\n    distribution of numerical values.\n    \"\"\"\n\n    mean: float = Field(\n        description=\"The mean/average of the distribution.\",\n    )\n    median: float = Field(\n        description=\"The median of the distribution.\",\n    )\n    mode: float = Field(\n        description=\"The mode of the distribution.\",\n    )\n    variance: float = Field(\n        description=\"The variance of the distribution.\",\n    )\n    std_dev: float = Field(\n        description=\"The standard deviation of the distribution.\",\n    )\n    min: float = Field(\n        description=\"The minimum value of the distribution.\",\n    )\n    max: float = Field(\n        description=\"The maximum value of the distribution.\",\n    )\n    count: int = Field(\n        description=\"The number of values in the distribution.\",\n    )\n    total_sum: float = Field(\n        description=\"The total sum of the values in the distribution.\",\n    )\n    percentiles: Percentiles = Field(\n        description=\"The percentiles of the distribution.\",\n    )\n    cumulative_distribution_function: Optional[list[tuple[float, float]]] = Field(\n        description=\"The cumulative distribution function (CDF) of the distribution.\",\n        default=None,\n    )\n\n    @staticmethod\n    def from_distribution_function(\n        distribution: list[tuple[float, float]],\n        include_cdf: bool = False,\n    ) -&gt; \"DistributionSummary\":\n        \"\"\"\n        Create a statistical summary for a given distribution of weighted numerical\n        values or a probability distribution function (PDF).\n        1.  If the distribution is a PDF, it is expected to be a list of tuples\n            where each tuple contains (value, probability). The sum of the\n            probabilities should be 1. If it is not, it will be normalized.\n        2.  If the distribution is a values distribution function, it is expected\n            to be a list of tuples where each tuple contains (value, weight).\n            The weights are normalized to a probability distribution function.\n\n        :param distribution: A list of tuples representing the distribution.\n            Each tuple contains (value, weight) or (value, probability).\n        :param include_cdf: Whether to include the calculated cumulative distribution\n            function (CDF) in the output DistributionSummary.\n        :return: An instance of DistributionSummary with calculated values.\n        \"\"\"\n        values, weights = zip(*distribution) if distribution else ([], [])\n        values = np.array(values)  # type: ignore[assignment]\n        weights = np.array(weights)  # type: ignore[assignment]\n\n        # create the PDF\n        probabilities = weights / np.sum(weights)  # type: ignore[operator]\n        pdf = np.column_stack((values, probabilities))\n        pdf = pdf[np.argsort(pdf[:, 0])]\n        values = pdf[:, 0]  # type: ignore[assignment]\n        probabilities = pdf[:, 1]\n\n        # calculate the CDF\n        cumulative_probabilities = np.cumsum(probabilities)\n        cdf = np.column_stack((values, cumulative_probabilities))\n\n        # calculate statistics\n        mean = np.sum(values * probabilities).item()  # type: ignore[attr-defined]\n        median = cdf[np.argmax(cdf[:, 1] &gt;= 0.5), 0].item() if len(cdf) &gt; 0 else 0  # noqa: PLR2004\n        mode = values[np.argmax(probabilities)].item() if len(values) &gt; 0 else 0  # type: ignore[call-overload]\n        variance = np.sum((values - mean) ** 2 * probabilities).item()  # type: ignore[attr-defined]\n        std_dev = math.sqrt(variance)\n        minimum = values[0].item() if len(values) &gt; 0 else 0\n        maximum = values[-1].item() if len(values) &gt; 0 else 0\n        count = len(values)\n        total_sum = np.sum(values).item()  # type: ignore[attr-defined]\n\n        return DistributionSummary(\n            mean=mean,\n            median=median,\n            mode=mode,\n            variance=variance,\n            std_dev=std_dev,\n            min=minimum,\n            max=maximum,\n            count=count,\n            total_sum=total_sum,\n            percentiles=(\n                Percentiles(\n                    p001=cdf[np.argmax(cdf[:, 1] &gt;= 0.001), 0].item(),  # noqa: PLR2004\n                    p01=cdf[np.argmax(cdf[:, 1] &gt;= 0.01), 0].item(),  # noqa: PLR2004\n                    p05=cdf[np.argmax(cdf[:, 1] &gt;= 0.05), 0].item(),  # noqa: PLR2004\n                    p10=cdf[np.argmax(cdf[:, 1] &gt;= 0.1), 0].item(),  # noqa: PLR2004\n                    p25=cdf[np.argmax(cdf[:, 1] &gt;= 0.25), 0].item(),  # noqa: PLR2004\n                    p75=cdf[np.argmax(cdf[:, 1] &gt;= 0.75), 0].item(),  # noqa: PLR2004\n                    p90=cdf[np.argmax(cdf[:, 1] &gt;= 0.9), 0].item(),  # noqa: PLR2004\n                    p95=cdf[np.argmax(cdf[:, 1] &gt;= 0.95), 0].item(),  # noqa: PLR2004\n                    p99=cdf[np.argmax(cdf[:, 1] &gt;= 0.99), 0].item(),  # noqa: PLR2004\n                    p999=cdf[np.argmax(cdf[:, 1] &gt;= 0.999), 0].item(),  # noqa: PLR2004\n                )\n                if len(cdf) &gt; 0\n                else Percentiles(\n                    p001=0,\n                    p01=0,\n                    p05=0,\n                    p10=0,\n                    p25=0,\n                    p75=0,\n                    p90=0,\n                    p95=0,\n                    p99=0,\n                    p999=0,\n                )\n            ),\n            cumulative_distribution_function=cdf.tolist() if include_cdf else None,\n        )\n\n    @staticmethod\n    def from_values(\n        values: list[float],\n        weights: Optional[list[float]] = None,\n        include_cdf: bool = False,\n    ) -&gt; \"DistributionSummary\":\n        \"\"\"\n        Create a statistical summary for a given distribution of numerical values.\n        This is a wrapper around from_distribution_function to handle the optional case\n        of including weights for the values. If weights are not provided, they are\n        automatically set to 1.0 for each value, so each value is equally weighted.\n\n        :param values: A list of numerical values representing the distribution.\n        :param weights: A list of weights for each value in the distribution.\n            If not provided, all values are equally weighted.\n        :param include_cdf: Whether to include the calculated cumulative distribution\n            function (CDF) in the output DistributionSummary.\n        \"\"\"\n        if weights is None:\n            weights = [1.0] * len(values)\n\n        if len(values) != len(weights):\n            raise ValueError(\n                \"The length of values and weights must be the same.\",\n            )\n\n        return DistributionSummary.from_distribution_function(\n            distribution=list(zip(values, weights)),\n            include_cdf=include_cdf,\n        )\n\n    @staticmethod\n    def from_request_times(\n        requests: list[tuple[float, float]],\n        distribution_type: Literal[\"concurrency\", \"rate\"],\n        include_cdf: bool = False,\n        epsilon: float = 1e-6,\n    ) -&gt; \"DistributionSummary\":\n        \"\"\"\n        Create a statistical summary for a given distribution of request times.\n        Specifically, this is used to measure concurrency or rate of requests\n        given an input list containing the start and end time of each request.\n        This will first convert the request times into a distribution function\n        and then calculate the statistics with from_distribution_function.\n\n        :param requests: A list of tuples representing the start and end times of\n            each request. Example: [(start_1, end_1), (start_2, end_2), ...]\n        :param distribution_type: The type of distribution to calculate.\n            Either \"concurrency\" or \"rate\".\n        :param include_cdf: Whether to include the calculated cumulative distribution\n            function (CDF) in the output DistributionSummary.\n        :param epsilon: The epsilon value for merging close events.\n        :return: An instance of DistributionSummary with calculated values.\n        \"\"\"\n        if distribution_type == \"concurrency\":\n            # convert to delta changes based on when requests were running\n            time_deltas: dict[float, int] = defaultdict(int)\n            for start, end in requests:\n                time_deltas[start] += 1\n                time_deltas[end] -= 1\n\n            # convert to the events over time measuring concurrency changes\n            events = []\n            active = 0\n\n            for time, delta in sorted(time_deltas.items()):\n                active += delta\n                events.append((time, active))\n        elif distribution_type == \"rate\":\n            # convert to events for when requests finished\n            global_start = min(start for start, _ in requests) if requests else 0\n            events = [(global_start, 1)] + [(end, 1) for _, end in requests]\n        else:\n            raise ValueError(\n                f\"Invalid distribution_type '{distribution_type}'. \"\n                \"Must be 'concurrency' or 'rate'.\"\n            )\n\n        # combine any events that are very close together\n        flattened_events: list[tuple[float, float]] = []\n        for time, val in sorted(events):\n            last_time, last_val = (\n                flattened_events[-1] if flattened_events else (None, None)\n            )\n\n            if (\n                last_time is not None\n                and last_val is not None\n                and abs(last_time - time) &lt;= epsilon\n            ):\n                flattened_events[-1] = (last_time, last_val + val)\n            else:\n                flattened_events.append((time, val))\n\n        # convert to value distribution function\n        distribution: dict[float, float] = defaultdict(float)\n\n        for ind in range(len(flattened_events) - 1):\n            start_time, value = flattened_events[ind]\n            end_time, _ = flattened_events[ind + 1]\n            duration = end_time - start_time\n\n            if distribution_type == \"concurrency\":\n                # weight the concurrency value by the duration\n                distribution[value] += duration\n            elif distribution_type == \"rate\":\n                # weight the rate value by the duration\n                rate = value / duration\n                distribution[rate] += duration\n\n        distribution_list: list[tuple[float, float]] = sorted(distribution.items())\n\n        return DistributionSummary.from_distribution_function(\n            distribution=distribution_list,\n            include_cdf=include_cdf,\n        )\n\n    @staticmethod\n    def from_iterable_request_times(\n        requests: list[tuple[float, float]],\n        first_iter_times: list[float],\n        iter_counts: list[int],\n        first_iter_counts: Optional[list[int]] = None,\n        include_cdf: bool = False,\n        epsilon: float = 1e-6,\n    ) -&gt; \"DistributionSummary\":\n        \"\"\"\n        Create a statistical summary for a given distribution of request times\n        for a request with iterable responses between the start and end.\n        For example, this is used to measure auto regressive requests where\n        a request is started and at some later point, iterative responses are\n        received. This will convert the request times and iterable values into\n        a distribution function and then calculate the statistics with\n        from_distribution_function.\n\n        :param requests: A list of tuples representing the start and end times of\n            each request. Example: [(start_1, end_1), (start_2, end_2), ...]\n        :param first_iter_times: A list of times when the first iteration of\n            each request was received. Must be the same length as requests.\n        :param iter_counts: A list of the total number of iterations for each\n            request that occurred starting at the first iteration and ending\n            at the request end time. Must be the same length as requests.\n        :param first_iter_counts: A list of the number of iterations to log\n            for the first iteration of each request. For example, when calculating\n            total number of tokens processed, this is set to the prompt tokens number.\n            If not provided, defaults to 1 for each request.\n        :param include_cdf: Whether to include the calculated cumulative distribution\n            function (CDF) in the output DistributionSummary.\n        :param epsilon: The epsilon value for merging close events.\n        :return: An instance of DistributionSummary with calculated values.\n        \"\"\"\n\n        if first_iter_counts is None:\n            first_iter_counts = [1] * len(requests)\n\n        if (\n            len(requests) != len(first_iter_times)\n            or len(requests) != len(iter_counts)\n            or len(requests) != len(first_iter_counts)\n        ):\n            raise ValueError(\n                \"requests, first_iter_times, iter_counts, and first_iter_counts must\"\n                \"be the same length.\"\n                f\"Given {len(requests)}, {len(first_iter_times)}, {len(iter_counts)}, \"\n                f\"{len(first_iter_counts)}\",\n            )\n\n        # first break up the requests into individual iterable events\n        events = defaultdict(int)\n        global_start = min(start for start, _ in requests) if requests else 0\n        global_end = max(end for _, end in requests) if requests else 0\n        events[global_start] = 0\n        events[global_end] = 0\n\n        for (_, end), first_iter, first_iter_count, total_count in zip(\n            requests, first_iter_times, first_iter_counts, iter_counts\n        ):\n            events[first_iter] += first_iter_count\n\n            if total_count &gt; 1:\n                iter_latency = (end - first_iter) / (total_count - 1)\n                for ind in range(1, total_count):\n                    events[first_iter + ind * iter_latency] += 1\n\n        # combine any events that are very close together\n        flattened_events: list[tuple[float, int]] = []\n\n        for time, count in sorted(events.items()):\n            last_time, last_count = (\n                flattened_events[-1] if flattened_events else (None, None)\n            )\n\n            if (\n                last_time is not None\n                and last_count is not None\n                and abs(last_time - time) &lt;= epsilon\n            ):\n                flattened_events[-1] = (last_time, last_count + count)\n            else:\n                flattened_events.append((time, count))\n\n        # convert to value distribution function\n        distribution: dict[float, float] = defaultdict(float)\n\n        for ind in range(len(flattened_events) - 1):\n            start_time, count = flattened_events[ind]\n            end_time, _ = flattened_events[ind + 1]\n            duration = end_time - start_time\n            rate = count / duration\n            distribution[rate] += duration\n\n        distribution_list = sorted(distribution.items())\n\n        return DistributionSummary.from_distribution_function(\n            distribution=distribution_list,\n            include_cdf=include_cdf,\n        )\n</code></pre>"},{"location":"reference/guidellm/objects/#guidellm.objects.DistributionSummary.from_distribution_function","title":"<code>from_distribution_function(distribution, include_cdf=False)</code>  <code>staticmethod</code>","text":"<p>Create a statistical summary for a given distribution of weighted numerical values or a probability distribution function (PDF). 1.  If the distribution is a PDF, it is expected to be a list of tuples     where each tuple contains (value, probability). The sum of the     probabilities should be 1. If it is not, it will be normalized. 2.  If the distribution is a values distribution function, it is expected     to be a list of tuples where each tuple contains (value, weight).     The weights are normalized to a probability distribution function.</p> <p>Parameters:</p> Name Type Description Default <code>distribution</code> <code>list[tuple[float, float]]</code> <p>A list of tuples representing the distribution. Each tuple contains (value, weight) or (value, probability).</p> required <code>include_cdf</code> <code>bool</code> <p>Whether to include the calculated cumulative distribution function (CDF) in the output DistributionSummary.</p> <code>False</code> <p>Returns:</p> Type Description <code>DistributionSummary</code> <p>An instance of DistributionSummary with calculated values.</p> Source code in <code>src/guidellm/objects/statistics.py</code> <pre><code>@staticmethod\ndef from_distribution_function(\n    distribution: list[tuple[float, float]],\n    include_cdf: bool = False,\n) -&gt; \"DistributionSummary\":\n    \"\"\"\n    Create a statistical summary for a given distribution of weighted numerical\n    values or a probability distribution function (PDF).\n    1.  If the distribution is a PDF, it is expected to be a list of tuples\n        where each tuple contains (value, probability). The sum of the\n        probabilities should be 1. If it is not, it will be normalized.\n    2.  If the distribution is a values distribution function, it is expected\n        to be a list of tuples where each tuple contains (value, weight).\n        The weights are normalized to a probability distribution function.\n\n    :param distribution: A list of tuples representing the distribution.\n        Each tuple contains (value, weight) or (value, probability).\n    :param include_cdf: Whether to include the calculated cumulative distribution\n        function (CDF) in the output DistributionSummary.\n    :return: An instance of DistributionSummary with calculated values.\n    \"\"\"\n    values, weights = zip(*distribution) if distribution else ([], [])\n    values = np.array(values)  # type: ignore[assignment]\n    weights = np.array(weights)  # type: ignore[assignment]\n\n    # create the PDF\n    probabilities = weights / np.sum(weights)  # type: ignore[operator]\n    pdf = np.column_stack((values, probabilities))\n    pdf = pdf[np.argsort(pdf[:, 0])]\n    values = pdf[:, 0]  # type: ignore[assignment]\n    probabilities = pdf[:, 1]\n\n    # calculate the CDF\n    cumulative_probabilities = np.cumsum(probabilities)\n    cdf = np.column_stack((values, cumulative_probabilities))\n\n    # calculate statistics\n    mean = np.sum(values * probabilities).item()  # type: ignore[attr-defined]\n    median = cdf[np.argmax(cdf[:, 1] &gt;= 0.5), 0].item() if len(cdf) &gt; 0 else 0  # noqa: PLR2004\n    mode = values[np.argmax(probabilities)].item() if len(values) &gt; 0 else 0  # type: ignore[call-overload]\n    variance = np.sum((values - mean) ** 2 * probabilities).item()  # type: ignore[attr-defined]\n    std_dev = math.sqrt(variance)\n    minimum = values[0].item() if len(values) &gt; 0 else 0\n    maximum = values[-1].item() if len(values) &gt; 0 else 0\n    count = len(values)\n    total_sum = np.sum(values).item()  # type: ignore[attr-defined]\n\n    return DistributionSummary(\n        mean=mean,\n        median=median,\n        mode=mode,\n        variance=variance,\n        std_dev=std_dev,\n        min=minimum,\n        max=maximum,\n        count=count,\n        total_sum=total_sum,\n        percentiles=(\n            Percentiles(\n                p001=cdf[np.argmax(cdf[:, 1] &gt;= 0.001), 0].item(),  # noqa: PLR2004\n                p01=cdf[np.argmax(cdf[:, 1] &gt;= 0.01), 0].item(),  # noqa: PLR2004\n                p05=cdf[np.argmax(cdf[:, 1] &gt;= 0.05), 0].item(),  # noqa: PLR2004\n                p10=cdf[np.argmax(cdf[:, 1] &gt;= 0.1), 0].item(),  # noqa: PLR2004\n                p25=cdf[np.argmax(cdf[:, 1] &gt;= 0.25), 0].item(),  # noqa: PLR2004\n                p75=cdf[np.argmax(cdf[:, 1] &gt;= 0.75), 0].item(),  # noqa: PLR2004\n                p90=cdf[np.argmax(cdf[:, 1] &gt;= 0.9), 0].item(),  # noqa: PLR2004\n                p95=cdf[np.argmax(cdf[:, 1] &gt;= 0.95), 0].item(),  # noqa: PLR2004\n                p99=cdf[np.argmax(cdf[:, 1] &gt;= 0.99), 0].item(),  # noqa: PLR2004\n                p999=cdf[np.argmax(cdf[:, 1] &gt;= 0.999), 0].item(),  # noqa: PLR2004\n            )\n            if len(cdf) &gt; 0\n            else Percentiles(\n                p001=0,\n                p01=0,\n                p05=0,\n                p10=0,\n                p25=0,\n                p75=0,\n                p90=0,\n                p95=0,\n                p99=0,\n                p999=0,\n            )\n        ),\n        cumulative_distribution_function=cdf.tolist() if include_cdf else None,\n    )\n</code></pre>"},{"location":"reference/guidellm/objects/#guidellm.objects.DistributionSummary.from_iterable_request_times","title":"<code>from_iterable_request_times(requests, first_iter_times, iter_counts, first_iter_counts=None, include_cdf=False, epsilon=1e-06)</code>  <code>staticmethod</code>","text":"<p>Create a statistical summary for a given distribution of request times for a request with iterable responses between the start and end. For example, this is used to measure auto regressive requests where a request is started and at some later point, iterative responses are received. This will convert the request times and iterable values into a distribution function and then calculate the statistics with from_distribution_function.</p> <p>Parameters:</p> Name Type Description Default <code>requests</code> <code>list[tuple[float, float]]</code> <p>A list of tuples representing the start and end times of each request. Example: [(start_1, end_1), (start_2, end_2), ...]</p> required <code>first_iter_times</code> <code>list[float]</code> <p>A list of times when the first iteration of each request was received. Must be the same length as requests.</p> required <code>iter_counts</code> <code>list[int]</code> <p>A list of the total number of iterations for each request that occurred starting at the first iteration and ending at the request end time. Must be the same length as requests.</p> required <code>first_iter_counts</code> <code>Optional[list[int]]</code> <p>A list of the number of iterations to log for the first iteration of each request. For example, when calculating total number of tokens processed, this is set to the prompt tokens number. If not provided, defaults to 1 for each request.</p> <code>None</code> <code>include_cdf</code> <code>bool</code> <p>Whether to include the calculated cumulative distribution function (CDF) in the output DistributionSummary.</p> <code>False</code> <code>epsilon</code> <code>float</code> <p>The epsilon value for merging close events.</p> <code>1e-06</code> <p>Returns:</p> Type Description <code>DistributionSummary</code> <p>An instance of DistributionSummary with calculated values.</p> Source code in <code>src/guidellm/objects/statistics.py</code> <pre><code>@staticmethod\ndef from_iterable_request_times(\n    requests: list[tuple[float, float]],\n    first_iter_times: list[float],\n    iter_counts: list[int],\n    first_iter_counts: Optional[list[int]] = None,\n    include_cdf: bool = False,\n    epsilon: float = 1e-6,\n) -&gt; \"DistributionSummary\":\n    \"\"\"\n    Create a statistical summary for a given distribution of request times\n    for a request with iterable responses between the start and end.\n    For example, this is used to measure auto regressive requests where\n    a request is started and at some later point, iterative responses are\n    received. This will convert the request times and iterable values into\n    a distribution function and then calculate the statistics with\n    from_distribution_function.\n\n    :param requests: A list of tuples representing the start and end times of\n        each request. Example: [(start_1, end_1), (start_2, end_2), ...]\n    :param first_iter_times: A list of times when the first iteration of\n        each request was received. Must be the same length as requests.\n    :param iter_counts: A list of the total number of iterations for each\n        request that occurred starting at the first iteration and ending\n        at the request end time. Must be the same length as requests.\n    :param first_iter_counts: A list of the number of iterations to log\n        for the first iteration of each request. For example, when calculating\n        total number of tokens processed, this is set to the prompt tokens number.\n        If not provided, defaults to 1 for each request.\n    :param include_cdf: Whether to include the calculated cumulative distribution\n        function (CDF) in the output DistributionSummary.\n    :param epsilon: The epsilon value for merging close events.\n    :return: An instance of DistributionSummary with calculated values.\n    \"\"\"\n\n    if first_iter_counts is None:\n        first_iter_counts = [1] * len(requests)\n\n    if (\n        len(requests) != len(first_iter_times)\n        or len(requests) != len(iter_counts)\n        or len(requests) != len(first_iter_counts)\n    ):\n        raise ValueError(\n            \"requests, first_iter_times, iter_counts, and first_iter_counts must\"\n            \"be the same length.\"\n            f\"Given {len(requests)}, {len(first_iter_times)}, {len(iter_counts)}, \"\n            f\"{len(first_iter_counts)}\",\n        )\n\n    # first break up the requests into individual iterable events\n    events = defaultdict(int)\n    global_start = min(start for start, _ in requests) if requests else 0\n    global_end = max(end for _, end in requests) if requests else 0\n    events[global_start] = 0\n    events[global_end] = 0\n\n    for (_, end), first_iter, first_iter_count, total_count in zip(\n        requests, first_iter_times, first_iter_counts, iter_counts\n    ):\n        events[first_iter] += first_iter_count\n\n        if total_count &gt; 1:\n            iter_latency = (end - first_iter) / (total_count - 1)\n            for ind in range(1, total_count):\n                events[first_iter + ind * iter_latency] += 1\n\n    # combine any events that are very close together\n    flattened_events: list[tuple[float, int]] = []\n\n    for time, count in sorted(events.items()):\n        last_time, last_count = (\n            flattened_events[-1] if flattened_events else (None, None)\n        )\n\n        if (\n            last_time is not None\n            and last_count is not None\n            and abs(last_time - time) &lt;= epsilon\n        ):\n            flattened_events[-1] = (last_time, last_count + count)\n        else:\n            flattened_events.append((time, count))\n\n    # convert to value distribution function\n    distribution: dict[float, float] = defaultdict(float)\n\n    for ind in range(len(flattened_events) - 1):\n        start_time, count = flattened_events[ind]\n        end_time, _ = flattened_events[ind + 1]\n        duration = end_time - start_time\n        rate = count / duration\n        distribution[rate] += duration\n\n    distribution_list = sorted(distribution.items())\n\n    return DistributionSummary.from_distribution_function(\n        distribution=distribution_list,\n        include_cdf=include_cdf,\n    )\n</code></pre>"},{"location":"reference/guidellm/objects/#guidellm.objects.DistributionSummary.from_request_times","title":"<code>from_request_times(requests, distribution_type, include_cdf=False, epsilon=1e-06)</code>  <code>staticmethod</code>","text":"<p>Create a statistical summary for a given distribution of request times. Specifically, this is used to measure concurrency or rate of requests given an input list containing the start and end time of each request. This will first convert the request times into a distribution function and then calculate the statistics with from_distribution_function.</p> <p>Parameters:</p> Name Type Description Default <code>requests</code> <code>list[tuple[float, float]]</code> <p>A list of tuples representing the start and end times of each request. Example: [(start_1, end_1), (start_2, end_2), ...]</p> required <code>distribution_type</code> <code>Literal['concurrency', 'rate']</code> <p>The type of distribution to calculate. Either \"concurrency\" or \"rate\".</p> required <code>include_cdf</code> <code>bool</code> <p>Whether to include the calculated cumulative distribution function (CDF) in the output DistributionSummary.</p> <code>False</code> <code>epsilon</code> <code>float</code> <p>The epsilon value for merging close events.</p> <code>1e-06</code> <p>Returns:</p> Type Description <code>DistributionSummary</code> <p>An instance of DistributionSummary with calculated values.</p> Source code in <code>src/guidellm/objects/statistics.py</code> <pre><code>@staticmethod\ndef from_request_times(\n    requests: list[tuple[float, float]],\n    distribution_type: Literal[\"concurrency\", \"rate\"],\n    include_cdf: bool = False,\n    epsilon: float = 1e-6,\n) -&gt; \"DistributionSummary\":\n    \"\"\"\n    Create a statistical summary for a given distribution of request times.\n    Specifically, this is used to measure concurrency or rate of requests\n    given an input list containing the start and end time of each request.\n    This will first convert the request times into a distribution function\n    and then calculate the statistics with from_distribution_function.\n\n    :param requests: A list of tuples representing the start and end times of\n        each request. Example: [(start_1, end_1), (start_2, end_2), ...]\n    :param distribution_type: The type of distribution to calculate.\n        Either \"concurrency\" or \"rate\".\n    :param include_cdf: Whether to include the calculated cumulative distribution\n        function (CDF) in the output DistributionSummary.\n    :param epsilon: The epsilon value for merging close events.\n    :return: An instance of DistributionSummary with calculated values.\n    \"\"\"\n    if distribution_type == \"concurrency\":\n        # convert to delta changes based on when requests were running\n        time_deltas: dict[float, int] = defaultdict(int)\n        for start, end in requests:\n            time_deltas[start] += 1\n            time_deltas[end] -= 1\n\n        # convert to the events over time measuring concurrency changes\n        events = []\n        active = 0\n\n        for time, delta in sorted(time_deltas.items()):\n            active += delta\n            events.append((time, active))\n    elif distribution_type == \"rate\":\n        # convert to events for when requests finished\n        global_start = min(start for start, _ in requests) if requests else 0\n        events = [(global_start, 1)] + [(end, 1) for _, end in requests]\n    else:\n        raise ValueError(\n            f\"Invalid distribution_type '{distribution_type}'. \"\n            \"Must be 'concurrency' or 'rate'.\"\n        )\n\n    # combine any events that are very close together\n    flattened_events: list[tuple[float, float]] = []\n    for time, val in sorted(events):\n        last_time, last_val = (\n            flattened_events[-1] if flattened_events else (None, None)\n        )\n\n        if (\n            last_time is not None\n            and last_val is not None\n            and abs(last_time - time) &lt;= epsilon\n        ):\n            flattened_events[-1] = (last_time, last_val + val)\n        else:\n            flattened_events.append((time, val))\n\n    # convert to value distribution function\n    distribution: dict[float, float] = defaultdict(float)\n\n    for ind in range(len(flattened_events) - 1):\n        start_time, value = flattened_events[ind]\n        end_time, _ = flattened_events[ind + 1]\n        duration = end_time - start_time\n\n        if distribution_type == \"concurrency\":\n            # weight the concurrency value by the duration\n            distribution[value] += duration\n        elif distribution_type == \"rate\":\n            # weight the rate value by the duration\n            rate = value / duration\n            distribution[rate] += duration\n\n    distribution_list: list[tuple[float, float]] = sorted(distribution.items())\n\n    return DistributionSummary.from_distribution_function(\n        distribution=distribution_list,\n        include_cdf=include_cdf,\n    )\n</code></pre>"},{"location":"reference/guidellm/objects/#guidellm.objects.DistributionSummary.from_values","title":"<code>from_values(values, weights=None, include_cdf=False)</code>  <code>staticmethod</code>","text":"<p>Create a statistical summary for a given distribution of numerical values. This is a wrapper around from_distribution_function to handle the optional case of including weights for the values. If weights are not provided, they are automatically set to 1.0 for each value, so each value is equally weighted.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>list[float]</code> <p>A list of numerical values representing the distribution.</p> required <code>weights</code> <code>Optional[list[float]]</code> <p>A list of weights for each value in the distribution. If not provided, all values are equally weighted.</p> <code>None</code> <code>include_cdf</code> <code>bool</code> <p>Whether to include the calculated cumulative distribution function (CDF) in the output DistributionSummary.</p> <code>False</code> Source code in <code>src/guidellm/objects/statistics.py</code> <pre><code>@staticmethod\ndef from_values(\n    values: list[float],\n    weights: Optional[list[float]] = None,\n    include_cdf: bool = False,\n) -&gt; \"DistributionSummary\":\n    \"\"\"\n    Create a statistical summary for a given distribution of numerical values.\n    This is a wrapper around from_distribution_function to handle the optional case\n    of including weights for the values. If weights are not provided, they are\n    automatically set to 1.0 for each value, so each value is equally weighted.\n\n    :param values: A list of numerical values representing the distribution.\n    :param weights: A list of weights for each value in the distribution.\n        If not provided, all values are equally weighted.\n    :param include_cdf: Whether to include the calculated cumulative distribution\n        function (CDF) in the output DistributionSummary.\n    \"\"\"\n    if weights is None:\n        weights = [1.0] * len(values)\n\n    if len(values) != len(weights):\n        raise ValueError(\n            \"The length of values and weights must be the same.\",\n        )\n\n    return DistributionSummary.from_distribution_function(\n        distribution=list(zip(values, weights)),\n        include_cdf=include_cdf,\n    )\n</code></pre>"},{"location":"reference/guidellm/objects/#guidellm.objects.Percentiles","title":"<code>Percentiles</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>A pydantic model representing the standard percentiles of a distribution.</p> Source code in <code>src/guidellm/objects/statistics.py</code> <pre><code>class Percentiles(StandardBaseModel):\n    \"\"\"\n    A pydantic model representing the standard percentiles of a distribution.\n    \"\"\"\n\n    p001: float = Field(\n        description=\"The 0.1th percentile of the distribution.\",\n    )\n    p01: float = Field(\n        description=\"The 1st percentile of the distribution.\",\n    )\n    p05: float = Field(\n        description=\"The 5th percentile of the distribution.\",\n    )\n    p10: float = Field(\n        description=\"The 10th percentile of the distribution.\",\n    )\n    p25: float = Field(\n        description=\"The 25th percentile of the distribution.\",\n    )\n    p75: float = Field(\n        description=\"The 75th percentile of the distribution.\",\n    )\n    p90: float = Field(\n        description=\"The 90th percentile of the distribution.\",\n    )\n    p95: float = Field(\n        description=\"The 95th percentile of the distribution.\",\n    )\n    p99: float = Field(\n        description=\"The 99th percentile of the distribution.\",\n    )\n    p999: float = Field(\n        description=\"The 99.9th percentile of the distribution.\",\n    )\n</code></pre>"},{"location":"reference/guidellm/objects/#guidellm.objects.RunningStats","title":"<code>RunningStats</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>Create a running statistics object to track the mean, rate, and other statistics of a stream of values. 1.  The start time is set to the time the object is created. 2.  The count is set to 0. 3.  The total is set to 0. 4.  The last value is set to 0. 5.  The mean is calculated as the total / count.</p> Source code in <code>src/guidellm/objects/statistics.py</code> <pre><code>class RunningStats(StandardBaseModel):\n    \"\"\"\n    Create a running statistics object to track the mean, rate, and other\n    statistics of a stream of values.\n    1.  The start time is set to the time the object is created.\n    2.  The count is set to 0.\n    3.  The total is set to 0.\n    4.  The last value is set to 0.\n    5.  The mean is calculated as the total / count.\n    \"\"\"\n\n    start_time: float = Field(\n        default_factory=timer.time,\n        description=(\n            \"The time the running statistics object was created. \"\n            \"This is used to calculate the rate of the statistics.\"\n        ),\n    )\n    count: int = Field(\n        default=0,\n        description=\"The number of values added to the running statistics.\",\n    )\n    total: float = Field(\n        default=0.0,\n        description=\"The total sum of the values added to the running statistics.\",\n    )\n    last: float = Field(\n        default=0.0,\n        description=\"The last value added to the running statistics.\",\n    )\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def mean(self) -&gt; float:\n        \"\"\"\n        :return: The mean of the running statistics (total / count).\n            If count is 0, return 0.0.\n        \"\"\"\n        if self.count == 0:\n            return 0.0\n        return self.total / self.count\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def rate(self) -&gt; float:\n        \"\"\"\n        :return: The rate of the running statistics\n            (total / (time.time() - start_time)).\n            If count is 0, return 0.0.\n        \"\"\"\n        if self.count == 0:\n            return 0.0\n        return self.total / (timer.time() - self.start_time)\n\n    def __add__(self, value: Any) -&gt; float:\n        \"\"\"\n        Enable the use of the + operator to add a value to the running statistics.\n\n        :param value: The value to add to the running statistics.\n        :return: The mean of the running statistics.\n        \"\"\"\n        if not isinstance(value, (int, float)):\n            raise ValueError(\n                f\"Value must be an int or float, got {type(value)} instead.\",\n            )\n\n        self.update(value)\n\n        return self.mean\n\n    def __iadd__(self, value: Any) -&gt; \"RunningStats\":\n        \"\"\"\n        Enable the use of the += operator to add a value to the running statistics.\n\n        :param value: The value to add to the running statistics.\n        :return: The running statistics object.\n        \"\"\"\n        if not isinstance(value, (int, float)):\n            raise ValueError(\n                f\"Value must be an int or float, got {type(value)} instead.\",\n            )\n\n        self.update(value)\n\n        return self\n\n    def update(self, value: float, count: int = 1) -&gt; None:\n        \"\"\"\n        Update the running statistics with a new value.\n\n        :param value: The new value to add to the running statistics.\n        :param count: The number of times to 'count' for the value.\n            If not provided, defaults to 1.\n        \"\"\"\n        self.count += count\n        self.total += value\n        self.last = value\n</code></pre>"},{"location":"reference/guidellm/objects/#guidellm.objects.RunningStats.mean","title":"<code>mean</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>float</code> <p>The mean of the running statistics (total / count). If count is 0, return 0.0.</p>"},{"location":"reference/guidellm/objects/#guidellm.objects.RunningStats.rate","title":"<code>rate</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>float</code> <p>The rate of the running statistics (total / (time.time() - start_time)). If count is 0, return 0.0.</p>"},{"location":"reference/guidellm/objects/#guidellm.objects.RunningStats.__add__","title":"<code>__add__(value)</code>","text":"<p>Enable the use of the + operator to add a value to the running statistics.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The value to add to the running statistics.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The mean of the running statistics.</p> Source code in <code>src/guidellm/objects/statistics.py</code> <pre><code>def __add__(self, value: Any) -&gt; float:\n    \"\"\"\n    Enable the use of the + operator to add a value to the running statistics.\n\n    :param value: The value to add to the running statistics.\n    :return: The mean of the running statistics.\n    \"\"\"\n    if not isinstance(value, (int, float)):\n        raise ValueError(\n            f\"Value must be an int or float, got {type(value)} instead.\",\n        )\n\n    self.update(value)\n\n    return self.mean\n</code></pre>"},{"location":"reference/guidellm/objects/#guidellm.objects.RunningStats.__iadd__","title":"<code>__iadd__(value)</code>","text":"<p>Enable the use of the += operator to add a value to the running statistics.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The value to add to the running statistics.</p> required <p>Returns:</p> Type Description <code>RunningStats</code> <p>The running statistics object.</p> Source code in <code>src/guidellm/objects/statistics.py</code> <pre><code>def __iadd__(self, value: Any) -&gt; \"RunningStats\":\n    \"\"\"\n    Enable the use of the += operator to add a value to the running statistics.\n\n    :param value: The value to add to the running statistics.\n    :return: The running statistics object.\n    \"\"\"\n    if not isinstance(value, (int, float)):\n        raise ValueError(\n            f\"Value must be an int or float, got {type(value)} instead.\",\n        )\n\n    self.update(value)\n\n    return self\n</code></pre>"},{"location":"reference/guidellm/objects/#guidellm.objects.RunningStats.update","title":"<code>update(value, count=1)</code>","text":"<p>Update the running statistics with a new value.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The new value to add to the running statistics.</p> required <code>count</code> <code>int</code> <p>The number of times to 'count' for the value. If not provided, defaults to 1.</p> <code>1</code> Source code in <code>src/guidellm/objects/statistics.py</code> <pre><code>def update(self, value: float, count: int = 1) -&gt; None:\n    \"\"\"\n    Update the running statistics with a new value.\n\n    :param value: The new value to add to the running statistics.\n    :param count: The number of times to 'count' for the value.\n        If not provided, defaults to 1.\n    \"\"\"\n    self.count += count\n    self.total += value\n    self.last = value\n</code></pre>"},{"location":"reference/guidellm/objects/#guidellm.objects.StandardBaseModel","title":"<code>StandardBaseModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A base class for Pydantic models throughout GuideLLM enabling standard configuration and logging.</p> Source code in <code>src/guidellm/objects/pydantic.py</code> <pre><code>class StandardBaseModel(BaseModel):\n    \"\"\"\n    A base class for Pydantic models throughout GuideLLM enabling standard\n    configuration and logging.\n    \"\"\"\n\n    model_config = ConfigDict(\n        extra=\"ignore\",\n        use_enum_values=True,\n        validate_assignment=True,\n        from_attributes=True,\n    )\n\n    def __init__(self, /, **data: Any) -&gt; None:\n        super().__init__(**data)\n        logger.debug(\n            \"Initialized new instance of {} with data: {}\",\n            self.__class__.__name__,\n            data,\n        )\n</code></pre>"},{"location":"reference/guidellm/objects/#guidellm.objects.StatusBreakdown","title":"<code>StatusBreakdown</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[SuccessfulT, ErroredT, IncompleteT, TotalT]</code></p> <p>A base class for Pydantic models that are separated by statuses including successful, incomplete, and errored. It additionally enables the inclusion of total, which is intended as the combination of all statuses. Total may or may not be used depending on if it duplicates information.</p> Source code in <code>src/guidellm/objects/pydantic.py</code> <pre><code>class StatusBreakdown(BaseModel, Generic[SuccessfulT, ErroredT, IncompleteT, TotalT]):\n    \"\"\"\n    A base class for Pydantic models that are separated by statuses including\n    successful, incomplete, and errored. It additionally enables the inclusion\n    of total, which is intended as the combination of all statuses.\n    Total may or may not be used depending on if it duplicates information.\n    \"\"\"\n\n    successful: SuccessfulT = Field(\n        description=\"The results with a successful status.\",\n        default=None,  # type: ignore[assignment]\n    )\n    errored: ErroredT = Field(\n        description=\"The results with an errored status.\",\n        default=None,  # type: ignore[assignment]\n    )\n    incomplete: IncompleteT = Field(\n        description=\"The results with an incomplete status.\",\n        default=None,  # type: ignore[assignment]\n    )\n    total: TotalT = Field(\n        description=\"The combination of all statuses.\",\n        default=None,  # type: ignore[assignment]\n    )\n</code></pre>"},{"location":"reference/guidellm/objects/#guidellm.objects.StatusDistributionSummary","title":"<code>StatusDistributionSummary</code>","text":"<p>               Bases: <code>StatusBreakdown[DistributionSummary, DistributionSummary, DistributionSummary, DistributionSummary]</code></p> <p>A pydantic model representing a statistical summary for a given distribution of numerical values grouped by status. Specifically used to represent the total, successful, incomplete, and errored values for a benchmark or other statistical summary.</p> Source code in <code>src/guidellm/objects/statistics.py</code> <pre><code>class StatusDistributionSummary(\n    StatusBreakdown[\n        DistributionSummary,\n        DistributionSummary,\n        DistributionSummary,\n        DistributionSummary,\n    ]\n):\n    \"\"\"\n    A pydantic model representing a statistical summary for a given\n    distribution of numerical values grouped by status.\n    Specifically used to represent the total, successful, incomplete,\n    and errored values for a benchmark or other statistical summary.\n    \"\"\"\n\n    @staticmethod\n    def from_values(\n        value_types: list[Literal[\"successful\", \"incomplete\", \"error\"]],\n        values: list[float],\n        weights: Optional[list[float]] = None,\n        include_cdf: bool = False,\n    ) -&gt; \"StatusDistributionSummary\":\n        \"\"\"\n        Create a statistical summary by status for a given distribution of numerical\n        values. This is used to measure the distribution of values for different\n        statuses (e.g., successful, incomplete, error) and calculate the statistics\n        for each status. Weights are optional to weight the probability distribution\n        for each value by. If not provided, all values are equally weighted.\n\n        :param value_types: A list of status types for each value in the distribution.\n            Must be one of 'successful', 'incomplete', or 'error'.\n        :param values: A list of numerical values representing the distribution.\n            Must be the same length as value_types.\n        :param weights: A list of weights for each value in the distribution.\n            If not provided, all values are equally weighted (set to 1).\n            Must be the same length as value_types.\n        :param include_cdf: Whether to include the calculated cumulative distribution\n            function (CDF) in the output StatusDistributionSummary.\n        :return: An instance of StatusDistributionSummary with calculated values.\n        \"\"\"\n        if any(\n            type_ not in {\"successful\", \"incomplete\", \"error\"} for type_ in value_types\n        ):\n            raise ValueError(\n                \"value_types must be one of 'successful', 'incomplete', or 'error'. \"\n                f\"Got {value_types} instead.\",\n            )\n\n        if weights is None:\n            weights = [1.0] * len(values)\n\n        if len(value_types) != len(values) or len(value_types) != len(weights):\n            raise ValueError(\n                \"The length of value_types, values, and weights must be the same.\",\n            )\n\n        _, successful_values, successful_weights = (\n            zip(*successful)\n            if (\n                successful := list(\n                    filter(\n                        lambda val: val[0] == \"successful\",\n                        zip(value_types, values, weights),\n                    )\n                )\n            )\n            else ([], [], [])\n        )\n        _, incomplete_values, incomplete_weights = (\n            zip(*incomplete)\n            if (\n                incomplete := list(\n                    filter(\n                        lambda val: val[0] == \"incomplete\",\n                        zip(value_types, values, weights),\n                    )\n                )\n            )\n            else ([], [], [])\n        )\n        _, errored_values, errored_weights = (\n            zip(*errored)\n            if (\n                errored := list(\n                    filter(\n                        lambda val: val[0] == \"error\",\n                        zip(value_types, values, weights),\n                    )\n                )\n            )\n            else ([], [], [])\n        )\n\n        return StatusDistributionSummary(\n            total=DistributionSummary.from_values(\n                values,\n                weights,\n                include_cdf=include_cdf,\n            ),\n            successful=DistributionSummary.from_values(\n                successful_values,  # type: ignore[arg-type]\n                successful_weights,  # type: ignore[arg-type]\n                include_cdf=include_cdf,\n            ),\n            incomplete=DistributionSummary.from_values(\n                incomplete_values,  # type: ignore[arg-type]\n                incomplete_weights,  # type: ignore[arg-type]\n                include_cdf=include_cdf,\n            ),\n            errored=DistributionSummary.from_values(\n                errored_values,  # type: ignore[arg-type]\n                errored_weights,  # type: ignore[arg-type]\n                include_cdf=include_cdf,\n            ),\n        )\n\n    @staticmethod\n    def from_request_times(\n        request_types: list[Literal[\"successful\", \"incomplete\", \"error\"]],\n        requests: list[tuple[float, float]],\n        distribution_type: Literal[\"concurrency\", \"rate\"],\n        include_cdf: bool = False,\n        epsilon: float = 1e-6,\n    ) -&gt; \"StatusDistributionSummary\":\n        \"\"\"\n        Create a statistical summary by status for given distribution of request times.\n        This is used to measure the distribution of request times for different statuses\n        (e.g., successful, incomplete, error) for concurrency and rates.\n        This will call into DistributionSummary.from_request_times to calculate\n        the statistics for each status.\n\n        :param request_types: List of status types for each request in the distribution.\n            Must be one of 'successful', 'incomplete', or 'error'.\n        :param requests: A list of tuples representing the start and end times of\n            each request. Example: [(start_1, end_1), (start_2, end_2), ...].\n            Must be the same length as request_types.\n        :param distribution_type: The type of distribution to calculate.\n            Either \"concurrency\" or \"rate\".\n        :param include_cdf: Whether to include the calculated cumulative distribution\n            function (CDF) in the output StatusDistributionSummary.\n        :param epsilon: The epsilon value for merging close events.\n        :return: An instance of StatusDistributionSummary with calculated values.\n        \"\"\"\n        if distribution_type not in {\"concurrency\", \"rate\"}:\n            raise ValueError(\n                f\"Invalid distribution_type '{distribution_type}'. \"\n                \"Must be 'concurrency' or 'rate'.\"\n            )\n\n        if any(\n            type_ not in {\"successful\", \"incomplete\", \"error\"}\n            for type_ in request_types\n        ):\n            raise ValueError(\n                \"request_types must be one of 'successful', 'incomplete', or 'error'. \"\n                f\"Got {request_types} instead.\",\n            )\n\n        if len(request_types) != len(requests):\n            raise ValueError(\n                \"The length of request_types and requests must be the same. \"\n                f\"Got {len(request_types)} and {len(requests)} instead.\",\n            )\n\n        _, successful_requests = (\n            zip(*successful)\n            if (\n                successful := list(\n                    filter(\n                        lambda val: val[0] == \"successful\",\n                        zip(request_types, requests),\n                    )\n                )\n            )\n            else ([], [])\n        )\n        _, incomplete_requests = (\n            zip(*incomplete)\n            if (\n                incomplete := list(\n                    filter(\n                        lambda val: val[0] == \"incomplete\",\n                        zip(request_types, requests),\n                    )\n                )\n            )\n            else ([], [])\n        )\n        _, errored_requests = (\n            zip(*errored)\n            if (\n                errored := list(\n                    filter(\n                        lambda val: val[0] == \"error\",\n                        zip(request_types, requests),\n                    )\n                )\n            )\n            else ([], [])\n        )\n\n        return StatusDistributionSummary(\n            total=DistributionSummary.from_request_times(\n                requests,\n                distribution_type=distribution_type,\n                include_cdf=include_cdf,\n                epsilon=epsilon,\n            ),\n            successful=DistributionSummary.from_request_times(\n                successful_requests,  # type: ignore[arg-type]\n                distribution_type=distribution_type,\n                include_cdf=include_cdf,\n                epsilon=epsilon,\n            ),\n            incomplete=DistributionSummary.from_request_times(\n                incomplete_requests,  # type: ignore[arg-type]\n                distribution_type=distribution_type,\n                include_cdf=include_cdf,\n                epsilon=epsilon,\n            ),\n            errored=DistributionSummary.from_request_times(\n                errored_requests,  # type: ignore[arg-type]\n                distribution_type=distribution_type,\n                include_cdf=include_cdf,\n                epsilon=epsilon,\n            ),\n        )\n\n    @staticmethod\n    def from_iterable_request_times(\n        request_types: list[Literal[\"successful\", \"incomplete\", \"error\"]],\n        requests: list[tuple[float, float]],\n        first_iter_times: list[float],\n        iter_counts: Optional[list[int]] = None,\n        first_iter_counts: Optional[list[int]] = None,\n        include_cdf: bool = False,\n        epsilon: float = 1e-6,\n    ) -&gt; \"StatusDistributionSummary\":\n        \"\"\"\n        Create a statistical summary by status for given distribution of request times\n        for a request with iterable responses between the start and end.\n        For example, this is used to measure auto regressive requests where\n        a request is started and at some later point, iterative responses are\n        received. This will call into DistributionSummary.from_iterable_request_times\n        to calculate the statistics for each status.\n\n        :param request_types: List of status types for each request in the distribution.\n            Must be one of 'successful', 'incomplete', or 'error'.\n        :param requests: A list of tuples representing the start and end times of\n            each request. Example: [(start_1, end_1), (start_2, end_2), ...].\n            Must be the same length as request_types.\n        :param first_iter_times: A list of times when the first iteration of\n            each request was received. Must be the same length as requests.\n        :param iter_counts: A list of the total number of iterations for each\n            request that occurred starting at the first iteration and ending\n            at the request end time. Must be the same length as requests.\n            If not provided, defaults to 1 for each request.\n        :param first_iter_counts: A list of the number of iterations to log\n            for the first iteration of each request. For example, when calculating\n            total number of tokens processed, this is set to the prompt tokens number.\n            If not provided, defaults to 1 for each request.\n        :param include_cdf: Whether to include the calculated cumulative distribution\n            function (CDF) in the output StatusDistributionSummary.\n        :param epsilon: The epsilon value for merging close events.\n        :return: An instance of StatusDistributionSummary with calculated values.\n        \"\"\"\n        if any(\n            type_ not in {\"successful\", \"incomplete\", \"error\"}\n            for type_ in request_types\n        ):\n            raise ValueError(\n                \"request_types must be one of 'successful', 'incomplete', or 'error'. \"\n                f\"Got {request_types} instead.\",\n            )\n\n        if iter_counts is None:\n            iter_counts = [1] * len(requests)\n\n        if first_iter_counts is None:\n            first_iter_counts = [1] * len(requests)\n\n        if (\n            len(request_types) != len(requests)\n            or len(requests) != len(first_iter_times)\n            or len(requests) != len(iter_counts)\n            or len(requests) != len(first_iter_counts)\n        ):\n            raise ValueError(\n                \"request_types, requests, first_iter_times, iter_counts, and \"\n                \"first_iter_counts must be the same length.\"\n                f\"Given {len(request_types)}, {len(requests)}, \"\n                f\"{len(first_iter_times)}, {len(iter_counts)}, \"\n                f\"{len(first_iter_counts)}\",\n            )\n\n        (\n            _,\n            successful_requests,\n            successful_first_iter_times,\n            successful_iter_counts,\n            successful_first_iter_counts,\n        ) = (\n            zip(*successful)\n            if (\n                successful := list(\n                    filter(\n                        lambda val: val[0] == \"successful\",\n                        zip(\n                            request_types,\n                            requests,\n                            first_iter_times,\n                            iter_counts,\n                            first_iter_counts,\n                        ),\n                    )\n                )\n            )\n            else ([], [], [], [], [])\n        )\n        (\n            _,\n            incomplete_requests,\n            incomplete_first_iter_times,\n            incomplete_iter_counts,\n            incomplete_first_iter_counts,\n        ) = (\n            zip(*incomplete)\n            if (\n                incomplete := list(\n                    filter(\n                        lambda val: val[0] == \"incomplete\",\n                        zip(\n                            request_types,\n                            requests,\n                            first_iter_times,\n                            iter_counts,\n                            first_iter_counts,\n                        ),\n                    )\n                )\n            )\n            else ([], [], [], [], [])\n        )\n        (\n            _,\n            errored_requests,\n            errored_first_iter_times,\n            errored_iter_counts,\n            errored_first_iter_counts,\n        ) = (\n            zip(*errored)\n            if (\n                errored := list(\n                    filter(\n                        lambda val: val[0] == \"error\",\n                        zip(\n                            request_types,\n                            requests,\n                            first_iter_times,\n                            iter_counts,\n                            first_iter_counts,\n                        ),\n                    )\n                )\n            )\n            else ([], [], [], [], [])\n        )\n\n        return StatusDistributionSummary(\n            total=DistributionSummary.from_iterable_request_times(\n                requests,\n                first_iter_times,\n                iter_counts,\n                first_iter_counts,\n                include_cdf=include_cdf,\n                epsilon=epsilon,\n            ),\n            successful=DistributionSummary.from_iterable_request_times(\n                successful_requests,  # type: ignore[arg-type]\n                successful_first_iter_times,  # type: ignore[arg-type]\n                successful_iter_counts,  # type: ignore[arg-type]\n                successful_first_iter_counts,  # type: ignore[arg-type]\n                include_cdf=include_cdf,\n                epsilon=epsilon,\n            ),\n            incomplete=DistributionSummary.from_iterable_request_times(\n                incomplete_requests,  # type: ignore[arg-type]\n                incomplete_first_iter_times,  # type: ignore[arg-type]\n                incomplete_iter_counts,  # type: ignore[arg-type]\n                incomplete_first_iter_counts,  # type: ignore[arg-type]\n                include_cdf=include_cdf,\n                epsilon=epsilon,\n            ),\n            errored=DistributionSummary.from_iterable_request_times(\n                errored_requests,  # type: ignore[arg-type]\n                errored_first_iter_times,  # type: ignore[arg-type]\n                errored_iter_counts,  # type: ignore[arg-type]\n                errored_first_iter_counts,  # type: ignore[arg-type]\n                include_cdf=include_cdf,\n                epsilon=epsilon,\n            ),\n        )\n</code></pre>"},{"location":"reference/guidellm/objects/#guidellm.objects.StatusDistributionSummary.from_iterable_request_times","title":"<code>from_iterable_request_times(request_types, requests, first_iter_times, iter_counts=None, first_iter_counts=None, include_cdf=False, epsilon=1e-06)</code>  <code>staticmethod</code>","text":"<p>Create a statistical summary by status for given distribution of request times for a request with iterable responses between the start and end. For example, this is used to measure auto regressive requests where a request is started and at some later point, iterative responses are received. This will call into DistributionSummary.from_iterable_request_times to calculate the statistics for each status.</p> <p>Parameters:</p> Name Type Description Default <code>request_types</code> <code>list[Literal['successful', 'incomplete', 'error']]</code> <p>List of status types for each request in the distribution. Must be one of 'successful', 'incomplete', or 'error'.</p> required <code>requests</code> <code>list[tuple[float, float]]</code> <p>A list of tuples representing the start and end times of each request. Example: [(start_1, end_1), (start_2, end_2), ...]. Must be the same length as request_types.</p> required <code>first_iter_times</code> <code>list[float]</code> <p>A list of times when the first iteration of each request was received. Must be the same length as requests.</p> required <code>iter_counts</code> <code>Optional[list[int]]</code> <p>A list of the total number of iterations for each request that occurred starting at the first iteration and ending at the request end time. Must be the same length as requests. If not provided, defaults to 1 for each request.</p> <code>None</code> <code>first_iter_counts</code> <code>Optional[list[int]]</code> <p>A list of the number of iterations to log for the first iteration of each request. For example, when calculating total number of tokens processed, this is set to the prompt tokens number. If not provided, defaults to 1 for each request.</p> <code>None</code> <code>include_cdf</code> <code>bool</code> <p>Whether to include the calculated cumulative distribution function (CDF) in the output StatusDistributionSummary.</p> <code>False</code> <code>epsilon</code> <code>float</code> <p>The epsilon value for merging close events.</p> <code>1e-06</code> <p>Returns:</p> Type Description <code>StatusDistributionSummary</code> <p>An instance of StatusDistributionSummary with calculated values.</p> Source code in <code>src/guidellm/objects/statistics.py</code> <pre><code>@staticmethod\ndef from_iterable_request_times(\n    request_types: list[Literal[\"successful\", \"incomplete\", \"error\"]],\n    requests: list[tuple[float, float]],\n    first_iter_times: list[float],\n    iter_counts: Optional[list[int]] = None,\n    first_iter_counts: Optional[list[int]] = None,\n    include_cdf: bool = False,\n    epsilon: float = 1e-6,\n) -&gt; \"StatusDistributionSummary\":\n    \"\"\"\n    Create a statistical summary by status for given distribution of request times\n    for a request with iterable responses between the start and end.\n    For example, this is used to measure auto regressive requests where\n    a request is started and at some later point, iterative responses are\n    received. This will call into DistributionSummary.from_iterable_request_times\n    to calculate the statistics for each status.\n\n    :param request_types: List of status types for each request in the distribution.\n        Must be one of 'successful', 'incomplete', or 'error'.\n    :param requests: A list of tuples representing the start and end times of\n        each request. Example: [(start_1, end_1), (start_2, end_2), ...].\n        Must be the same length as request_types.\n    :param first_iter_times: A list of times when the first iteration of\n        each request was received. Must be the same length as requests.\n    :param iter_counts: A list of the total number of iterations for each\n        request that occurred starting at the first iteration and ending\n        at the request end time. Must be the same length as requests.\n        If not provided, defaults to 1 for each request.\n    :param first_iter_counts: A list of the number of iterations to log\n        for the first iteration of each request. For example, when calculating\n        total number of tokens processed, this is set to the prompt tokens number.\n        If not provided, defaults to 1 for each request.\n    :param include_cdf: Whether to include the calculated cumulative distribution\n        function (CDF) in the output StatusDistributionSummary.\n    :param epsilon: The epsilon value for merging close events.\n    :return: An instance of StatusDistributionSummary with calculated values.\n    \"\"\"\n    if any(\n        type_ not in {\"successful\", \"incomplete\", \"error\"}\n        for type_ in request_types\n    ):\n        raise ValueError(\n            \"request_types must be one of 'successful', 'incomplete', or 'error'. \"\n            f\"Got {request_types} instead.\",\n        )\n\n    if iter_counts is None:\n        iter_counts = [1] * len(requests)\n\n    if first_iter_counts is None:\n        first_iter_counts = [1] * len(requests)\n\n    if (\n        len(request_types) != len(requests)\n        or len(requests) != len(first_iter_times)\n        or len(requests) != len(iter_counts)\n        or len(requests) != len(first_iter_counts)\n    ):\n        raise ValueError(\n            \"request_types, requests, first_iter_times, iter_counts, and \"\n            \"first_iter_counts must be the same length.\"\n            f\"Given {len(request_types)}, {len(requests)}, \"\n            f\"{len(first_iter_times)}, {len(iter_counts)}, \"\n            f\"{len(first_iter_counts)}\",\n        )\n\n    (\n        _,\n        successful_requests,\n        successful_first_iter_times,\n        successful_iter_counts,\n        successful_first_iter_counts,\n    ) = (\n        zip(*successful)\n        if (\n            successful := list(\n                filter(\n                    lambda val: val[0] == \"successful\",\n                    zip(\n                        request_types,\n                        requests,\n                        first_iter_times,\n                        iter_counts,\n                        first_iter_counts,\n                    ),\n                )\n            )\n        )\n        else ([], [], [], [], [])\n    )\n    (\n        _,\n        incomplete_requests,\n        incomplete_first_iter_times,\n        incomplete_iter_counts,\n        incomplete_first_iter_counts,\n    ) = (\n        zip(*incomplete)\n        if (\n            incomplete := list(\n                filter(\n                    lambda val: val[0] == \"incomplete\",\n                    zip(\n                        request_types,\n                        requests,\n                        first_iter_times,\n                        iter_counts,\n                        first_iter_counts,\n                    ),\n                )\n            )\n        )\n        else ([], [], [], [], [])\n    )\n    (\n        _,\n        errored_requests,\n        errored_first_iter_times,\n        errored_iter_counts,\n        errored_first_iter_counts,\n    ) = (\n        zip(*errored)\n        if (\n            errored := list(\n                filter(\n                    lambda val: val[0] == \"error\",\n                    zip(\n                        request_types,\n                        requests,\n                        first_iter_times,\n                        iter_counts,\n                        first_iter_counts,\n                    ),\n                )\n            )\n        )\n        else ([], [], [], [], [])\n    )\n\n    return StatusDistributionSummary(\n        total=DistributionSummary.from_iterable_request_times(\n            requests,\n            first_iter_times,\n            iter_counts,\n            first_iter_counts,\n            include_cdf=include_cdf,\n            epsilon=epsilon,\n        ),\n        successful=DistributionSummary.from_iterable_request_times(\n            successful_requests,  # type: ignore[arg-type]\n            successful_first_iter_times,  # type: ignore[arg-type]\n            successful_iter_counts,  # type: ignore[arg-type]\n            successful_first_iter_counts,  # type: ignore[arg-type]\n            include_cdf=include_cdf,\n            epsilon=epsilon,\n        ),\n        incomplete=DistributionSummary.from_iterable_request_times(\n            incomplete_requests,  # type: ignore[arg-type]\n            incomplete_first_iter_times,  # type: ignore[arg-type]\n            incomplete_iter_counts,  # type: ignore[arg-type]\n            incomplete_first_iter_counts,  # type: ignore[arg-type]\n            include_cdf=include_cdf,\n            epsilon=epsilon,\n        ),\n        errored=DistributionSummary.from_iterable_request_times(\n            errored_requests,  # type: ignore[arg-type]\n            errored_first_iter_times,  # type: ignore[arg-type]\n            errored_iter_counts,  # type: ignore[arg-type]\n            errored_first_iter_counts,  # type: ignore[arg-type]\n            include_cdf=include_cdf,\n            epsilon=epsilon,\n        ),\n    )\n</code></pre>"},{"location":"reference/guidellm/objects/#guidellm.objects.StatusDistributionSummary.from_request_times","title":"<code>from_request_times(request_types, requests, distribution_type, include_cdf=False, epsilon=1e-06)</code>  <code>staticmethod</code>","text":"<p>Create a statistical summary by status for given distribution of request times. This is used to measure the distribution of request times for different statuses (e.g., successful, incomplete, error) for concurrency and rates. This will call into DistributionSummary.from_request_times to calculate the statistics for each status.</p> <p>Parameters:</p> Name Type Description Default <code>request_types</code> <code>list[Literal['successful', 'incomplete', 'error']]</code> <p>List of status types for each request in the distribution. Must be one of 'successful', 'incomplete', or 'error'.</p> required <code>requests</code> <code>list[tuple[float, float]]</code> <p>A list of tuples representing the start and end times of each request. Example: [(start_1, end_1), (start_2, end_2), ...]. Must be the same length as request_types.</p> required <code>distribution_type</code> <code>Literal['concurrency', 'rate']</code> <p>The type of distribution to calculate. Either \"concurrency\" or \"rate\".</p> required <code>include_cdf</code> <code>bool</code> <p>Whether to include the calculated cumulative distribution function (CDF) in the output StatusDistributionSummary.</p> <code>False</code> <code>epsilon</code> <code>float</code> <p>The epsilon value for merging close events.</p> <code>1e-06</code> <p>Returns:</p> Type Description <code>StatusDistributionSummary</code> <p>An instance of StatusDistributionSummary with calculated values.</p> Source code in <code>src/guidellm/objects/statistics.py</code> <pre><code>@staticmethod\ndef from_request_times(\n    request_types: list[Literal[\"successful\", \"incomplete\", \"error\"]],\n    requests: list[tuple[float, float]],\n    distribution_type: Literal[\"concurrency\", \"rate\"],\n    include_cdf: bool = False,\n    epsilon: float = 1e-6,\n) -&gt; \"StatusDistributionSummary\":\n    \"\"\"\n    Create a statistical summary by status for given distribution of request times.\n    This is used to measure the distribution of request times for different statuses\n    (e.g., successful, incomplete, error) for concurrency and rates.\n    This will call into DistributionSummary.from_request_times to calculate\n    the statistics for each status.\n\n    :param request_types: List of status types for each request in the distribution.\n        Must be one of 'successful', 'incomplete', or 'error'.\n    :param requests: A list of tuples representing the start and end times of\n        each request. Example: [(start_1, end_1), (start_2, end_2), ...].\n        Must be the same length as request_types.\n    :param distribution_type: The type of distribution to calculate.\n        Either \"concurrency\" or \"rate\".\n    :param include_cdf: Whether to include the calculated cumulative distribution\n        function (CDF) in the output StatusDistributionSummary.\n    :param epsilon: The epsilon value for merging close events.\n    :return: An instance of StatusDistributionSummary with calculated values.\n    \"\"\"\n    if distribution_type not in {\"concurrency\", \"rate\"}:\n        raise ValueError(\n            f\"Invalid distribution_type '{distribution_type}'. \"\n            \"Must be 'concurrency' or 'rate'.\"\n        )\n\n    if any(\n        type_ not in {\"successful\", \"incomplete\", \"error\"}\n        for type_ in request_types\n    ):\n        raise ValueError(\n            \"request_types must be one of 'successful', 'incomplete', or 'error'. \"\n            f\"Got {request_types} instead.\",\n        )\n\n    if len(request_types) != len(requests):\n        raise ValueError(\n            \"The length of request_types and requests must be the same. \"\n            f\"Got {len(request_types)} and {len(requests)} instead.\",\n        )\n\n    _, successful_requests = (\n        zip(*successful)\n        if (\n            successful := list(\n                filter(\n                    lambda val: val[0] == \"successful\",\n                    zip(request_types, requests),\n                )\n            )\n        )\n        else ([], [])\n    )\n    _, incomplete_requests = (\n        zip(*incomplete)\n        if (\n            incomplete := list(\n                filter(\n                    lambda val: val[0] == \"incomplete\",\n                    zip(request_types, requests),\n                )\n            )\n        )\n        else ([], [])\n    )\n    _, errored_requests = (\n        zip(*errored)\n        if (\n            errored := list(\n                filter(\n                    lambda val: val[0] == \"error\",\n                    zip(request_types, requests),\n                )\n            )\n        )\n        else ([], [])\n    )\n\n    return StatusDistributionSummary(\n        total=DistributionSummary.from_request_times(\n            requests,\n            distribution_type=distribution_type,\n            include_cdf=include_cdf,\n            epsilon=epsilon,\n        ),\n        successful=DistributionSummary.from_request_times(\n            successful_requests,  # type: ignore[arg-type]\n            distribution_type=distribution_type,\n            include_cdf=include_cdf,\n            epsilon=epsilon,\n        ),\n        incomplete=DistributionSummary.from_request_times(\n            incomplete_requests,  # type: ignore[arg-type]\n            distribution_type=distribution_type,\n            include_cdf=include_cdf,\n            epsilon=epsilon,\n        ),\n        errored=DistributionSummary.from_request_times(\n            errored_requests,  # type: ignore[arg-type]\n            distribution_type=distribution_type,\n            include_cdf=include_cdf,\n            epsilon=epsilon,\n        ),\n    )\n</code></pre>"},{"location":"reference/guidellm/objects/#guidellm.objects.StatusDistributionSummary.from_values","title":"<code>from_values(value_types, values, weights=None, include_cdf=False)</code>  <code>staticmethod</code>","text":"<p>Create a statistical summary by status for a given distribution of numerical values. This is used to measure the distribution of values for different statuses (e.g., successful, incomplete, error) and calculate the statistics for each status. Weights are optional to weight the probability distribution for each value by. If not provided, all values are equally weighted.</p> <p>Parameters:</p> Name Type Description Default <code>value_types</code> <code>list[Literal['successful', 'incomplete', 'error']]</code> <p>A list of status types for each value in the distribution. Must be one of 'successful', 'incomplete', or 'error'.</p> required <code>values</code> <code>list[float]</code> <p>A list of numerical values representing the distribution. Must be the same length as value_types.</p> required <code>weights</code> <code>Optional[list[float]]</code> <p>A list of weights for each value in the distribution. If not provided, all values are equally weighted (set to 1). Must be the same length as value_types.</p> <code>None</code> <code>include_cdf</code> <code>bool</code> <p>Whether to include the calculated cumulative distribution function (CDF) in the output StatusDistributionSummary.</p> <code>False</code> <p>Returns:</p> Type Description <code>StatusDistributionSummary</code> <p>An instance of StatusDistributionSummary with calculated values.</p> Source code in <code>src/guidellm/objects/statistics.py</code> <pre><code>@staticmethod\ndef from_values(\n    value_types: list[Literal[\"successful\", \"incomplete\", \"error\"]],\n    values: list[float],\n    weights: Optional[list[float]] = None,\n    include_cdf: bool = False,\n) -&gt; \"StatusDistributionSummary\":\n    \"\"\"\n    Create a statistical summary by status for a given distribution of numerical\n    values. This is used to measure the distribution of values for different\n    statuses (e.g., successful, incomplete, error) and calculate the statistics\n    for each status. Weights are optional to weight the probability distribution\n    for each value by. If not provided, all values are equally weighted.\n\n    :param value_types: A list of status types for each value in the distribution.\n        Must be one of 'successful', 'incomplete', or 'error'.\n    :param values: A list of numerical values representing the distribution.\n        Must be the same length as value_types.\n    :param weights: A list of weights for each value in the distribution.\n        If not provided, all values are equally weighted (set to 1).\n        Must be the same length as value_types.\n    :param include_cdf: Whether to include the calculated cumulative distribution\n        function (CDF) in the output StatusDistributionSummary.\n    :return: An instance of StatusDistributionSummary with calculated values.\n    \"\"\"\n    if any(\n        type_ not in {\"successful\", \"incomplete\", \"error\"} for type_ in value_types\n    ):\n        raise ValueError(\n            \"value_types must be one of 'successful', 'incomplete', or 'error'. \"\n            f\"Got {value_types} instead.\",\n        )\n\n    if weights is None:\n        weights = [1.0] * len(values)\n\n    if len(value_types) != len(values) or len(value_types) != len(weights):\n        raise ValueError(\n            \"The length of value_types, values, and weights must be the same.\",\n        )\n\n    _, successful_values, successful_weights = (\n        zip(*successful)\n        if (\n            successful := list(\n                filter(\n                    lambda val: val[0] == \"successful\",\n                    zip(value_types, values, weights),\n                )\n            )\n        )\n        else ([], [], [])\n    )\n    _, incomplete_values, incomplete_weights = (\n        zip(*incomplete)\n        if (\n            incomplete := list(\n                filter(\n                    lambda val: val[0] == \"incomplete\",\n                    zip(value_types, values, weights),\n                )\n            )\n        )\n        else ([], [], [])\n    )\n    _, errored_values, errored_weights = (\n        zip(*errored)\n        if (\n            errored := list(\n                filter(\n                    lambda val: val[0] == \"error\",\n                    zip(value_types, values, weights),\n                )\n            )\n        )\n        else ([], [], [])\n    )\n\n    return StatusDistributionSummary(\n        total=DistributionSummary.from_values(\n            values,\n            weights,\n            include_cdf=include_cdf,\n        ),\n        successful=DistributionSummary.from_values(\n            successful_values,  # type: ignore[arg-type]\n            successful_weights,  # type: ignore[arg-type]\n            include_cdf=include_cdf,\n        ),\n        incomplete=DistributionSummary.from_values(\n            incomplete_values,  # type: ignore[arg-type]\n            incomplete_weights,  # type: ignore[arg-type]\n            include_cdf=include_cdf,\n        ),\n        errored=DistributionSummary.from_values(\n            errored_values,  # type: ignore[arg-type]\n            errored_weights,  # type: ignore[arg-type]\n            include_cdf=include_cdf,\n        ),\n    )\n</code></pre>"},{"location":"reference/guidellm/objects/#guidellm.objects.TimeRunningStats","title":"<code>TimeRunningStats</code>","text":"<p>               Bases: <code>RunningStats</code></p> <p>Create a running statistics object to track the mean, rate, and other statistics of a stream of time values. This is used to track time values in milliseconds and seconds.</p> <p>Adds time specific computed_fields such as measurements in milliseconds and seconds.</p> Source code in <code>src/guidellm/objects/statistics.py</code> <pre><code>class TimeRunningStats(RunningStats):\n    \"\"\"\n    Create a running statistics object to track the mean, rate, and other\n    statistics of a stream of time values. This is used to track time values\n    in milliseconds and seconds.\n\n    Adds time specific computed_fields such as measurements in milliseconds and seconds.\n    \"\"\"\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def total_ms(self) -&gt; float:\n        \"\"\"\n        :return: The total time multiplied by 1000.0 to convert to milliseconds.\n        \"\"\"\n        return self.total * 1000.0\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def last_ms(self) -&gt; float:\n        \"\"\"\n        :return: The last time multiplied by 1000.0 to convert to milliseconds.\n        \"\"\"\n        return self.last * 1000.0\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def mean_ms(self) -&gt; float:\n        \"\"\"\n        :return: The mean time multiplied by 1000.0 to convert to milliseconds.\n        \"\"\"\n        return self.mean * 1000.0\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def rate_ms(self) -&gt; float:\n        \"\"\"\n        :return: The rate of the running statistics multiplied by 1000.0\n            to convert to milliseconds.\n        \"\"\"\n        return self.rate * 1000.0\n</code></pre>"},{"location":"reference/guidellm/objects/#guidellm.objects.TimeRunningStats.last_ms","title":"<code>last_ms</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>float</code> <p>The last time multiplied by 1000.0 to convert to milliseconds.</p>"},{"location":"reference/guidellm/objects/#guidellm.objects.TimeRunningStats.mean_ms","title":"<code>mean_ms</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>float</code> <p>The mean time multiplied by 1000.0 to convert to milliseconds.</p>"},{"location":"reference/guidellm/objects/#guidellm.objects.TimeRunningStats.rate_ms","title":"<code>rate_ms</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>float</code> <p>The rate of the running statistics multiplied by 1000.0 to convert to milliseconds.</p>"},{"location":"reference/guidellm/objects/#guidellm.objects.TimeRunningStats.total_ms","title":"<code>total_ms</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>float</code> <p>The total time multiplied by 1000.0 to convert to milliseconds.</p>"},{"location":"reference/guidellm/objects/pydantic/","title":"guidellm.objects.pydantic","text":""},{"location":"reference/guidellm/objects/pydantic/#guidellm.objects.pydantic.StandardBaseModel","title":"<code>StandardBaseModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A base class for Pydantic models throughout GuideLLM enabling standard configuration and logging.</p> Source code in <code>src/guidellm/objects/pydantic.py</code> <pre><code>class StandardBaseModel(BaseModel):\n    \"\"\"\n    A base class for Pydantic models throughout GuideLLM enabling standard\n    configuration and logging.\n    \"\"\"\n\n    model_config = ConfigDict(\n        extra=\"ignore\",\n        use_enum_values=True,\n        validate_assignment=True,\n        from_attributes=True,\n    )\n\n    def __init__(self, /, **data: Any) -&gt; None:\n        super().__init__(**data)\n        logger.debug(\n            \"Initialized new instance of {} with data: {}\",\n            self.__class__.__name__,\n            data,\n        )\n</code></pre>"},{"location":"reference/guidellm/objects/pydantic/#guidellm.objects.pydantic.StatusBreakdown","title":"<code>StatusBreakdown</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[SuccessfulT, ErroredT, IncompleteT, TotalT]</code></p> <p>A base class for Pydantic models that are separated by statuses including successful, incomplete, and errored. It additionally enables the inclusion of total, which is intended as the combination of all statuses. Total may or may not be used depending on if it duplicates information.</p> Source code in <code>src/guidellm/objects/pydantic.py</code> <pre><code>class StatusBreakdown(BaseModel, Generic[SuccessfulT, ErroredT, IncompleteT, TotalT]):\n    \"\"\"\n    A base class for Pydantic models that are separated by statuses including\n    successful, incomplete, and errored. It additionally enables the inclusion\n    of total, which is intended as the combination of all statuses.\n    Total may or may not be used depending on if it duplicates information.\n    \"\"\"\n\n    successful: SuccessfulT = Field(\n        description=\"The results with a successful status.\",\n        default=None,  # type: ignore[assignment]\n    )\n    errored: ErroredT = Field(\n        description=\"The results with an errored status.\",\n        default=None,  # type: ignore[assignment]\n    )\n    incomplete: IncompleteT = Field(\n        description=\"The results with an incomplete status.\",\n        default=None,  # type: ignore[assignment]\n    )\n    total: TotalT = Field(\n        description=\"The combination of all statuses.\",\n        default=None,  # type: ignore[assignment]\n    )\n</code></pre>"},{"location":"reference/guidellm/objects/statistics/","title":"guidellm.objects.statistics","text":""},{"location":"reference/guidellm/objects/statistics/#guidellm.objects.statistics.DistributionSummary","title":"<code>DistributionSummary</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>A pydantic model representing a statistical summary for a given distribution of numerical values.</p> Source code in <code>src/guidellm/objects/statistics.py</code> <pre><code>class DistributionSummary(StandardBaseModel):\n    \"\"\"\n    A pydantic model representing a statistical summary for a given\n    distribution of numerical values.\n    \"\"\"\n\n    mean: float = Field(\n        description=\"The mean/average of the distribution.\",\n    )\n    median: float = Field(\n        description=\"The median of the distribution.\",\n    )\n    mode: float = Field(\n        description=\"The mode of the distribution.\",\n    )\n    variance: float = Field(\n        description=\"The variance of the distribution.\",\n    )\n    std_dev: float = Field(\n        description=\"The standard deviation of the distribution.\",\n    )\n    min: float = Field(\n        description=\"The minimum value of the distribution.\",\n    )\n    max: float = Field(\n        description=\"The maximum value of the distribution.\",\n    )\n    count: int = Field(\n        description=\"The number of values in the distribution.\",\n    )\n    total_sum: float = Field(\n        description=\"The total sum of the values in the distribution.\",\n    )\n    percentiles: Percentiles = Field(\n        description=\"The percentiles of the distribution.\",\n    )\n    cumulative_distribution_function: Optional[list[tuple[float, float]]] = Field(\n        description=\"The cumulative distribution function (CDF) of the distribution.\",\n        default=None,\n    )\n\n    @staticmethod\n    def from_distribution_function(\n        distribution: list[tuple[float, float]],\n        include_cdf: bool = False,\n    ) -&gt; \"DistributionSummary\":\n        \"\"\"\n        Create a statistical summary for a given distribution of weighted numerical\n        values or a probability distribution function (PDF).\n        1.  If the distribution is a PDF, it is expected to be a list of tuples\n            where each tuple contains (value, probability). The sum of the\n            probabilities should be 1. If it is not, it will be normalized.\n        2.  If the distribution is a values distribution function, it is expected\n            to be a list of tuples where each tuple contains (value, weight).\n            The weights are normalized to a probability distribution function.\n\n        :param distribution: A list of tuples representing the distribution.\n            Each tuple contains (value, weight) or (value, probability).\n        :param include_cdf: Whether to include the calculated cumulative distribution\n            function (CDF) in the output DistributionSummary.\n        :return: An instance of DistributionSummary with calculated values.\n        \"\"\"\n        values, weights = zip(*distribution) if distribution else ([], [])\n        values = np.array(values)  # type: ignore[assignment]\n        weights = np.array(weights)  # type: ignore[assignment]\n\n        # create the PDF\n        probabilities = weights / np.sum(weights)  # type: ignore[operator]\n        pdf = np.column_stack((values, probabilities))\n        pdf = pdf[np.argsort(pdf[:, 0])]\n        values = pdf[:, 0]  # type: ignore[assignment]\n        probabilities = pdf[:, 1]\n\n        # calculate the CDF\n        cumulative_probabilities = np.cumsum(probabilities)\n        cdf = np.column_stack((values, cumulative_probabilities))\n\n        # calculate statistics\n        mean = np.sum(values * probabilities).item()  # type: ignore[attr-defined]\n        median = cdf[np.argmax(cdf[:, 1] &gt;= 0.5), 0].item() if len(cdf) &gt; 0 else 0  # noqa: PLR2004\n        mode = values[np.argmax(probabilities)].item() if len(values) &gt; 0 else 0  # type: ignore[call-overload]\n        variance = np.sum((values - mean) ** 2 * probabilities).item()  # type: ignore[attr-defined]\n        std_dev = math.sqrt(variance)\n        minimum = values[0].item() if len(values) &gt; 0 else 0\n        maximum = values[-1].item() if len(values) &gt; 0 else 0\n        count = len(values)\n        total_sum = np.sum(values).item()  # type: ignore[attr-defined]\n\n        return DistributionSummary(\n            mean=mean,\n            median=median,\n            mode=mode,\n            variance=variance,\n            std_dev=std_dev,\n            min=minimum,\n            max=maximum,\n            count=count,\n            total_sum=total_sum,\n            percentiles=(\n                Percentiles(\n                    p001=cdf[np.argmax(cdf[:, 1] &gt;= 0.001), 0].item(),  # noqa: PLR2004\n                    p01=cdf[np.argmax(cdf[:, 1] &gt;= 0.01), 0].item(),  # noqa: PLR2004\n                    p05=cdf[np.argmax(cdf[:, 1] &gt;= 0.05), 0].item(),  # noqa: PLR2004\n                    p10=cdf[np.argmax(cdf[:, 1] &gt;= 0.1), 0].item(),  # noqa: PLR2004\n                    p25=cdf[np.argmax(cdf[:, 1] &gt;= 0.25), 0].item(),  # noqa: PLR2004\n                    p75=cdf[np.argmax(cdf[:, 1] &gt;= 0.75), 0].item(),  # noqa: PLR2004\n                    p90=cdf[np.argmax(cdf[:, 1] &gt;= 0.9), 0].item(),  # noqa: PLR2004\n                    p95=cdf[np.argmax(cdf[:, 1] &gt;= 0.95), 0].item(),  # noqa: PLR2004\n                    p99=cdf[np.argmax(cdf[:, 1] &gt;= 0.99), 0].item(),  # noqa: PLR2004\n                    p999=cdf[np.argmax(cdf[:, 1] &gt;= 0.999), 0].item(),  # noqa: PLR2004\n                )\n                if len(cdf) &gt; 0\n                else Percentiles(\n                    p001=0,\n                    p01=0,\n                    p05=0,\n                    p10=0,\n                    p25=0,\n                    p75=0,\n                    p90=0,\n                    p95=0,\n                    p99=0,\n                    p999=0,\n                )\n            ),\n            cumulative_distribution_function=cdf.tolist() if include_cdf else None,\n        )\n\n    @staticmethod\n    def from_values(\n        values: list[float],\n        weights: Optional[list[float]] = None,\n        include_cdf: bool = False,\n    ) -&gt; \"DistributionSummary\":\n        \"\"\"\n        Create a statistical summary for a given distribution of numerical values.\n        This is a wrapper around from_distribution_function to handle the optional case\n        of including weights for the values. If weights are not provided, they are\n        automatically set to 1.0 for each value, so each value is equally weighted.\n\n        :param values: A list of numerical values representing the distribution.\n        :param weights: A list of weights for each value in the distribution.\n            If not provided, all values are equally weighted.\n        :param include_cdf: Whether to include the calculated cumulative distribution\n            function (CDF) in the output DistributionSummary.\n        \"\"\"\n        if weights is None:\n            weights = [1.0] * len(values)\n\n        if len(values) != len(weights):\n            raise ValueError(\n                \"The length of values and weights must be the same.\",\n            )\n\n        return DistributionSummary.from_distribution_function(\n            distribution=list(zip(values, weights)),\n            include_cdf=include_cdf,\n        )\n\n    @staticmethod\n    def from_request_times(\n        requests: list[tuple[float, float]],\n        distribution_type: Literal[\"concurrency\", \"rate\"],\n        include_cdf: bool = False,\n        epsilon: float = 1e-6,\n    ) -&gt; \"DistributionSummary\":\n        \"\"\"\n        Create a statistical summary for a given distribution of request times.\n        Specifically, this is used to measure concurrency or rate of requests\n        given an input list containing the start and end time of each request.\n        This will first convert the request times into a distribution function\n        and then calculate the statistics with from_distribution_function.\n\n        :param requests: A list of tuples representing the start and end times of\n            each request. Example: [(start_1, end_1), (start_2, end_2), ...]\n        :param distribution_type: The type of distribution to calculate.\n            Either \"concurrency\" or \"rate\".\n        :param include_cdf: Whether to include the calculated cumulative distribution\n            function (CDF) in the output DistributionSummary.\n        :param epsilon: The epsilon value for merging close events.\n        :return: An instance of DistributionSummary with calculated values.\n        \"\"\"\n        if distribution_type == \"concurrency\":\n            # convert to delta changes based on when requests were running\n            time_deltas: dict[float, int] = defaultdict(int)\n            for start, end in requests:\n                time_deltas[start] += 1\n                time_deltas[end] -= 1\n\n            # convert to the events over time measuring concurrency changes\n            events = []\n            active = 0\n\n            for time, delta in sorted(time_deltas.items()):\n                active += delta\n                events.append((time, active))\n        elif distribution_type == \"rate\":\n            # convert to events for when requests finished\n            global_start = min(start for start, _ in requests) if requests else 0\n            events = [(global_start, 1)] + [(end, 1) for _, end in requests]\n        else:\n            raise ValueError(\n                f\"Invalid distribution_type '{distribution_type}'. \"\n                \"Must be 'concurrency' or 'rate'.\"\n            )\n\n        # combine any events that are very close together\n        flattened_events: list[tuple[float, float]] = []\n        for time, val in sorted(events):\n            last_time, last_val = (\n                flattened_events[-1] if flattened_events else (None, None)\n            )\n\n            if (\n                last_time is not None\n                and last_val is not None\n                and abs(last_time - time) &lt;= epsilon\n            ):\n                flattened_events[-1] = (last_time, last_val + val)\n            else:\n                flattened_events.append((time, val))\n\n        # convert to value distribution function\n        distribution: dict[float, float] = defaultdict(float)\n\n        for ind in range(len(flattened_events) - 1):\n            start_time, value = flattened_events[ind]\n            end_time, _ = flattened_events[ind + 1]\n            duration = end_time - start_time\n\n            if distribution_type == \"concurrency\":\n                # weight the concurrency value by the duration\n                distribution[value] += duration\n            elif distribution_type == \"rate\":\n                # weight the rate value by the duration\n                rate = value / duration\n                distribution[rate] += duration\n\n        distribution_list: list[tuple[float, float]] = sorted(distribution.items())\n\n        return DistributionSummary.from_distribution_function(\n            distribution=distribution_list,\n            include_cdf=include_cdf,\n        )\n\n    @staticmethod\n    def from_iterable_request_times(\n        requests: list[tuple[float, float]],\n        first_iter_times: list[float],\n        iter_counts: list[int],\n        first_iter_counts: Optional[list[int]] = None,\n        include_cdf: bool = False,\n        epsilon: float = 1e-6,\n    ) -&gt; \"DistributionSummary\":\n        \"\"\"\n        Create a statistical summary for a given distribution of request times\n        for a request with iterable responses between the start and end.\n        For example, this is used to measure auto regressive requests where\n        a request is started and at some later point, iterative responses are\n        received. This will convert the request times and iterable values into\n        a distribution function and then calculate the statistics with\n        from_distribution_function.\n\n        :param requests: A list of tuples representing the start and end times of\n            each request. Example: [(start_1, end_1), (start_2, end_2), ...]\n        :param first_iter_times: A list of times when the first iteration of\n            each request was received. Must be the same length as requests.\n        :param iter_counts: A list of the total number of iterations for each\n            request that occurred starting at the first iteration and ending\n            at the request end time. Must be the same length as requests.\n        :param first_iter_counts: A list of the number of iterations to log\n            for the first iteration of each request. For example, when calculating\n            total number of tokens processed, this is set to the prompt tokens number.\n            If not provided, defaults to 1 for each request.\n        :param include_cdf: Whether to include the calculated cumulative distribution\n            function (CDF) in the output DistributionSummary.\n        :param epsilon: The epsilon value for merging close events.\n        :return: An instance of DistributionSummary with calculated values.\n        \"\"\"\n\n        if first_iter_counts is None:\n            first_iter_counts = [1] * len(requests)\n\n        if (\n            len(requests) != len(first_iter_times)\n            or len(requests) != len(iter_counts)\n            or len(requests) != len(first_iter_counts)\n        ):\n            raise ValueError(\n                \"requests, first_iter_times, iter_counts, and first_iter_counts must\"\n                \"be the same length.\"\n                f\"Given {len(requests)}, {len(first_iter_times)}, {len(iter_counts)}, \"\n                f\"{len(first_iter_counts)}\",\n            )\n\n        # first break up the requests into individual iterable events\n        events = defaultdict(int)\n        global_start = min(start for start, _ in requests) if requests else 0\n        global_end = max(end for _, end in requests) if requests else 0\n        events[global_start] = 0\n        events[global_end] = 0\n\n        for (_, end), first_iter, first_iter_count, total_count in zip(\n            requests, first_iter_times, first_iter_counts, iter_counts\n        ):\n            events[first_iter] += first_iter_count\n\n            if total_count &gt; 1:\n                iter_latency = (end - first_iter) / (total_count - 1)\n                for ind in range(1, total_count):\n                    events[first_iter + ind * iter_latency] += 1\n\n        # combine any events that are very close together\n        flattened_events: list[tuple[float, int]] = []\n\n        for time, count in sorted(events.items()):\n            last_time, last_count = (\n                flattened_events[-1] if flattened_events else (None, None)\n            )\n\n            if (\n                last_time is not None\n                and last_count is not None\n                and abs(last_time - time) &lt;= epsilon\n            ):\n                flattened_events[-1] = (last_time, last_count + count)\n            else:\n                flattened_events.append((time, count))\n\n        # convert to value distribution function\n        distribution: dict[float, float] = defaultdict(float)\n\n        for ind in range(len(flattened_events) - 1):\n            start_time, count = flattened_events[ind]\n            end_time, _ = flattened_events[ind + 1]\n            duration = end_time - start_time\n            rate = count / duration\n            distribution[rate] += duration\n\n        distribution_list = sorted(distribution.items())\n\n        return DistributionSummary.from_distribution_function(\n            distribution=distribution_list,\n            include_cdf=include_cdf,\n        )\n</code></pre>"},{"location":"reference/guidellm/objects/statistics/#guidellm.objects.statistics.DistributionSummary.from_distribution_function","title":"<code>from_distribution_function(distribution, include_cdf=False)</code>  <code>staticmethod</code>","text":"<p>Create a statistical summary for a given distribution of weighted numerical values or a probability distribution function (PDF). 1.  If the distribution is a PDF, it is expected to be a list of tuples     where each tuple contains (value, probability). The sum of the     probabilities should be 1. If it is not, it will be normalized. 2.  If the distribution is a values distribution function, it is expected     to be a list of tuples where each tuple contains (value, weight).     The weights are normalized to a probability distribution function.</p> <p>Parameters:</p> Name Type Description Default <code>distribution</code> <code>list[tuple[float, float]]</code> <p>A list of tuples representing the distribution. Each tuple contains (value, weight) or (value, probability).</p> required <code>include_cdf</code> <code>bool</code> <p>Whether to include the calculated cumulative distribution function (CDF) in the output DistributionSummary.</p> <code>False</code> <p>Returns:</p> Type Description <code>DistributionSummary</code> <p>An instance of DistributionSummary with calculated values.</p> Source code in <code>src/guidellm/objects/statistics.py</code> <pre><code>@staticmethod\ndef from_distribution_function(\n    distribution: list[tuple[float, float]],\n    include_cdf: bool = False,\n) -&gt; \"DistributionSummary\":\n    \"\"\"\n    Create a statistical summary for a given distribution of weighted numerical\n    values or a probability distribution function (PDF).\n    1.  If the distribution is a PDF, it is expected to be a list of tuples\n        where each tuple contains (value, probability). The sum of the\n        probabilities should be 1. If it is not, it will be normalized.\n    2.  If the distribution is a values distribution function, it is expected\n        to be a list of tuples where each tuple contains (value, weight).\n        The weights are normalized to a probability distribution function.\n\n    :param distribution: A list of tuples representing the distribution.\n        Each tuple contains (value, weight) or (value, probability).\n    :param include_cdf: Whether to include the calculated cumulative distribution\n        function (CDF) in the output DistributionSummary.\n    :return: An instance of DistributionSummary with calculated values.\n    \"\"\"\n    values, weights = zip(*distribution) if distribution else ([], [])\n    values = np.array(values)  # type: ignore[assignment]\n    weights = np.array(weights)  # type: ignore[assignment]\n\n    # create the PDF\n    probabilities = weights / np.sum(weights)  # type: ignore[operator]\n    pdf = np.column_stack((values, probabilities))\n    pdf = pdf[np.argsort(pdf[:, 0])]\n    values = pdf[:, 0]  # type: ignore[assignment]\n    probabilities = pdf[:, 1]\n\n    # calculate the CDF\n    cumulative_probabilities = np.cumsum(probabilities)\n    cdf = np.column_stack((values, cumulative_probabilities))\n\n    # calculate statistics\n    mean = np.sum(values * probabilities).item()  # type: ignore[attr-defined]\n    median = cdf[np.argmax(cdf[:, 1] &gt;= 0.5), 0].item() if len(cdf) &gt; 0 else 0  # noqa: PLR2004\n    mode = values[np.argmax(probabilities)].item() if len(values) &gt; 0 else 0  # type: ignore[call-overload]\n    variance = np.sum((values - mean) ** 2 * probabilities).item()  # type: ignore[attr-defined]\n    std_dev = math.sqrt(variance)\n    minimum = values[0].item() if len(values) &gt; 0 else 0\n    maximum = values[-1].item() if len(values) &gt; 0 else 0\n    count = len(values)\n    total_sum = np.sum(values).item()  # type: ignore[attr-defined]\n\n    return DistributionSummary(\n        mean=mean,\n        median=median,\n        mode=mode,\n        variance=variance,\n        std_dev=std_dev,\n        min=minimum,\n        max=maximum,\n        count=count,\n        total_sum=total_sum,\n        percentiles=(\n            Percentiles(\n                p001=cdf[np.argmax(cdf[:, 1] &gt;= 0.001), 0].item(),  # noqa: PLR2004\n                p01=cdf[np.argmax(cdf[:, 1] &gt;= 0.01), 0].item(),  # noqa: PLR2004\n                p05=cdf[np.argmax(cdf[:, 1] &gt;= 0.05), 0].item(),  # noqa: PLR2004\n                p10=cdf[np.argmax(cdf[:, 1] &gt;= 0.1), 0].item(),  # noqa: PLR2004\n                p25=cdf[np.argmax(cdf[:, 1] &gt;= 0.25), 0].item(),  # noqa: PLR2004\n                p75=cdf[np.argmax(cdf[:, 1] &gt;= 0.75), 0].item(),  # noqa: PLR2004\n                p90=cdf[np.argmax(cdf[:, 1] &gt;= 0.9), 0].item(),  # noqa: PLR2004\n                p95=cdf[np.argmax(cdf[:, 1] &gt;= 0.95), 0].item(),  # noqa: PLR2004\n                p99=cdf[np.argmax(cdf[:, 1] &gt;= 0.99), 0].item(),  # noqa: PLR2004\n                p999=cdf[np.argmax(cdf[:, 1] &gt;= 0.999), 0].item(),  # noqa: PLR2004\n            )\n            if len(cdf) &gt; 0\n            else Percentiles(\n                p001=0,\n                p01=0,\n                p05=0,\n                p10=0,\n                p25=0,\n                p75=0,\n                p90=0,\n                p95=0,\n                p99=0,\n                p999=0,\n            )\n        ),\n        cumulative_distribution_function=cdf.tolist() if include_cdf else None,\n    )\n</code></pre>"},{"location":"reference/guidellm/objects/statistics/#guidellm.objects.statistics.DistributionSummary.from_iterable_request_times","title":"<code>from_iterable_request_times(requests, first_iter_times, iter_counts, first_iter_counts=None, include_cdf=False, epsilon=1e-06)</code>  <code>staticmethod</code>","text":"<p>Create a statistical summary for a given distribution of request times for a request with iterable responses between the start and end. For example, this is used to measure auto regressive requests where a request is started and at some later point, iterative responses are received. This will convert the request times and iterable values into a distribution function and then calculate the statistics with from_distribution_function.</p> <p>Parameters:</p> Name Type Description Default <code>requests</code> <code>list[tuple[float, float]]</code> <p>A list of tuples representing the start and end times of each request. Example: [(start_1, end_1), (start_2, end_2), ...]</p> required <code>first_iter_times</code> <code>list[float]</code> <p>A list of times when the first iteration of each request was received. Must be the same length as requests.</p> required <code>iter_counts</code> <code>list[int]</code> <p>A list of the total number of iterations for each request that occurred starting at the first iteration and ending at the request end time. Must be the same length as requests.</p> required <code>first_iter_counts</code> <code>Optional[list[int]]</code> <p>A list of the number of iterations to log for the first iteration of each request. For example, when calculating total number of tokens processed, this is set to the prompt tokens number. If not provided, defaults to 1 for each request.</p> <code>None</code> <code>include_cdf</code> <code>bool</code> <p>Whether to include the calculated cumulative distribution function (CDF) in the output DistributionSummary.</p> <code>False</code> <code>epsilon</code> <code>float</code> <p>The epsilon value for merging close events.</p> <code>1e-06</code> <p>Returns:</p> Type Description <code>DistributionSummary</code> <p>An instance of DistributionSummary with calculated values.</p> Source code in <code>src/guidellm/objects/statistics.py</code> <pre><code>@staticmethod\ndef from_iterable_request_times(\n    requests: list[tuple[float, float]],\n    first_iter_times: list[float],\n    iter_counts: list[int],\n    first_iter_counts: Optional[list[int]] = None,\n    include_cdf: bool = False,\n    epsilon: float = 1e-6,\n) -&gt; \"DistributionSummary\":\n    \"\"\"\n    Create a statistical summary for a given distribution of request times\n    for a request with iterable responses between the start and end.\n    For example, this is used to measure auto regressive requests where\n    a request is started and at some later point, iterative responses are\n    received. This will convert the request times and iterable values into\n    a distribution function and then calculate the statistics with\n    from_distribution_function.\n\n    :param requests: A list of tuples representing the start and end times of\n        each request. Example: [(start_1, end_1), (start_2, end_2), ...]\n    :param first_iter_times: A list of times when the first iteration of\n        each request was received. Must be the same length as requests.\n    :param iter_counts: A list of the total number of iterations for each\n        request that occurred starting at the first iteration and ending\n        at the request end time. Must be the same length as requests.\n    :param first_iter_counts: A list of the number of iterations to log\n        for the first iteration of each request. For example, when calculating\n        total number of tokens processed, this is set to the prompt tokens number.\n        If not provided, defaults to 1 for each request.\n    :param include_cdf: Whether to include the calculated cumulative distribution\n        function (CDF) in the output DistributionSummary.\n    :param epsilon: The epsilon value for merging close events.\n    :return: An instance of DistributionSummary with calculated values.\n    \"\"\"\n\n    if first_iter_counts is None:\n        first_iter_counts = [1] * len(requests)\n\n    if (\n        len(requests) != len(first_iter_times)\n        or len(requests) != len(iter_counts)\n        or len(requests) != len(first_iter_counts)\n    ):\n        raise ValueError(\n            \"requests, first_iter_times, iter_counts, and first_iter_counts must\"\n            \"be the same length.\"\n            f\"Given {len(requests)}, {len(first_iter_times)}, {len(iter_counts)}, \"\n            f\"{len(first_iter_counts)}\",\n        )\n\n    # first break up the requests into individual iterable events\n    events = defaultdict(int)\n    global_start = min(start for start, _ in requests) if requests else 0\n    global_end = max(end for _, end in requests) if requests else 0\n    events[global_start] = 0\n    events[global_end] = 0\n\n    for (_, end), first_iter, first_iter_count, total_count in zip(\n        requests, first_iter_times, first_iter_counts, iter_counts\n    ):\n        events[first_iter] += first_iter_count\n\n        if total_count &gt; 1:\n            iter_latency = (end - first_iter) / (total_count - 1)\n            for ind in range(1, total_count):\n                events[first_iter + ind * iter_latency] += 1\n\n    # combine any events that are very close together\n    flattened_events: list[tuple[float, int]] = []\n\n    for time, count in sorted(events.items()):\n        last_time, last_count = (\n            flattened_events[-1] if flattened_events else (None, None)\n        )\n\n        if (\n            last_time is not None\n            and last_count is not None\n            and abs(last_time - time) &lt;= epsilon\n        ):\n            flattened_events[-1] = (last_time, last_count + count)\n        else:\n            flattened_events.append((time, count))\n\n    # convert to value distribution function\n    distribution: dict[float, float] = defaultdict(float)\n\n    for ind in range(len(flattened_events) - 1):\n        start_time, count = flattened_events[ind]\n        end_time, _ = flattened_events[ind + 1]\n        duration = end_time - start_time\n        rate = count / duration\n        distribution[rate] += duration\n\n    distribution_list = sorted(distribution.items())\n\n    return DistributionSummary.from_distribution_function(\n        distribution=distribution_list,\n        include_cdf=include_cdf,\n    )\n</code></pre>"},{"location":"reference/guidellm/objects/statistics/#guidellm.objects.statistics.DistributionSummary.from_request_times","title":"<code>from_request_times(requests, distribution_type, include_cdf=False, epsilon=1e-06)</code>  <code>staticmethod</code>","text":"<p>Create a statistical summary for a given distribution of request times. Specifically, this is used to measure concurrency or rate of requests given an input list containing the start and end time of each request. This will first convert the request times into a distribution function and then calculate the statistics with from_distribution_function.</p> <p>Parameters:</p> Name Type Description Default <code>requests</code> <code>list[tuple[float, float]]</code> <p>A list of tuples representing the start and end times of each request. Example: [(start_1, end_1), (start_2, end_2), ...]</p> required <code>distribution_type</code> <code>Literal['concurrency', 'rate']</code> <p>The type of distribution to calculate. Either \"concurrency\" or \"rate\".</p> required <code>include_cdf</code> <code>bool</code> <p>Whether to include the calculated cumulative distribution function (CDF) in the output DistributionSummary.</p> <code>False</code> <code>epsilon</code> <code>float</code> <p>The epsilon value for merging close events.</p> <code>1e-06</code> <p>Returns:</p> Type Description <code>DistributionSummary</code> <p>An instance of DistributionSummary with calculated values.</p> Source code in <code>src/guidellm/objects/statistics.py</code> <pre><code>@staticmethod\ndef from_request_times(\n    requests: list[tuple[float, float]],\n    distribution_type: Literal[\"concurrency\", \"rate\"],\n    include_cdf: bool = False,\n    epsilon: float = 1e-6,\n) -&gt; \"DistributionSummary\":\n    \"\"\"\n    Create a statistical summary for a given distribution of request times.\n    Specifically, this is used to measure concurrency or rate of requests\n    given an input list containing the start and end time of each request.\n    This will first convert the request times into a distribution function\n    and then calculate the statistics with from_distribution_function.\n\n    :param requests: A list of tuples representing the start and end times of\n        each request. Example: [(start_1, end_1), (start_2, end_2), ...]\n    :param distribution_type: The type of distribution to calculate.\n        Either \"concurrency\" or \"rate\".\n    :param include_cdf: Whether to include the calculated cumulative distribution\n        function (CDF) in the output DistributionSummary.\n    :param epsilon: The epsilon value for merging close events.\n    :return: An instance of DistributionSummary with calculated values.\n    \"\"\"\n    if distribution_type == \"concurrency\":\n        # convert to delta changes based on when requests were running\n        time_deltas: dict[float, int] = defaultdict(int)\n        for start, end in requests:\n            time_deltas[start] += 1\n            time_deltas[end] -= 1\n\n        # convert to the events over time measuring concurrency changes\n        events = []\n        active = 0\n\n        for time, delta in sorted(time_deltas.items()):\n            active += delta\n            events.append((time, active))\n    elif distribution_type == \"rate\":\n        # convert to events for when requests finished\n        global_start = min(start for start, _ in requests) if requests else 0\n        events = [(global_start, 1)] + [(end, 1) for _, end in requests]\n    else:\n        raise ValueError(\n            f\"Invalid distribution_type '{distribution_type}'. \"\n            \"Must be 'concurrency' or 'rate'.\"\n        )\n\n    # combine any events that are very close together\n    flattened_events: list[tuple[float, float]] = []\n    for time, val in sorted(events):\n        last_time, last_val = (\n            flattened_events[-1] if flattened_events else (None, None)\n        )\n\n        if (\n            last_time is not None\n            and last_val is not None\n            and abs(last_time - time) &lt;= epsilon\n        ):\n            flattened_events[-1] = (last_time, last_val + val)\n        else:\n            flattened_events.append((time, val))\n\n    # convert to value distribution function\n    distribution: dict[float, float] = defaultdict(float)\n\n    for ind in range(len(flattened_events) - 1):\n        start_time, value = flattened_events[ind]\n        end_time, _ = flattened_events[ind + 1]\n        duration = end_time - start_time\n\n        if distribution_type == \"concurrency\":\n            # weight the concurrency value by the duration\n            distribution[value] += duration\n        elif distribution_type == \"rate\":\n            # weight the rate value by the duration\n            rate = value / duration\n            distribution[rate] += duration\n\n    distribution_list: list[tuple[float, float]] = sorted(distribution.items())\n\n    return DistributionSummary.from_distribution_function(\n        distribution=distribution_list,\n        include_cdf=include_cdf,\n    )\n</code></pre>"},{"location":"reference/guidellm/objects/statistics/#guidellm.objects.statistics.DistributionSummary.from_values","title":"<code>from_values(values, weights=None, include_cdf=False)</code>  <code>staticmethod</code>","text":"<p>Create a statistical summary for a given distribution of numerical values. This is a wrapper around from_distribution_function to handle the optional case of including weights for the values. If weights are not provided, they are automatically set to 1.0 for each value, so each value is equally weighted.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>list[float]</code> <p>A list of numerical values representing the distribution.</p> required <code>weights</code> <code>Optional[list[float]]</code> <p>A list of weights for each value in the distribution. If not provided, all values are equally weighted.</p> <code>None</code> <code>include_cdf</code> <code>bool</code> <p>Whether to include the calculated cumulative distribution function (CDF) in the output DistributionSummary.</p> <code>False</code> Source code in <code>src/guidellm/objects/statistics.py</code> <pre><code>@staticmethod\ndef from_values(\n    values: list[float],\n    weights: Optional[list[float]] = None,\n    include_cdf: bool = False,\n) -&gt; \"DistributionSummary\":\n    \"\"\"\n    Create a statistical summary for a given distribution of numerical values.\n    This is a wrapper around from_distribution_function to handle the optional case\n    of including weights for the values. If weights are not provided, they are\n    automatically set to 1.0 for each value, so each value is equally weighted.\n\n    :param values: A list of numerical values representing the distribution.\n    :param weights: A list of weights for each value in the distribution.\n        If not provided, all values are equally weighted.\n    :param include_cdf: Whether to include the calculated cumulative distribution\n        function (CDF) in the output DistributionSummary.\n    \"\"\"\n    if weights is None:\n        weights = [1.0] * len(values)\n\n    if len(values) != len(weights):\n        raise ValueError(\n            \"The length of values and weights must be the same.\",\n        )\n\n    return DistributionSummary.from_distribution_function(\n        distribution=list(zip(values, weights)),\n        include_cdf=include_cdf,\n    )\n</code></pre>"},{"location":"reference/guidellm/objects/statistics/#guidellm.objects.statistics.Percentiles","title":"<code>Percentiles</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>A pydantic model representing the standard percentiles of a distribution.</p> Source code in <code>src/guidellm/objects/statistics.py</code> <pre><code>class Percentiles(StandardBaseModel):\n    \"\"\"\n    A pydantic model representing the standard percentiles of a distribution.\n    \"\"\"\n\n    p001: float = Field(\n        description=\"The 0.1th percentile of the distribution.\",\n    )\n    p01: float = Field(\n        description=\"The 1st percentile of the distribution.\",\n    )\n    p05: float = Field(\n        description=\"The 5th percentile of the distribution.\",\n    )\n    p10: float = Field(\n        description=\"The 10th percentile of the distribution.\",\n    )\n    p25: float = Field(\n        description=\"The 25th percentile of the distribution.\",\n    )\n    p75: float = Field(\n        description=\"The 75th percentile of the distribution.\",\n    )\n    p90: float = Field(\n        description=\"The 90th percentile of the distribution.\",\n    )\n    p95: float = Field(\n        description=\"The 95th percentile of the distribution.\",\n    )\n    p99: float = Field(\n        description=\"The 99th percentile of the distribution.\",\n    )\n    p999: float = Field(\n        description=\"The 99.9th percentile of the distribution.\",\n    )\n</code></pre>"},{"location":"reference/guidellm/objects/statistics/#guidellm.objects.statistics.RunningStats","title":"<code>RunningStats</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>Create a running statistics object to track the mean, rate, and other statistics of a stream of values. 1.  The start time is set to the time the object is created. 2.  The count is set to 0. 3.  The total is set to 0. 4.  The last value is set to 0. 5.  The mean is calculated as the total / count.</p> Source code in <code>src/guidellm/objects/statistics.py</code> <pre><code>class RunningStats(StandardBaseModel):\n    \"\"\"\n    Create a running statistics object to track the mean, rate, and other\n    statistics of a stream of values.\n    1.  The start time is set to the time the object is created.\n    2.  The count is set to 0.\n    3.  The total is set to 0.\n    4.  The last value is set to 0.\n    5.  The mean is calculated as the total / count.\n    \"\"\"\n\n    start_time: float = Field(\n        default_factory=timer.time,\n        description=(\n            \"The time the running statistics object was created. \"\n            \"This is used to calculate the rate of the statistics.\"\n        ),\n    )\n    count: int = Field(\n        default=0,\n        description=\"The number of values added to the running statistics.\",\n    )\n    total: float = Field(\n        default=0.0,\n        description=\"The total sum of the values added to the running statistics.\",\n    )\n    last: float = Field(\n        default=0.0,\n        description=\"The last value added to the running statistics.\",\n    )\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def mean(self) -&gt; float:\n        \"\"\"\n        :return: The mean of the running statistics (total / count).\n            If count is 0, return 0.0.\n        \"\"\"\n        if self.count == 0:\n            return 0.0\n        return self.total / self.count\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def rate(self) -&gt; float:\n        \"\"\"\n        :return: The rate of the running statistics\n            (total / (time.time() - start_time)).\n            If count is 0, return 0.0.\n        \"\"\"\n        if self.count == 0:\n            return 0.0\n        return self.total / (timer.time() - self.start_time)\n\n    def __add__(self, value: Any) -&gt; float:\n        \"\"\"\n        Enable the use of the + operator to add a value to the running statistics.\n\n        :param value: The value to add to the running statistics.\n        :return: The mean of the running statistics.\n        \"\"\"\n        if not isinstance(value, (int, float)):\n            raise ValueError(\n                f\"Value must be an int or float, got {type(value)} instead.\",\n            )\n\n        self.update(value)\n\n        return self.mean\n\n    def __iadd__(self, value: Any) -&gt; \"RunningStats\":\n        \"\"\"\n        Enable the use of the += operator to add a value to the running statistics.\n\n        :param value: The value to add to the running statistics.\n        :return: The running statistics object.\n        \"\"\"\n        if not isinstance(value, (int, float)):\n            raise ValueError(\n                f\"Value must be an int or float, got {type(value)} instead.\",\n            )\n\n        self.update(value)\n\n        return self\n\n    def update(self, value: float, count: int = 1) -&gt; None:\n        \"\"\"\n        Update the running statistics with a new value.\n\n        :param value: The new value to add to the running statistics.\n        :param count: The number of times to 'count' for the value.\n            If not provided, defaults to 1.\n        \"\"\"\n        self.count += count\n        self.total += value\n        self.last = value\n</code></pre>"},{"location":"reference/guidellm/objects/statistics/#guidellm.objects.statistics.RunningStats.mean","title":"<code>mean</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>float</code> <p>The mean of the running statistics (total / count). If count is 0, return 0.0.</p>"},{"location":"reference/guidellm/objects/statistics/#guidellm.objects.statistics.RunningStats.rate","title":"<code>rate</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>float</code> <p>The rate of the running statistics (total / (time.time() - start_time)). If count is 0, return 0.0.</p>"},{"location":"reference/guidellm/objects/statistics/#guidellm.objects.statistics.RunningStats.__add__","title":"<code>__add__(value)</code>","text":"<p>Enable the use of the + operator to add a value to the running statistics.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The value to add to the running statistics.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The mean of the running statistics.</p> Source code in <code>src/guidellm/objects/statistics.py</code> <pre><code>def __add__(self, value: Any) -&gt; float:\n    \"\"\"\n    Enable the use of the + operator to add a value to the running statistics.\n\n    :param value: The value to add to the running statistics.\n    :return: The mean of the running statistics.\n    \"\"\"\n    if not isinstance(value, (int, float)):\n        raise ValueError(\n            f\"Value must be an int or float, got {type(value)} instead.\",\n        )\n\n    self.update(value)\n\n    return self.mean\n</code></pre>"},{"location":"reference/guidellm/objects/statistics/#guidellm.objects.statistics.RunningStats.__iadd__","title":"<code>__iadd__(value)</code>","text":"<p>Enable the use of the += operator to add a value to the running statistics.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The value to add to the running statistics.</p> required <p>Returns:</p> Type Description <code>RunningStats</code> <p>The running statistics object.</p> Source code in <code>src/guidellm/objects/statistics.py</code> <pre><code>def __iadd__(self, value: Any) -&gt; \"RunningStats\":\n    \"\"\"\n    Enable the use of the += operator to add a value to the running statistics.\n\n    :param value: The value to add to the running statistics.\n    :return: The running statistics object.\n    \"\"\"\n    if not isinstance(value, (int, float)):\n        raise ValueError(\n            f\"Value must be an int or float, got {type(value)} instead.\",\n        )\n\n    self.update(value)\n\n    return self\n</code></pre>"},{"location":"reference/guidellm/objects/statistics/#guidellm.objects.statistics.RunningStats.update","title":"<code>update(value, count=1)</code>","text":"<p>Update the running statistics with a new value.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The new value to add to the running statistics.</p> required <code>count</code> <code>int</code> <p>The number of times to 'count' for the value. If not provided, defaults to 1.</p> <code>1</code> Source code in <code>src/guidellm/objects/statistics.py</code> <pre><code>def update(self, value: float, count: int = 1) -&gt; None:\n    \"\"\"\n    Update the running statistics with a new value.\n\n    :param value: The new value to add to the running statistics.\n    :param count: The number of times to 'count' for the value.\n        If not provided, defaults to 1.\n    \"\"\"\n    self.count += count\n    self.total += value\n    self.last = value\n</code></pre>"},{"location":"reference/guidellm/objects/statistics/#guidellm.objects.statistics.StatusDistributionSummary","title":"<code>StatusDistributionSummary</code>","text":"<p>               Bases: <code>StatusBreakdown[DistributionSummary, DistributionSummary, DistributionSummary, DistributionSummary]</code></p> <p>A pydantic model representing a statistical summary for a given distribution of numerical values grouped by status. Specifically used to represent the total, successful, incomplete, and errored values for a benchmark or other statistical summary.</p> Source code in <code>src/guidellm/objects/statistics.py</code> <pre><code>class StatusDistributionSummary(\n    StatusBreakdown[\n        DistributionSummary,\n        DistributionSummary,\n        DistributionSummary,\n        DistributionSummary,\n    ]\n):\n    \"\"\"\n    A pydantic model representing a statistical summary for a given\n    distribution of numerical values grouped by status.\n    Specifically used to represent the total, successful, incomplete,\n    and errored values for a benchmark or other statistical summary.\n    \"\"\"\n\n    @staticmethod\n    def from_values(\n        value_types: list[Literal[\"successful\", \"incomplete\", \"error\"]],\n        values: list[float],\n        weights: Optional[list[float]] = None,\n        include_cdf: bool = False,\n    ) -&gt; \"StatusDistributionSummary\":\n        \"\"\"\n        Create a statistical summary by status for a given distribution of numerical\n        values. This is used to measure the distribution of values for different\n        statuses (e.g., successful, incomplete, error) and calculate the statistics\n        for each status. Weights are optional to weight the probability distribution\n        for each value by. If not provided, all values are equally weighted.\n\n        :param value_types: A list of status types for each value in the distribution.\n            Must be one of 'successful', 'incomplete', or 'error'.\n        :param values: A list of numerical values representing the distribution.\n            Must be the same length as value_types.\n        :param weights: A list of weights for each value in the distribution.\n            If not provided, all values are equally weighted (set to 1).\n            Must be the same length as value_types.\n        :param include_cdf: Whether to include the calculated cumulative distribution\n            function (CDF) in the output StatusDistributionSummary.\n        :return: An instance of StatusDistributionSummary with calculated values.\n        \"\"\"\n        if any(\n            type_ not in {\"successful\", \"incomplete\", \"error\"} for type_ in value_types\n        ):\n            raise ValueError(\n                \"value_types must be one of 'successful', 'incomplete', or 'error'. \"\n                f\"Got {value_types} instead.\",\n            )\n\n        if weights is None:\n            weights = [1.0] * len(values)\n\n        if len(value_types) != len(values) or len(value_types) != len(weights):\n            raise ValueError(\n                \"The length of value_types, values, and weights must be the same.\",\n            )\n\n        _, successful_values, successful_weights = (\n            zip(*successful)\n            if (\n                successful := list(\n                    filter(\n                        lambda val: val[0] == \"successful\",\n                        zip(value_types, values, weights),\n                    )\n                )\n            )\n            else ([], [], [])\n        )\n        _, incomplete_values, incomplete_weights = (\n            zip(*incomplete)\n            if (\n                incomplete := list(\n                    filter(\n                        lambda val: val[0] == \"incomplete\",\n                        zip(value_types, values, weights),\n                    )\n                )\n            )\n            else ([], [], [])\n        )\n        _, errored_values, errored_weights = (\n            zip(*errored)\n            if (\n                errored := list(\n                    filter(\n                        lambda val: val[0] == \"error\",\n                        zip(value_types, values, weights),\n                    )\n                )\n            )\n            else ([], [], [])\n        )\n\n        return StatusDistributionSummary(\n            total=DistributionSummary.from_values(\n                values,\n                weights,\n                include_cdf=include_cdf,\n            ),\n            successful=DistributionSummary.from_values(\n                successful_values,  # type: ignore[arg-type]\n                successful_weights,  # type: ignore[arg-type]\n                include_cdf=include_cdf,\n            ),\n            incomplete=DistributionSummary.from_values(\n                incomplete_values,  # type: ignore[arg-type]\n                incomplete_weights,  # type: ignore[arg-type]\n                include_cdf=include_cdf,\n            ),\n            errored=DistributionSummary.from_values(\n                errored_values,  # type: ignore[arg-type]\n                errored_weights,  # type: ignore[arg-type]\n                include_cdf=include_cdf,\n            ),\n        )\n\n    @staticmethod\n    def from_request_times(\n        request_types: list[Literal[\"successful\", \"incomplete\", \"error\"]],\n        requests: list[tuple[float, float]],\n        distribution_type: Literal[\"concurrency\", \"rate\"],\n        include_cdf: bool = False,\n        epsilon: float = 1e-6,\n    ) -&gt; \"StatusDistributionSummary\":\n        \"\"\"\n        Create a statistical summary by status for given distribution of request times.\n        This is used to measure the distribution of request times for different statuses\n        (e.g., successful, incomplete, error) for concurrency and rates.\n        This will call into DistributionSummary.from_request_times to calculate\n        the statistics for each status.\n\n        :param request_types: List of status types for each request in the distribution.\n            Must be one of 'successful', 'incomplete', or 'error'.\n        :param requests: A list of tuples representing the start and end times of\n            each request. Example: [(start_1, end_1), (start_2, end_2), ...].\n            Must be the same length as request_types.\n        :param distribution_type: The type of distribution to calculate.\n            Either \"concurrency\" or \"rate\".\n        :param include_cdf: Whether to include the calculated cumulative distribution\n            function (CDF) in the output StatusDistributionSummary.\n        :param epsilon: The epsilon value for merging close events.\n        :return: An instance of StatusDistributionSummary with calculated values.\n        \"\"\"\n        if distribution_type not in {\"concurrency\", \"rate\"}:\n            raise ValueError(\n                f\"Invalid distribution_type '{distribution_type}'. \"\n                \"Must be 'concurrency' or 'rate'.\"\n            )\n\n        if any(\n            type_ not in {\"successful\", \"incomplete\", \"error\"}\n            for type_ in request_types\n        ):\n            raise ValueError(\n                \"request_types must be one of 'successful', 'incomplete', or 'error'. \"\n                f\"Got {request_types} instead.\",\n            )\n\n        if len(request_types) != len(requests):\n            raise ValueError(\n                \"The length of request_types and requests must be the same. \"\n                f\"Got {len(request_types)} and {len(requests)} instead.\",\n            )\n\n        _, successful_requests = (\n            zip(*successful)\n            if (\n                successful := list(\n                    filter(\n                        lambda val: val[0] == \"successful\",\n                        zip(request_types, requests),\n                    )\n                )\n            )\n            else ([], [])\n        )\n        _, incomplete_requests = (\n            zip(*incomplete)\n            if (\n                incomplete := list(\n                    filter(\n                        lambda val: val[0] == \"incomplete\",\n                        zip(request_types, requests),\n                    )\n                )\n            )\n            else ([], [])\n        )\n        _, errored_requests = (\n            zip(*errored)\n            if (\n                errored := list(\n                    filter(\n                        lambda val: val[0] == \"error\",\n                        zip(request_types, requests),\n                    )\n                )\n            )\n            else ([], [])\n        )\n\n        return StatusDistributionSummary(\n            total=DistributionSummary.from_request_times(\n                requests,\n                distribution_type=distribution_type,\n                include_cdf=include_cdf,\n                epsilon=epsilon,\n            ),\n            successful=DistributionSummary.from_request_times(\n                successful_requests,  # type: ignore[arg-type]\n                distribution_type=distribution_type,\n                include_cdf=include_cdf,\n                epsilon=epsilon,\n            ),\n            incomplete=DistributionSummary.from_request_times(\n                incomplete_requests,  # type: ignore[arg-type]\n                distribution_type=distribution_type,\n                include_cdf=include_cdf,\n                epsilon=epsilon,\n            ),\n            errored=DistributionSummary.from_request_times(\n                errored_requests,  # type: ignore[arg-type]\n                distribution_type=distribution_type,\n                include_cdf=include_cdf,\n                epsilon=epsilon,\n            ),\n        )\n\n    @staticmethod\n    def from_iterable_request_times(\n        request_types: list[Literal[\"successful\", \"incomplete\", \"error\"]],\n        requests: list[tuple[float, float]],\n        first_iter_times: list[float],\n        iter_counts: Optional[list[int]] = None,\n        first_iter_counts: Optional[list[int]] = None,\n        include_cdf: bool = False,\n        epsilon: float = 1e-6,\n    ) -&gt; \"StatusDistributionSummary\":\n        \"\"\"\n        Create a statistical summary by status for given distribution of request times\n        for a request with iterable responses between the start and end.\n        For example, this is used to measure auto regressive requests where\n        a request is started and at some later point, iterative responses are\n        received. This will call into DistributionSummary.from_iterable_request_times\n        to calculate the statistics for each status.\n\n        :param request_types: List of status types for each request in the distribution.\n            Must be one of 'successful', 'incomplete', or 'error'.\n        :param requests: A list of tuples representing the start and end times of\n            each request. Example: [(start_1, end_1), (start_2, end_2), ...].\n            Must be the same length as request_types.\n        :param first_iter_times: A list of times when the first iteration of\n            each request was received. Must be the same length as requests.\n        :param iter_counts: A list of the total number of iterations for each\n            request that occurred starting at the first iteration and ending\n            at the request end time. Must be the same length as requests.\n            If not provided, defaults to 1 for each request.\n        :param first_iter_counts: A list of the number of iterations to log\n            for the first iteration of each request. For example, when calculating\n            total number of tokens processed, this is set to the prompt tokens number.\n            If not provided, defaults to 1 for each request.\n        :param include_cdf: Whether to include the calculated cumulative distribution\n            function (CDF) in the output StatusDistributionSummary.\n        :param epsilon: The epsilon value for merging close events.\n        :return: An instance of StatusDistributionSummary with calculated values.\n        \"\"\"\n        if any(\n            type_ not in {\"successful\", \"incomplete\", \"error\"}\n            for type_ in request_types\n        ):\n            raise ValueError(\n                \"request_types must be one of 'successful', 'incomplete', or 'error'. \"\n                f\"Got {request_types} instead.\",\n            )\n\n        if iter_counts is None:\n            iter_counts = [1] * len(requests)\n\n        if first_iter_counts is None:\n            first_iter_counts = [1] * len(requests)\n\n        if (\n            len(request_types) != len(requests)\n            or len(requests) != len(first_iter_times)\n            or len(requests) != len(iter_counts)\n            or len(requests) != len(first_iter_counts)\n        ):\n            raise ValueError(\n                \"request_types, requests, first_iter_times, iter_counts, and \"\n                \"first_iter_counts must be the same length.\"\n                f\"Given {len(request_types)}, {len(requests)}, \"\n                f\"{len(first_iter_times)}, {len(iter_counts)}, \"\n                f\"{len(first_iter_counts)}\",\n            )\n\n        (\n            _,\n            successful_requests,\n            successful_first_iter_times,\n            successful_iter_counts,\n            successful_first_iter_counts,\n        ) = (\n            zip(*successful)\n            if (\n                successful := list(\n                    filter(\n                        lambda val: val[0] == \"successful\",\n                        zip(\n                            request_types,\n                            requests,\n                            first_iter_times,\n                            iter_counts,\n                            first_iter_counts,\n                        ),\n                    )\n                )\n            )\n            else ([], [], [], [], [])\n        )\n        (\n            _,\n            incomplete_requests,\n            incomplete_first_iter_times,\n            incomplete_iter_counts,\n            incomplete_first_iter_counts,\n        ) = (\n            zip(*incomplete)\n            if (\n                incomplete := list(\n                    filter(\n                        lambda val: val[0] == \"incomplete\",\n                        zip(\n                            request_types,\n                            requests,\n                            first_iter_times,\n                            iter_counts,\n                            first_iter_counts,\n                        ),\n                    )\n                )\n            )\n            else ([], [], [], [], [])\n        )\n        (\n            _,\n            errored_requests,\n            errored_first_iter_times,\n            errored_iter_counts,\n            errored_first_iter_counts,\n        ) = (\n            zip(*errored)\n            if (\n                errored := list(\n                    filter(\n                        lambda val: val[0] == \"error\",\n                        zip(\n                            request_types,\n                            requests,\n                            first_iter_times,\n                            iter_counts,\n                            first_iter_counts,\n                        ),\n                    )\n                )\n            )\n            else ([], [], [], [], [])\n        )\n\n        return StatusDistributionSummary(\n            total=DistributionSummary.from_iterable_request_times(\n                requests,\n                first_iter_times,\n                iter_counts,\n                first_iter_counts,\n                include_cdf=include_cdf,\n                epsilon=epsilon,\n            ),\n            successful=DistributionSummary.from_iterable_request_times(\n                successful_requests,  # type: ignore[arg-type]\n                successful_first_iter_times,  # type: ignore[arg-type]\n                successful_iter_counts,  # type: ignore[arg-type]\n                successful_first_iter_counts,  # type: ignore[arg-type]\n                include_cdf=include_cdf,\n                epsilon=epsilon,\n            ),\n            incomplete=DistributionSummary.from_iterable_request_times(\n                incomplete_requests,  # type: ignore[arg-type]\n                incomplete_first_iter_times,  # type: ignore[arg-type]\n                incomplete_iter_counts,  # type: ignore[arg-type]\n                incomplete_first_iter_counts,  # type: ignore[arg-type]\n                include_cdf=include_cdf,\n                epsilon=epsilon,\n            ),\n            errored=DistributionSummary.from_iterable_request_times(\n                errored_requests,  # type: ignore[arg-type]\n                errored_first_iter_times,  # type: ignore[arg-type]\n                errored_iter_counts,  # type: ignore[arg-type]\n                errored_first_iter_counts,  # type: ignore[arg-type]\n                include_cdf=include_cdf,\n                epsilon=epsilon,\n            ),\n        )\n</code></pre>"},{"location":"reference/guidellm/objects/statistics/#guidellm.objects.statistics.StatusDistributionSummary.from_iterable_request_times","title":"<code>from_iterable_request_times(request_types, requests, first_iter_times, iter_counts=None, first_iter_counts=None, include_cdf=False, epsilon=1e-06)</code>  <code>staticmethod</code>","text":"<p>Create a statistical summary by status for given distribution of request times for a request with iterable responses between the start and end. For example, this is used to measure auto regressive requests where a request is started and at some later point, iterative responses are received. This will call into DistributionSummary.from_iterable_request_times to calculate the statistics for each status.</p> <p>Parameters:</p> Name Type Description Default <code>request_types</code> <code>list[Literal['successful', 'incomplete', 'error']]</code> <p>List of status types for each request in the distribution. Must be one of 'successful', 'incomplete', or 'error'.</p> required <code>requests</code> <code>list[tuple[float, float]]</code> <p>A list of tuples representing the start and end times of each request. Example: [(start_1, end_1), (start_2, end_2), ...]. Must be the same length as request_types.</p> required <code>first_iter_times</code> <code>list[float]</code> <p>A list of times when the first iteration of each request was received. Must be the same length as requests.</p> required <code>iter_counts</code> <code>Optional[list[int]]</code> <p>A list of the total number of iterations for each request that occurred starting at the first iteration and ending at the request end time. Must be the same length as requests. If not provided, defaults to 1 for each request.</p> <code>None</code> <code>first_iter_counts</code> <code>Optional[list[int]]</code> <p>A list of the number of iterations to log for the first iteration of each request. For example, when calculating total number of tokens processed, this is set to the prompt tokens number. If not provided, defaults to 1 for each request.</p> <code>None</code> <code>include_cdf</code> <code>bool</code> <p>Whether to include the calculated cumulative distribution function (CDF) in the output StatusDistributionSummary.</p> <code>False</code> <code>epsilon</code> <code>float</code> <p>The epsilon value for merging close events.</p> <code>1e-06</code> <p>Returns:</p> Type Description <code>StatusDistributionSummary</code> <p>An instance of StatusDistributionSummary with calculated values.</p> Source code in <code>src/guidellm/objects/statistics.py</code> <pre><code>@staticmethod\ndef from_iterable_request_times(\n    request_types: list[Literal[\"successful\", \"incomplete\", \"error\"]],\n    requests: list[tuple[float, float]],\n    first_iter_times: list[float],\n    iter_counts: Optional[list[int]] = None,\n    first_iter_counts: Optional[list[int]] = None,\n    include_cdf: bool = False,\n    epsilon: float = 1e-6,\n) -&gt; \"StatusDistributionSummary\":\n    \"\"\"\n    Create a statistical summary by status for given distribution of request times\n    for a request with iterable responses between the start and end.\n    For example, this is used to measure auto regressive requests where\n    a request is started and at some later point, iterative responses are\n    received. This will call into DistributionSummary.from_iterable_request_times\n    to calculate the statistics for each status.\n\n    :param request_types: List of status types for each request in the distribution.\n        Must be one of 'successful', 'incomplete', or 'error'.\n    :param requests: A list of tuples representing the start and end times of\n        each request. Example: [(start_1, end_1), (start_2, end_2), ...].\n        Must be the same length as request_types.\n    :param first_iter_times: A list of times when the first iteration of\n        each request was received. Must be the same length as requests.\n    :param iter_counts: A list of the total number of iterations for each\n        request that occurred starting at the first iteration and ending\n        at the request end time. Must be the same length as requests.\n        If not provided, defaults to 1 for each request.\n    :param first_iter_counts: A list of the number of iterations to log\n        for the first iteration of each request. For example, when calculating\n        total number of tokens processed, this is set to the prompt tokens number.\n        If not provided, defaults to 1 for each request.\n    :param include_cdf: Whether to include the calculated cumulative distribution\n        function (CDF) in the output StatusDistributionSummary.\n    :param epsilon: The epsilon value for merging close events.\n    :return: An instance of StatusDistributionSummary with calculated values.\n    \"\"\"\n    if any(\n        type_ not in {\"successful\", \"incomplete\", \"error\"}\n        for type_ in request_types\n    ):\n        raise ValueError(\n            \"request_types must be one of 'successful', 'incomplete', or 'error'. \"\n            f\"Got {request_types} instead.\",\n        )\n\n    if iter_counts is None:\n        iter_counts = [1] * len(requests)\n\n    if first_iter_counts is None:\n        first_iter_counts = [1] * len(requests)\n\n    if (\n        len(request_types) != len(requests)\n        or len(requests) != len(first_iter_times)\n        or len(requests) != len(iter_counts)\n        or len(requests) != len(first_iter_counts)\n    ):\n        raise ValueError(\n            \"request_types, requests, first_iter_times, iter_counts, and \"\n            \"first_iter_counts must be the same length.\"\n            f\"Given {len(request_types)}, {len(requests)}, \"\n            f\"{len(first_iter_times)}, {len(iter_counts)}, \"\n            f\"{len(first_iter_counts)}\",\n        )\n\n    (\n        _,\n        successful_requests,\n        successful_first_iter_times,\n        successful_iter_counts,\n        successful_first_iter_counts,\n    ) = (\n        zip(*successful)\n        if (\n            successful := list(\n                filter(\n                    lambda val: val[0] == \"successful\",\n                    zip(\n                        request_types,\n                        requests,\n                        first_iter_times,\n                        iter_counts,\n                        first_iter_counts,\n                    ),\n                )\n            )\n        )\n        else ([], [], [], [], [])\n    )\n    (\n        _,\n        incomplete_requests,\n        incomplete_first_iter_times,\n        incomplete_iter_counts,\n        incomplete_first_iter_counts,\n    ) = (\n        zip(*incomplete)\n        if (\n            incomplete := list(\n                filter(\n                    lambda val: val[0] == \"incomplete\",\n                    zip(\n                        request_types,\n                        requests,\n                        first_iter_times,\n                        iter_counts,\n                        first_iter_counts,\n                    ),\n                )\n            )\n        )\n        else ([], [], [], [], [])\n    )\n    (\n        _,\n        errored_requests,\n        errored_first_iter_times,\n        errored_iter_counts,\n        errored_first_iter_counts,\n    ) = (\n        zip(*errored)\n        if (\n            errored := list(\n                filter(\n                    lambda val: val[0] == \"error\",\n                    zip(\n                        request_types,\n                        requests,\n                        first_iter_times,\n                        iter_counts,\n                        first_iter_counts,\n                    ),\n                )\n            )\n        )\n        else ([], [], [], [], [])\n    )\n\n    return StatusDistributionSummary(\n        total=DistributionSummary.from_iterable_request_times(\n            requests,\n            first_iter_times,\n            iter_counts,\n            first_iter_counts,\n            include_cdf=include_cdf,\n            epsilon=epsilon,\n        ),\n        successful=DistributionSummary.from_iterable_request_times(\n            successful_requests,  # type: ignore[arg-type]\n            successful_first_iter_times,  # type: ignore[arg-type]\n            successful_iter_counts,  # type: ignore[arg-type]\n            successful_first_iter_counts,  # type: ignore[arg-type]\n            include_cdf=include_cdf,\n            epsilon=epsilon,\n        ),\n        incomplete=DistributionSummary.from_iterable_request_times(\n            incomplete_requests,  # type: ignore[arg-type]\n            incomplete_first_iter_times,  # type: ignore[arg-type]\n            incomplete_iter_counts,  # type: ignore[arg-type]\n            incomplete_first_iter_counts,  # type: ignore[arg-type]\n            include_cdf=include_cdf,\n            epsilon=epsilon,\n        ),\n        errored=DistributionSummary.from_iterable_request_times(\n            errored_requests,  # type: ignore[arg-type]\n            errored_first_iter_times,  # type: ignore[arg-type]\n            errored_iter_counts,  # type: ignore[arg-type]\n            errored_first_iter_counts,  # type: ignore[arg-type]\n            include_cdf=include_cdf,\n            epsilon=epsilon,\n        ),\n    )\n</code></pre>"},{"location":"reference/guidellm/objects/statistics/#guidellm.objects.statistics.StatusDistributionSummary.from_request_times","title":"<code>from_request_times(request_types, requests, distribution_type, include_cdf=False, epsilon=1e-06)</code>  <code>staticmethod</code>","text":"<p>Create a statistical summary by status for given distribution of request times. This is used to measure the distribution of request times for different statuses (e.g., successful, incomplete, error) for concurrency and rates. This will call into DistributionSummary.from_request_times to calculate the statistics for each status.</p> <p>Parameters:</p> Name Type Description Default <code>request_types</code> <code>list[Literal['successful', 'incomplete', 'error']]</code> <p>List of status types for each request in the distribution. Must be one of 'successful', 'incomplete', or 'error'.</p> required <code>requests</code> <code>list[tuple[float, float]]</code> <p>A list of tuples representing the start and end times of each request. Example: [(start_1, end_1), (start_2, end_2), ...]. Must be the same length as request_types.</p> required <code>distribution_type</code> <code>Literal['concurrency', 'rate']</code> <p>The type of distribution to calculate. Either \"concurrency\" or \"rate\".</p> required <code>include_cdf</code> <code>bool</code> <p>Whether to include the calculated cumulative distribution function (CDF) in the output StatusDistributionSummary.</p> <code>False</code> <code>epsilon</code> <code>float</code> <p>The epsilon value for merging close events.</p> <code>1e-06</code> <p>Returns:</p> Type Description <code>StatusDistributionSummary</code> <p>An instance of StatusDistributionSummary with calculated values.</p> Source code in <code>src/guidellm/objects/statistics.py</code> <pre><code>@staticmethod\ndef from_request_times(\n    request_types: list[Literal[\"successful\", \"incomplete\", \"error\"]],\n    requests: list[tuple[float, float]],\n    distribution_type: Literal[\"concurrency\", \"rate\"],\n    include_cdf: bool = False,\n    epsilon: float = 1e-6,\n) -&gt; \"StatusDistributionSummary\":\n    \"\"\"\n    Create a statistical summary by status for given distribution of request times.\n    This is used to measure the distribution of request times for different statuses\n    (e.g., successful, incomplete, error) for concurrency and rates.\n    This will call into DistributionSummary.from_request_times to calculate\n    the statistics for each status.\n\n    :param request_types: List of status types for each request in the distribution.\n        Must be one of 'successful', 'incomplete', or 'error'.\n    :param requests: A list of tuples representing the start and end times of\n        each request. Example: [(start_1, end_1), (start_2, end_2), ...].\n        Must be the same length as request_types.\n    :param distribution_type: The type of distribution to calculate.\n        Either \"concurrency\" or \"rate\".\n    :param include_cdf: Whether to include the calculated cumulative distribution\n        function (CDF) in the output StatusDistributionSummary.\n    :param epsilon: The epsilon value for merging close events.\n    :return: An instance of StatusDistributionSummary with calculated values.\n    \"\"\"\n    if distribution_type not in {\"concurrency\", \"rate\"}:\n        raise ValueError(\n            f\"Invalid distribution_type '{distribution_type}'. \"\n            \"Must be 'concurrency' or 'rate'.\"\n        )\n\n    if any(\n        type_ not in {\"successful\", \"incomplete\", \"error\"}\n        for type_ in request_types\n    ):\n        raise ValueError(\n            \"request_types must be one of 'successful', 'incomplete', or 'error'. \"\n            f\"Got {request_types} instead.\",\n        )\n\n    if len(request_types) != len(requests):\n        raise ValueError(\n            \"The length of request_types and requests must be the same. \"\n            f\"Got {len(request_types)} and {len(requests)} instead.\",\n        )\n\n    _, successful_requests = (\n        zip(*successful)\n        if (\n            successful := list(\n                filter(\n                    lambda val: val[0] == \"successful\",\n                    zip(request_types, requests),\n                )\n            )\n        )\n        else ([], [])\n    )\n    _, incomplete_requests = (\n        zip(*incomplete)\n        if (\n            incomplete := list(\n                filter(\n                    lambda val: val[0] == \"incomplete\",\n                    zip(request_types, requests),\n                )\n            )\n        )\n        else ([], [])\n    )\n    _, errored_requests = (\n        zip(*errored)\n        if (\n            errored := list(\n                filter(\n                    lambda val: val[0] == \"error\",\n                    zip(request_types, requests),\n                )\n            )\n        )\n        else ([], [])\n    )\n\n    return StatusDistributionSummary(\n        total=DistributionSummary.from_request_times(\n            requests,\n            distribution_type=distribution_type,\n            include_cdf=include_cdf,\n            epsilon=epsilon,\n        ),\n        successful=DistributionSummary.from_request_times(\n            successful_requests,  # type: ignore[arg-type]\n            distribution_type=distribution_type,\n            include_cdf=include_cdf,\n            epsilon=epsilon,\n        ),\n        incomplete=DistributionSummary.from_request_times(\n            incomplete_requests,  # type: ignore[arg-type]\n            distribution_type=distribution_type,\n            include_cdf=include_cdf,\n            epsilon=epsilon,\n        ),\n        errored=DistributionSummary.from_request_times(\n            errored_requests,  # type: ignore[arg-type]\n            distribution_type=distribution_type,\n            include_cdf=include_cdf,\n            epsilon=epsilon,\n        ),\n    )\n</code></pre>"},{"location":"reference/guidellm/objects/statistics/#guidellm.objects.statistics.StatusDistributionSummary.from_values","title":"<code>from_values(value_types, values, weights=None, include_cdf=False)</code>  <code>staticmethod</code>","text":"<p>Create a statistical summary by status for a given distribution of numerical values. This is used to measure the distribution of values for different statuses (e.g., successful, incomplete, error) and calculate the statistics for each status. Weights are optional to weight the probability distribution for each value by. If not provided, all values are equally weighted.</p> <p>Parameters:</p> Name Type Description Default <code>value_types</code> <code>list[Literal['successful', 'incomplete', 'error']]</code> <p>A list of status types for each value in the distribution. Must be one of 'successful', 'incomplete', or 'error'.</p> required <code>values</code> <code>list[float]</code> <p>A list of numerical values representing the distribution. Must be the same length as value_types.</p> required <code>weights</code> <code>Optional[list[float]]</code> <p>A list of weights for each value in the distribution. If not provided, all values are equally weighted (set to 1). Must be the same length as value_types.</p> <code>None</code> <code>include_cdf</code> <code>bool</code> <p>Whether to include the calculated cumulative distribution function (CDF) in the output StatusDistributionSummary.</p> <code>False</code> <p>Returns:</p> Type Description <code>StatusDistributionSummary</code> <p>An instance of StatusDistributionSummary with calculated values.</p> Source code in <code>src/guidellm/objects/statistics.py</code> <pre><code>@staticmethod\ndef from_values(\n    value_types: list[Literal[\"successful\", \"incomplete\", \"error\"]],\n    values: list[float],\n    weights: Optional[list[float]] = None,\n    include_cdf: bool = False,\n) -&gt; \"StatusDistributionSummary\":\n    \"\"\"\n    Create a statistical summary by status for a given distribution of numerical\n    values. This is used to measure the distribution of values for different\n    statuses (e.g., successful, incomplete, error) and calculate the statistics\n    for each status. Weights are optional to weight the probability distribution\n    for each value by. If not provided, all values are equally weighted.\n\n    :param value_types: A list of status types for each value in the distribution.\n        Must be one of 'successful', 'incomplete', or 'error'.\n    :param values: A list of numerical values representing the distribution.\n        Must be the same length as value_types.\n    :param weights: A list of weights for each value in the distribution.\n        If not provided, all values are equally weighted (set to 1).\n        Must be the same length as value_types.\n    :param include_cdf: Whether to include the calculated cumulative distribution\n        function (CDF) in the output StatusDistributionSummary.\n    :return: An instance of StatusDistributionSummary with calculated values.\n    \"\"\"\n    if any(\n        type_ not in {\"successful\", \"incomplete\", \"error\"} for type_ in value_types\n    ):\n        raise ValueError(\n            \"value_types must be one of 'successful', 'incomplete', or 'error'. \"\n            f\"Got {value_types} instead.\",\n        )\n\n    if weights is None:\n        weights = [1.0] * len(values)\n\n    if len(value_types) != len(values) or len(value_types) != len(weights):\n        raise ValueError(\n            \"The length of value_types, values, and weights must be the same.\",\n        )\n\n    _, successful_values, successful_weights = (\n        zip(*successful)\n        if (\n            successful := list(\n                filter(\n                    lambda val: val[0] == \"successful\",\n                    zip(value_types, values, weights),\n                )\n            )\n        )\n        else ([], [], [])\n    )\n    _, incomplete_values, incomplete_weights = (\n        zip(*incomplete)\n        if (\n            incomplete := list(\n                filter(\n                    lambda val: val[0] == \"incomplete\",\n                    zip(value_types, values, weights),\n                )\n            )\n        )\n        else ([], [], [])\n    )\n    _, errored_values, errored_weights = (\n        zip(*errored)\n        if (\n            errored := list(\n                filter(\n                    lambda val: val[0] == \"error\",\n                    zip(value_types, values, weights),\n                )\n            )\n        )\n        else ([], [], [])\n    )\n\n    return StatusDistributionSummary(\n        total=DistributionSummary.from_values(\n            values,\n            weights,\n            include_cdf=include_cdf,\n        ),\n        successful=DistributionSummary.from_values(\n            successful_values,  # type: ignore[arg-type]\n            successful_weights,  # type: ignore[arg-type]\n            include_cdf=include_cdf,\n        ),\n        incomplete=DistributionSummary.from_values(\n            incomplete_values,  # type: ignore[arg-type]\n            incomplete_weights,  # type: ignore[arg-type]\n            include_cdf=include_cdf,\n        ),\n        errored=DistributionSummary.from_values(\n            errored_values,  # type: ignore[arg-type]\n            errored_weights,  # type: ignore[arg-type]\n            include_cdf=include_cdf,\n        ),\n    )\n</code></pre>"},{"location":"reference/guidellm/objects/statistics/#guidellm.objects.statistics.TimeRunningStats","title":"<code>TimeRunningStats</code>","text":"<p>               Bases: <code>RunningStats</code></p> <p>Create a running statistics object to track the mean, rate, and other statistics of a stream of time values. This is used to track time values in milliseconds and seconds.</p> <p>Adds time specific computed_fields such as measurements in milliseconds and seconds.</p> Source code in <code>src/guidellm/objects/statistics.py</code> <pre><code>class TimeRunningStats(RunningStats):\n    \"\"\"\n    Create a running statistics object to track the mean, rate, and other\n    statistics of a stream of time values. This is used to track time values\n    in milliseconds and seconds.\n\n    Adds time specific computed_fields such as measurements in milliseconds and seconds.\n    \"\"\"\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def total_ms(self) -&gt; float:\n        \"\"\"\n        :return: The total time multiplied by 1000.0 to convert to milliseconds.\n        \"\"\"\n        return self.total * 1000.0\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def last_ms(self) -&gt; float:\n        \"\"\"\n        :return: The last time multiplied by 1000.0 to convert to milliseconds.\n        \"\"\"\n        return self.last * 1000.0\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def mean_ms(self) -&gt; float:\n        \"\"\"\n        :return: The mean time multiplied by 1000.0 to convert to milliseconds.\n        \"\"\"\n        return self.mean * 1000.0\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def rate_ms(self) -&gt; float:\n        \"\"\"\n        :return: The rate of the running statistics multiplied by 1000.0\n            to convert to milliseconds.\n        \"\"\"\n        return self.rate * 1000.0\n</code></pre>"},{"location":"reference/guidellm/objects/statistics/#guidellm.objects.statistics.TimeRunningStats.last_ms","title":"<code>last_ms</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>float</code> <p>The last time multiplied by 1000.0 to convert to milliseconds.</p>"},{"location":"reference/guidellm/objects/statistics/#guidellm.objects.statistics.TimeRunningStats.mean_ms","title":"<code>mean_ms</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>float</code> <p>The mean time multiplied by 1000.0 to convert to milliseconds.</p>"},{"location":"reference/guidellm/objects/statistics/#guidellm.objects.statistics.TimeRunningStats.rate_ms","title":"<code>rate_ms</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>float</code> <p>The rate of the running statistics multiplied by 1000.0 to convert to milliseconds.</p>"},{"location":"reference/guidellm/objects/statistics/#guidellm.objects.statistics.TimeRunningStats.total_ms","title":"<code>total_ms</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>float</code> <p>The total time multiplied by 1000.0 to convert to milliseconds.</p>"},{"location":"reference/guidellm/request/","title":"guidellm.request","text":""},{"location":"reference/guidellm/request/#guidellm.request.GenerationRequest","title":"<code>GenerationRequest</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>A class representing a request for generation. This class is used to encapsulate the details of a generation request, including the request ID, type, content, parameters, statistics, and constraints. It is designed to be used with the BackendRequestsWorker class to handle the generation process.</p> <p>Parameters:</p> Name Type Description Default <code>request_id</code> <p>The unique identifier for the request.</p> required <code>request_type</code> <p>The type of request (e.g., text, chat).</p> required <code>content</code> <p>The content for the request to send to the backend. If request_type is 'text', this should be a string or list of strings which will be resolved by backend.text_completions. If request_type is 'chat', this should be a string, a list of (str, Dict[str, Union[str, Dict[str, str]], Path, Image]), or Any raw content which will be resolved by backend.chat_completions. If raw content, raw_content=True must be passed in the params.</p> required <code>params</code> <p>Additional parameters for the request passed in as kwargs. For an http backend, these are passed into the body of the request.</p> required <code>stats</code> <p>Statistics for the request, such as the number of prompt tokens. Used for tracking and reporting purposes.</p> required <code>constraints</code> <p>Constraints for the request, such as the maximum number of output tokens. Used for controlling the behavior of the backend.</p> required Source code in <code>src/guidellm/request/request.py</code> <pre><code>class GenerationRequest(StandardBaseModel):\n    \"\"\"\n    A class representing a request for generation.\n    This class is used to encapsulate the details of a generation request,\n    including the request ID, type, content, parameters, statistics, and constraints.\n    It is designed to be used with the BackendRequestsWorker class to handle\n    the generation process.\n\n    :param request_id: The unique identifier for the request.\n    :param request_type: The type of request (e.g., text, chat).\n    :param content: The content for the request to send to the backend.\n        If request_type is 'text', this should be a string or list of strings\n        which will be resolved by backend.text_completions.\n        If request_type is 'chat', this should be a string,\n        a list of (str, Dict[str, Union[str, Dict[str, str]], Path, Image]),\n        or Any raw content which will be resolved by backend.chat_completions.\n        If raw content, raw_content=True must be passed in the params.\n    :param params: Additional parameters for the request passed in as kwargs.\n        For an http backend, these are passed into the body of the request.\n    :param stats: Statistics for the request, such as the number of prompt tokens.\n        Used for tracking and reporting purposes.\n    :param constraints: Constraints for the request, such as the maximum number\n        of output tokens. Used for controlling the behavior of the backend.\n    \"\"\"\n\n    request_id: Optional[str] = Field(\n        default_factory=lambda: str(uuid.uuid4()),\n        description=\"The unique identifier for the request.\",\n    )\n    request_type: Literal[\"text_completions\", \"chat_completions\"] = Field(\n        default=\"text_completions\",\n        description=(\n            \"The type of request (e.g., text, chat). \"\n            \"If request_type='text_completions', resolved by backend.text_completions. \"\n            \"If request_typ='chat_completions', resolved by backend.chat_completions.\"\n        ),\n    )\n    content: Any = Field(\n        description=(\n            \"The content for the request to send to the backend. \"\n            \"If request_type is 'text', this should be a string or list of strings \"\n            \"which will be resolved by backend.text_completions. \"\n            \"If request_type is 'chat', this should be a string, \"\n            \"a list of (str, Dict[str, Union[str, Dict[str, str]], Path, Image]), \"\n            \"or Any raw content which will be resolved by backend.chat_completions. \"\n            \"If raw content, raw_content=True must be passed in the params.\"\n        )\n    )\n    params: dict[str, Any] = Field(\n        default_factory=dict,\n        description=(\n            \"Additional parameters for the request that will be passed in as kwargs. \"\n            \"For an http backend, these are passed into the body of the request. \"\n        ),\n    )\n    stats: dict[Literal[\"prompt_tokens\"], int] = Field(\n        default_factory=dict,\n        description=(\n            \"Statistics for the request, such as the number of prompt tokens. \"\n            \"Used for tracking and reporting purposes.\"\n        ),\n    )\n    constraints: dict[Literal[\"output_tokens\"], int] = Field(\n        default_factory=dict,\n        description=(\n            \"Constraints for the request, such as the maximum number of output tokens. \"\n            \"Used for controlling the behavior of the backend.\"\n        ),\n    )\n</code></pre>"},{"location":"reference/guidellm/request/loader/","title":"guidellm.request.loader","text":""},{"location":"reference/guidellm/request/request/","title":"guidellm.request.request","text":""},{"location":"reference/guidellm/request/request/#guidellm.request.request.GenerationRequest","title":"<code>GenerationRequest</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>A class representing a request for generation. This class is used to encapsulate the details of a generation request, including the request ID, type, content, parameters, statistics, and constraints. It is designed to be used with the BackendRequestsWorker class to handle the generation process.</p> <p>Parameters:</p> Name Type Description Default <code>request_id</code> <p>The unique identifier for the request.</p> required <code>request_type</code> <p>The type of request (e.g., text, chat).</p> required <code>content</code> <p>The content for the request to send to the backend. If request_type is 'text', this should be a string or list of strings which will be resolved by backend.text_completions. If request_type is 'chat', this should be a string, a list of (str, Dict[str, Union[str, Dict[str, str]], Path, Image]), or Any raw content which will be resolved by backend.chat_completions. If raw content, raw_content=True must be passed in the params.</p> required <code>params</code> <p>Additional parameters for the request passed in as kwargs. For an http backend, these are passed into the body of the request.</p> required <code>stats</code> <p>Statistics for the request, such as the number of prompt tokens. Used for tracking and reporting purposes.</p> required <code>constraints</code> <p>Constraints for the request, such as the maximum number of output tokens. Used for controlling the behavior of the backend.</p> required Source code in <code>src/guidellm/request/request.py</code> <pre><code>class GenerationRequest(StandardBaseModel):\n    \"\"\"\n    A class representing a request for generation.\n    This class is used to encapsulate the details of a generation request,\n    including the request ID, type, content, parameters, statistics, and constraints.\n    It is designed to be used with the BackendRequestsWorker class to handle\n    the generation process.\n\n    :param request_id: The unique identifier for the request.\n    :param request_type: The type of request (e.g., text, chat).\n    :param content: The content for the request to send to the backend.\n        If request_type is 'text', this should be a string or list of strings\n        which will be resolved by backend.text_completions.\n        If request_type is 'chat', this should be a string,\n        a list of (str, Dict[str, Union[str, Dict[str, str]], Path, Image]),\n        or Any raw content which will be resolved by backend.chat_completions.\n        If raw content, raw_content=True must be passed in the params.\n    :param params: Additional parameters for the request passed in as kwargs.\n        For an http backend, these are passed into the body of the request.\n    :param stats: Statistics for the request, such as the number of prompt tokens.\n        Used for tracking and reporting purposes.\n    :param constraints: Constraints for the request, such as the maximum number\n        of output tokens. Used for controlling the behavior of the backend.\n    \"\"\"\n\n    request_id: Optional[str] = Field(\n        default_factory=lambda: str(uuid.uuid4()),\n        description=\"The unique identifier for the request.\",\n    )\n    request_type: Literal[\"text_completions\", \"chat_completions\"] = Field(\n        default=\"text_completions\",\n        description=(\n            \"The type of request (e.g., text, chat). \"\n            \"If request_type='text_completions', resolved by backend.text_completions. \"\n            \"If request_typ='chat_completions', resolved by backend.chat_completions.\"\n        ),\n    )\n    content: Any = Field(\n        description=(\n            \"The content for the request to send to the backend. \"\n            \"If request_type is 'text', this should be a string or list of strings \"\n            \"which will be resolved by backend.text_completions. \"\n            \"If request_type is 'chat', this should be a string, \"\n            \"a list of (str, Dict[str, Union[str, Dict[str, str]], Path, Image]), \"\n            \"or Any raw content which will be resolved by backend.chat_completions. \"\n            \"If raw content, raw_content=True must be passed in the params.\"\n        )\n    )\n    params: dict[str, Any] = Field(\n        default_factory=dict,\n        description=(\n            \"Additional parameters for the request that will be passed in as kwargs. \"\n            \"For an http backend, these are passed into the body of the request. \"\n        ),\n    )\n    stats: dict[Literal[\"prompt_tokens\"], int] = Field(\n        default_factory=dict,\n        description=(\n            \"Statistics for the request, such as the number of prompt tokens. \"\n            \"Used for tracking and reporting purposes.\"\n        ),\n    )\n    constraints: dict[Literal[\"output_tokens\"], int] = Field(\n        default_factory=dict,\n        description=(\n            \"Constraints for the request, such as the maximum number of output tokens. \"\n            \"Used for controlling the behavior of the backend.\"\n        ),\n    )\n</code></pre>"},{"location":"reference/guidellm/scheduler/","title":"guidellm.scheduler","text":""},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.AsyncConstantStrategy","title":"<code>AsyncConstantStrategy</code>","text":"<p>               Bases: <code>ThroughputStrategy</code></p> <p>A class representing an asynchronous constant scheduling strategy. This strategy schedules requests asynchronously at a constant request rate in requests per second. If initial_burst is set, it will send an initial burst of math.floor(rate) requests to reach the target rate. This is useful to ensure that the target rate is reached quickly and then maintained. It inherits from the <code>SchedulingStrategy</code> base class and implements the <code>request_times</code> method to provide the specific behavior for asynchronous constant scheduling.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <p>The constant StrategyType to schedule requests asynchronously.</p> required <code>rate</code> <p>The rate at which to schedule requests asynchronously in requests per second. This must be a positive float.</p> required <code>initial_burst</code> <p>True to send an initial burst of requests (math.floor(self.rate)) to reach target rate. False to not send an initial burst.</p> required Source code in <code>src/guidellm/scheduler/strategy.py</code> <pre><code>class AsyncConstantStrategy(ThroughputStrategy):\n    \"\"\"\n    A class representing an asynchronous constant scheduling strategy.\n    This strategy schedules requests asynchronously at a constant request rate\n    in requests per second.\n    If initial_burst is set, it will send an initial burst of math.floor(rate)\n    requests to reach the target rate.\n    This is useful to ensure that the target rate is reached quickly\n    and then maintained.\n    It inherits from the `SchedulingStrategy` base class and\n    implements the `request_times` method to provide the specific\n    behavior for asynchronous constant scheduling.\n\n    :param type_: The constant StrategyType to schedule requests asynchronously.\n    :param rate: The rate at which to schedule requests asynchronously in\n        requests per second. This must be a positive float.\n    :param initial_burst: True to send an initial burst of requests\n        (math.floor(self.rate)) to reach target rate.\n        False to not send an initial burst.\n    \"\"\"\n\n    type_: Literal[\"constant\"] = \"constant\"  # type: ignore[assignment]\n    rate: float = Field(\n        description=(\n            \"The rate at which to schedule requests asynchronously in \"\n            \"requests per second. This must be a positive float.\"\n        ),\n        gt=0,\n    )\n    initial_burst: bool = Field(\n        default=True,\n        description=(\n            \"True to send an initial burst of requests (math.floor(self.rate)) \"\n            \"to reach target rate. False to not send an initial burst.\"\n        ),\n    )\n\n    def request_times(self) -&gt; Generator[float, None, None]:\n        \"\"\"\n        A generator that yields timestamps for when requests should be sent.\n        This method schedules requests asynchronously at a constant rate\n        in requests per second.\n        If burst_time is set, it will send an initial burst of requests\n        to reach the target rate.\n        This is useful to ensure that the target rate is reached quickly\n        and then maintained.\n\n        :return: A generator that yields timestamps for request scheduling.\n        \"\"\"\n        start_time = time.time()\n        constant_increment = 1.0 / self.rate\n\n        # handle bursts first to get to the desired rate\n        if self.initial_burst is not None:\n            # send an initial burst equal to the rate\n            # to reach the target rate\n            burst_count = math.floor(self.rate)\n            for _ in range(burst_count):\n                yield start_time\n\n            start_time += constant_increment\n\n        counter = 0\n\n        # continue with constant rate after bursting\n        while True:\n            yield start_time + constant_increment * counter\n            counter += 1\n</code></pre>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.AsyncConstantStrategy.request_times","title":"<code>request_times()</code>","text":"<p>A generator that yields timestamps for when requests should be sent. This method schedules requests asynchronously at a constant rate in requests per second. If burst_time is set, it will send an initial burst of requests to reach the target rate. This is useful to ensure that the target rate is reached quickly and then maintained.</p> <p>Returns:</p> Type Description <code>Generator[float, None, None]</code> <p>A generator that yields timestamps for request scheduling.</p> Source code in <code>src/guidellm/scheduler/strategy.py</code> <pre><code>def request_times(self) -&gt; Generator[float, None, None]:\n    \"\"\"\n    A generator that yields timestamps for when requests should be sent.\n    This method schedules requests asynchronously at a constant rate\n    in requests per second.\n    If burst_time is set, it will send an initial burst of requests\n    to reach the target rate.\n    This is useful to ensure that the target rate is reached quickly\n    and then maintained.\n\n    :return: A generator that yields timestamps for request scheduling.\n    \"\"\"\n    start_time = time.time()\n    constant_increment = 1.0 / self.rate\n\n    # handle bursts first to get to the desired rate\n    if self.initial_burst is not None:\n        # send an initial burst equal to the rate\n        # to reach the target rate\n        burst_count = math.floor(self.rate)\n        for _ in range(burst_count):\n            yield start_time\n\n        start_time += constant_increment\n\n    counter = 0\n\n    # continue with constant rate after bursting\n    while True:\n        yield start_time + constant_increment * counter\n        counter += 1\n</code></pre>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.AsyncPoissonStrategy","title":"<code>AsyncPoissonStrategy</code>","text":"<p>               Bases: <code>ThroughputStrategy</code></p> <p>A class representing an asynchronous Poisson scheduling strategy. This strategy schedules requests asynchronously at a Poisson request rate in requests per second. If initial_burst is set, it will send an initial burst of math.floor(rate) requests to reach the target rate. It inherits from the <code>SchedulingStrategy</code> base class and implements the <code>request_times</code> method to provide the specific behavior for asynchronous Poisson scheduling.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <p>The Poisson StrategyType to schedule requests asynchronously.</p> required <code>rate</code> <p>The rate at which to schedule requests asynchronously in requests per second. This must be a positive float.</p> required <code>initial_burst</code> <p>True to send an initial burst of requests (math.floor(self.rate)) to reach target rate. False to not send an initial burst.</p> required Source code in <code>src/guidellm/scheduler/strategy.py</code> <pre><code>class AsyncPoissonStrategy(ThroughputStrategy):\n    \"\"\"\n    A class representing an asynchronous Poisson scheduling strategy.\n    This strategy schedules requests asynchronously at a Poisson request rate\n    in requests per second.\n    If initial_burst is set, it will send an initial burst of math.floor(rate)\n    requests to reach the target rate.\n    It inherits from the `SchedulingStrategy` base class and\n    implements the `request_times` method to provide the specific\n    behavior for asynchronous Poisson scheduling.\n\n    :param type_: The Poisson StrategyType to schedule requests asynchronously.\n    :param rate: The rate at which to schedule requests asynchronously in\n        requests per second. This must be a positive float.\n    :param initial_burst: True to send an initial burst of requests\n        (math.floor(self.rate)) to reach target rate.\n        False to not send an initial burst.\n    \"\"\"\n\n    type_: Literal[\"poisson\"] = \"poisson\"  # type: ignore[assignment]\n    rate: float = Field(\n        description=(\n            \"The rate at which to schedule requests asynchronously in \"\n            \"requests per second. This must be a positive float.\"\n        ),\n        gt=0,\n    )\n    initial_burst: bool = Field(\n        default=True,\n        description=(\n            \"True to send an initial burst of requests (math.floor(self.rate)) \"\n            \"to reach target rate. False to not send an initial burst.\"\n        ),\n    )\n    random_seed: int = Field(\n        default=42,\n        description=(\"The random seed to use for the Poisson distribution. \"),\n    )\n\n    def request_times(self) -&gt; Generator[float, None, None]:\n        \"\"\"\n        A generator that yields timestamps for when requests should be sent.\n        This method schedules requests asynchronously at a Poisson rate\n        in requests per second.\n        The inter arrival time between requests is exponentially distributed\n        based on the rate.\n\n        :return: A generator that yields timestamps for request scheduling.\n        \"\"\"\n        start_time = time.time()\n\n        if self.initial_burst is not None:\n            # send an initial burst equal to the rate\n            # to reach the target rate\n            burst_count = math.floor(self.rate)\n            for _ in range(burst_count):\n                yield start_time\n        else:\n            yield start_time\n\n        # set the random seed for reproducibility\n        rand = random.Random(self.random_seed)  # noqa: S311\n\n        while True:\n            inter_arrival_time = rand.expovariate(self.rate)\n            start_time += inter_arrival_time\n            yield start_time\n</code></pre>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.AsyncPoissonStrategy.request_times","title":"<code>request_times()</code>","text":"<p>A generator that yields timestamps for when requests should be sent. This method schedules requests asynchronously at a Poisson rate in requests per second. The inter arrival time between requests is exponentially distributed based on the rate.</p> <p>Returns:</p> Type Description <code>Generator[float, None, None]</code> <p>A generator that yields timestamps for request scheduling.</p> Source code in <code>src/guidellm/scheduler/strategy.py</code> <pre><code>def request_times(self) -&gt; Generator[float, None, None]:\n    \"\"\"\n    A generator that yields timestamps for when requests should be sent.\n    This method schedules requests asynchronously at a Poisson rate\n    in requests per second.\n    The inter arrival time between requests is exponentially distributed\n    based on the rate.\n\n    :return: A generator that yields timestamps for request scheduling.\n    \"\"\"\n    start_time = time.time()\n\n    if self.initial_burst is not None:\n        # send an initial burst equal to the rate\n        # to reach the target rate\n        burst_count = math.floor(self.rate)\n        for _ in range(burst_count):\n            yield start_time\n    else:\n        yield start_time\n\n    # set the random seed for reproducibility\n    rand = random.Random(self.random_seed)  # noqa: S311\n\n    while True:\n        inter_arrival_time = rand.expovariate(self.rate)\n        start_time += inter_arrival_time\n        yield start_time\n</code></pre>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.ConcurrentStrategy","title":"<code>ConcurrentStrategy</code>","text":"<p>               Bases: <code>SchedulingStrategy</code></p> <p>A class representing a concurrent scheduling strategy. This strategy schedules requests concurrently with the specified number of streams. It inherits from the <code>SchedulingStrategy</code> base class and implements the <code>request_times</code> method to provide the specific behavior for concurrent scheduling.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <p>The concurrent StrategyType to schedule requests concurrently.</p> required <code>streams</code> <p>The number of concurrent streams to use for scheduling requests. Each stream runs synchronously with the maximum rate possible. This must be a positive integer.</p> required Source code in <code>src/guidellm/scheduler/strategy.py</code> <pre><code>class ConcurrentStrategy(SchedulingStrategy):\n    \"\"\"\n    A class representing a concurrent scheduling strategy.\n    This strategy schedules requests concurrently with the specified\n    number of streams.\n    It inherits from the `SchedulingStrategy` base class and\n    implements the `request_times` method to provide the specific\n    behavior for concurrent scheduling.\n\n    :param type_: The concurrent StrategyType to schedule requests concurrently.\n    :param streams: The number of concurrent streams to use for scheduling requests.\n        Each stream runs synchronously with the maximum rate possible.\n        This must be a positive integer.\n    \"\"\"\n\n    type_: Literal[\"concurrent\"] = \"concurrent\"  # type: ignore[assignment]\n    streams: int = Field(\n        description=(\n            \"The number of concurrent streams to use for scheduling requests. \"\n            \"Each stream runs sychronously with the maximum rate possible. \"\n            \"This must be a positive integer.\"\n        ),\n        gt=0,\n    )\n\n    @property\n    def processing_mode(self) -&gt; Literal[\"sync\"]:\n        \"\"\"\n        The processing mode for the scheduling strategy, either 'sync' or 'async'.\n        This property determines how the worker processes are setup:\n        either to run synchronously with one request at a time or asynchronously.\n\n        :return: 'sync' for synchronous scheduling strategy\n            for the multiple worker processes equal to streams.\n        \"\"\"\n        return \"sync\"\n\n    @property\n    def processes_limit(self) -&gt; int:\n        \"\"\"\n        The limit on the number of worker processes for the scheduling strategy.\n        It determines how many worker processes are created\n        for the scheduling strategy and must be implemented by subclasses.\n\n        :return: {self.streams} for the concurrent scheduling strategy to limit\n            the worker processes to the number of streams.\n        \"\"\"\n        return self.streams\n\n    @property\n    def queued_requests_limit(self) -&gt; int:\n        \"\"\"\n        The maximum number of queued requests for the scheduling strategy.\n        It determines how many requests can be queued at one time\n        for the scheduling strategy and must be implemented by subclasses.\n\n        :return: {self.streams} for the concurrent scheduling strategy to limit\n            the queued requests to the number of streams that are ready to be processed.\n        \"\"\"\n        return self.streams\n\n    @property\n    def processing_requests_limit(self) -&gt; int:\n        \"\"\"\n        The maximum number of processing requests for the scheduling strategy.\n        It determines how many requests can be processed at one time\n        for the scheduling strategy and must be implemented by subclasses.\n\n        :return: {self.streams} for the concurrent scheduling strategy to limit\n            the processing requests to the number of streams that ready to be processed.\n        \"\"\"\n        return self.streams\n\n    def request_times(self) -&gt; Generator[float, None, None]:\n        \"\"\"\n        A generator that yields time.time() so requests are sent\n        immediately, while scheduling them concurrently with the specified\n        number of streams.\n\n        :return: A generator that yields time.time() for immediate request scheduling.\n        \"\"\"\n        while True:\n            yield time.time()\n</code></pre>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.ConcurrentStrategy.processes_limit","title":"<code>processes_limit</code>  <code>property</code>","text":"<p>The limit on the number of worker processes for the scheduling strategy. It determines how many worker processes are created for the scheduling strategy and must be implemented by subclasses.</p> <p>Returns:</p> Type Description <code>int</code> <p>{self.streams} for the concurrent scheduling strategy to limit the worker processes to the number of streams.</p>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.ConcurrentStrategy.processing_mode","title":"<code>processing_mode</code>  <code>property</code>","text":"<p>The processing mode for the scheduling strategy, either 'sync' or 'async'. This property determines how the worker processes are setup: either to run synchronously with one request at a time or asynchronously.</p> <p>Returns:</p> Type Description <code>Literal['sync']</code> <p>'sync' for synchronous scheduling strategy for the multiple worker processes equal to streams.</p>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.ConcurrentStrategy.processing_requests_limit","title":"<code>processing_requests_limit</code>  <code>property</code>","text":"<p>The maximum number of processing requests for the scheduling strategy. It determines how many requests can be processed at one time for the scheduling strategy and must be implemented by subclasses.</p> <p>Returns:</p> Type Description <code>int</code> <p>{self.streams} for the concurrent scheduling strategy to limit the processing requests to the number of streams that ready to be processed.</p>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.ConcurrentStrategy.queued_requests_limit","title":"<code>queued_requests_limit</code>  <code>property</code>","text":"<p>The maximum number of queued requests for the scheduling strategy. It determines how many requests can be queued at one time for the scheduling strategy and must be implemented by subclasses.</p> <p>Returns:</p> Type Description <code>int</code> <p>{self.streams} for the concurrent scheduling strategy to limit the queued requests to the number of streams that are ready to be processed.</p>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.ConcurrentStrategy.request_times","title":"<code>request_times()</code>","text":"<p>A generator that yields time.time() so requests are sent immediately, while scheduling them concurrently with the specified number of streams.</p> <p>Returns:</p> Type Description <code>Generator[float, None, None]</code> <p>A generator that yields time.time() for immediate request scheduling.</p> Source code in <code>src/guidellm/scheduler/strategy.py</code> <pre><code>def request_times(self) -&gt; Generator[float, None, None]:\n    \"\"\"\n    A generator that yields time.time() so requests are sent\n    immediately, while scheduling them concurrently with the specified\n    number of streams.\n\n    :return: A generator that yields time.time() for immediate request scheduling.\n    \"\"\"\n    while True:\n        yield time.time()\n</code></pre>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.GenerativeRequestsWorker","title":"<code>GenerativeRequestsWorker</code>","text":"<p>               Bases: <code>RequestsWorker[GenerationRequest, ResponseSummary]</code></p> <p>A class that handles the execution of requests using a backend. This class is responsible for sending requests to the backend, handling responses, and managing errors.</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <code>Backend</code> <p>The backend to use for handling requests. This should be an instance of Backend such as an OpenAIHTTPBackend.</p> required Source code in <code>src/guidellm/scheduler/worker.py</code> <pre><code>class GenerativeRequestsWorker(RequestsWorker[GenerationRequest, ResponseSummary]):\n    \"\"\"\n    A class that handles the execution of requests using a backend.\n    This class is responsible for sending requests to the backend,\n    handling responses, and managing errors.\n\n    :param backend: The backend to use for handling requests.\n        This should be an instance of Backend such as an OpenAIHTTPBackend.\n    \"\"\"\n\n    def __init__(self, backend: Backend):\n        self.backend = backend\n\n    @property\n    def description(self) -&gt; GenerativeRequestsWorkerDescription:\n        \"\"\"\n        Get the description of the worker.\n        :return: The description of the worker.\n        \"\"\"\n        return GenerativeRequestsWorkerDescription(\n            backend_type=self.backend.type_,\n            backend_target=self.backend.target,\n            backend_model=self.backend.model or \"None\",\n            backend_info=self.backend.info,\n        )\n\n    async def prepare_multiprocessing(self):\n        \"\"\"\n        Prepare the worker for multiprocessing.\n        This is useful for workers that have instance state that can not\n        be shared across processes and should be cleared out and re-initialized\n        for each new process.\n        \"\"\"\n        await self.backend.prepare_multiprocessing()\n\n    def process_loop_synchronous(\n        self,\n        requests_queue: multiprocessing.Queue,\n        results_queue: multiprocessing.Queue,\n        process_id: int,\n    ):\n        asyncio.run(self.backend.validate())\n        super().process_loop_synchronous(\n            requests_queue=requests_queue,\n            results_queue=results_queue,\n            process_id=process_id,\n        )\n\n    def process_loop_asynchronous(\n        self,\n        requests_queue: multiprocessing.Queue,\n        results_queue: multiprocessing.Queue,\n        max_concurrency: int,\n        process_id: int,\n    ):\n        asyncio.run(self.backend.validate())\n        super().process_loop_asynchronous(\n            requests_queue=requests_queue,\n            results_queue=results_queue,\n            max_concurrency=max_concurrency,\n            process_id=process_id,\n        )\n\n    async def resolve(\n        self,\n        request: GenerationRequest,\n        timeout_time: float,\n    ) -&gt; tuple[ResolveStatus, ResponseSummary]:\n        \"\"\"\n        Resolve a request by sending it to the backend and handling the response.\n        This method sends the request to the backend, waits for a response,\n        and handles any errors that may occur during the process.\n\n        :param request: The request to resolve.\n        :param timeout_time: The time to wait for a response before timing out.\n            If timeout_time is math.inf, the request will not timeout.\n        :return: A ResponseSummary object containing the response from the backend.\n            If an error occurs, the ResponseSummary will contain the error message.\n        \"\"\"\n        resolve_start_time = time.time()\n        response = None\n        error: Optional[str] = None\n        status = ResolveStatus(\n            requested=False,\n            completed=False,\n            errored=False,\n            canceled=False,\n            request_start=-1,\n            request_end=-1,\n        )\n\n        try:\n            if timeout_time &lt; time.time():\n                raise asyncio.TimeoutError(\n                    \"The timeout time has already passed.\"\n                )  # exit early\n\n            status.requested = True\n            request_func, request_kwargs = self._create_request_func_kwargs(request)\n\n            async def _runner():\n                # wrap function so we can enforce timeout and\n                # still return the latest state from the backend\n                async for resp in request_func(**request_kwargs):  # type: ignore[operator]\n                    nonlocal response\n                    response = resp\n\n            await asyncio.wait_for(\n                _runner(),\n                timeout=timeout_time - time.time() if timeout_time &lt; math.inf else None,\n            )\n\n            if not response:\n                raise ValueError(\n                    f\"No response received for request: {request} \"\n                    f\"and backend: {self.backend}\"\n                )\n            if not isinstance(response, ResponseSummary):\n                raise ValueError(\n                    f\"Received no ResponseSummary for request: {request} \"\n                    f\"and backend: {self.backend}, received: {response}\"\n                )\n\n            status.completed = True\n        except asyncio.TimeoutError:\n            error = \"TimeoutError: The request timed out before completing.\"\n            status.errored = True\n            status.canceled = True\n        except Exception as exc:  # noqa: BLE001\n            error = str(exc)\n            status.errored = True\n\n        return self._handle_response(\n            status=status,\n            request=request,\n            response=response,\n            error=error,\n            resolve_start_time=resolve_start_time,\n        )\n\n    def _create_request_func_kwargs(\n        self,\n        request: GenerationRequest,\n    ) -&gt; tuple[\n        AsyncGenerator[Union[StreamingTextResponse, ResponseSummary], None],\n        dict[str, Any],\n    ]:\n        request_func: AsyncGenerator[\n            Union[StreamingTextResponse, ResponseSummary], None\n        ]\n        request_kwargs: dict[str, Any]\n\n        if request.request_type == \"text_completions\":\n            request_func = self.backend.text_completions  # type: ignore[assignment]\n            request_kwargs = {\n                \"prompt\": request.content,\n                \"request_id\": request.request_id,\n                \"prompt_token_count\": request.stats.get(\"prompt_tokens\", None),\n                \"output_token_count\": request.constraints.get(\"output_tokens\", None),\n                **request.params,\n            }\n        elif request.request_type == \"chat_completions\":\n            request_func = self.backend.chat_completions  # type: ignore[assignment]\n            request_kwargs = {\n                \"content\": request.content,\n                \"request_id\": request.request_id,\n                \"prompt_token_count\": request.stats.get(\"prompt_tokens\", None),\n                \"output_token_count\": request.constraints.get(\"output_tokens\", None),\n                **request.params,\n            }\n        else:\n            raise ValueError(\n                f\"Invalid request type: {request.request_type} for {request}\"\n            )\n\n        return request_func, request_kwargs\n\n    def _handle_response(\n        self,\n        status: ResolveStatus,\n        request: GenerationRequest,\n        response: Any,\n        error: Optional[str],\n        resolve_start_time: float,\n    ) -&gt; tuple[ResolveStatus, ResponseSummary]:\n        if response is None or not isinstance(\n            response, (ResponseSummary, StreamingTextResponse)\n        ):\n            # nothing received or invalid response, fill in defaults for error\n            if response:\n                error = str(\n                    ValueError(\n                        f\"Invalid response: {type(response)} for request: {request}; \"\n                    )\n                ) + (error or \"\")\n\n            response = ResponseSummary(\n                value=\"\",\n                request_args=RequestArgs(\n                    target=self.backend.target,\n                    headers={},\n                    params={},\n                    payload={},\n                ),\n                start_time=resolve_start_time,\n                end_time=status.request_end,\n                first_iter_time=None,\n                last_iter_time=None,\n                request_id=request.request_id,\n                error=error or \"Unknown error\",\n            )\n        elif isinstance(response, StreamingTextResponse):\n            response = ResponseSummary(\n                value=response.value,\n                request_args=RequestArgs(\n                    target=self.backend.target,\n                    headers={},\n                    params={},\n                    payload={},\n                ),\n                start_time=response.start_time,\n                end_time=time.time(),\n                first_iter_time=response.first_iter_time,\n                last_iter_time=response.time if response.iter_count &gt; 0 else None,\n                request_prompt_tokens=request.stats.get(\"prompt_tokens\", None),\n                request_output_tokens=request.constraints.get(\"output_tokens\", None),\n                response_prompt_tokens=None,\n                response_output_tokens=response.iter_count,\n                request_id=request.request_id,\n                error=error or \"Unknown error\",\n            )\n\n        response.error = error\n        status.request_start = response.start_time\n        status.request_end = response.end_time\n\n        return status, response\n</code></pre>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.GenerativeRequestsWorker.description","title":"<code>description</code>  <code>property</code>","text":"<p>Get the description of the worker.</p> <p>Returns:</p> Type Description <code>GenerativeRequestsWorkerDescription</code> <p>The description of the worker.</p>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.GenerativeRequestsWorker.prepare_multiprocessing","title":"<code>prepare_multiprocessing()</code>  <code>async</code>","text":"<p>Prepare the worker for multiprocessing. This is useful for workers that have instance state that can not be shared across processes and should be cleared out and re-initialized for each new process.</p> Source code in <code>src/guidellm/scheduler/worker.py</code> <pre><code>async def prepare_multiprocessing(self):\n    \"\"\"\n    Prepare the worker for multiprocessing.\n    This is useful for workers that have instance state that can not\n    be shared across processes and should be cleared out and re-initialized\n    for each new process.\n    \"\"\"\n    await self.backend.prepare_multiprocessing()\n</code></pre>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.GenerativeRequestsWorker.resolve","title":"<code>resolve(request, timeout_time)</code>  <code>async</code>","text":"<p>Resolve a request by sending it to the backend and handling the response. This method sends the request to the backend, waits for a response, and handles any errors that may occur during the process.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>GenerationRequest</code> <p>The request to resolve.</p> required <code>timeout_time</code> <code>float</code> <p>The time to wait for a response before timing out. If timeout_time is math.inf, the request will not timeout.</p> required <p>Returns:</p> Type Description <code>tuple[ResolveStatus, ResponseSummary]</code> <p>A ResponseSummary object containing the response from the backend. If an error occurs, the ResponseSummary will contain the error message.</p> Source code in <code>src/guidellm/scheduler/worker.py</code> <pre><code>async def resolve(\n    self,\n    request: GenerationRequest,\n    timeout_time: float,\n) -&gt; tuple[ResolveStatus, ResponseSummary]:\n    \"\"\"\n    Resolve a request by sending it to the backend and handling the response.\n    This method sends the request to the backend, waits for a response,\n    and handles any errors that may occur during the process.\n\n    :param request: The request to resolve.\n    :param timeout_time: The time to wait for a response before timing out.\n        If timeout_time is math.inf, the request will not timeout.\n    :return: A ResponseSummary object containing the response from the backend.\n        If an error occurs, the ResponseSummary will contain the error message.\n    \"\"\"\n    resolve_start_time = time.time()\n    response = None\n    error: Optional[str] = None\n    status = ResolveStatus(\n        requested=False,\n        completed=False,\n        errored=False,\n        canceled=False,\n        request_start=-1,\n        request_end=-1,\n    )\n\n    try:\n        if timeout_time &lt; time.time():\n            raise asyncio.TimeoutError(\n                \"The timeout time has already passed.\"\n            )  # exit early\n\n        status.requested = True\n        request_func, request_kwargs = self._create_request_func_kwargs(request)\n\n        async def _runner():\n            # wrap function so we can enforce timeout and\n            # still return the latest state from the backend\n            async for resp in request_func(**request_kwargs):  # type: ignore[operator]\n                nonlocal response\n                response = resp\n\n        await asyncio.wait_for(\n            _runner(),\n            timeout=timeout_time - time.time() if timeout_time &lt; math.inf else None,\n        )\n\n        if not response:\n            raise ValueError(\n                f\"No response received for request: {request} \"\n                f\"and backend: {self.backend}\"\n            )\n        if not isinstance(response, ResponseSummary):\n            raise ValueError(\n                f\"Received no ResponseSummary for request: {request} \"\n                f\"and backend: {self.backend}, received: {response}\"\n            )\n\n        status.completed = True\n    except asyncio.TimeoutError:\n        error = \"TimeoutError: The request timed out before completing.\"\n        status.errored = True\n        status.canceled = True\n    except Exception as exc:  # noqa: BLE001\n        error = str(exc)\n        status.errored = True\n\n    return self._handle_response(\n        status=status,\n        request=request,\n        response=response,\n        error=error,\n        resolve_start_time=resolve_start_time,\n    )\n</code></pre>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.RequestsWorker","title":"<code>RequestsWorker</code>","text":"<p>               Bases: <code>ABC</code>, <code>Generic[RequestT, ResponseT]</code></p> <p>An abstract base class for a worker that processes requests. This class defines the interface for a worker that can resolve requests asynchronously or synchronously within the Scheduler class. Subclasses must implement the <code>resolve</code> method, which takes a request directly given from the load generator, along with the desired start_time for the request and a timeout_time. The <code>resolve</code> method should return the response from the backend.</p> Source code in <code>src/guidellm/scheduler/worker.py</code> <pre><code>class RequestsWorker(ABC, Generic[RequestT, ResponseT]):\n    \"\"\"\n    An abstract base class for a worker that processes requests.\n    This class defines the interface for a worker that can resolve requests\n    asynchronously or synchronously within the Scheduler class.\n    Subclasses must implement the `resolve` method,\n    which takes a request directly given from the load generator,\n    along with the desired start_time for the request and a timeout_time.\n    The `resolve` method should return the response from the backend.\n    \"\"\"\n\n    @property\n    @abstractmethod\n    def description(self) -&gt; WorkerDescription:\n        \"\"\"\n        An abstract property that must be implemented by subclasses.\n        This property should return a Serializable class representing the information\n        about the worker instance.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    async def prepare_multiprocessing(self):\n        \"\"\"\n        An abstract method that must be implemented by subclasses.\n        This is useful for workers that have instance state that can not\n        be shared across processes and should be cleared out and re-initialized\n        for each new process.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    async def resolve(\n        self,\n        request: RequestT,\n        timeout_time: float,\n    ) -&gt; tuple[ResolveStatus, ResponseT]:\n        \"\"\"\n        An abstract method that must be implemented by subclasses.\n        This method should handle the resolution of a request through asyncio,\n        including any necessary backend processing and response handling.\n\n        :param request: The request to be resolved generated by the load generator.\n        :param timeout_time: The timeout time for the request, if there is no timeout\n            given, then this will be math.inf.\n        :return: The response from the worker.\n        \"\"\"\n        ...\n\n    async def get_request(\n        self, requests_queue: multiprocessing.Queue\n    ) -&gt; Optional[WorkerProcessRequest[RequestT]]:\n        return await asyncio.to_thread(requests_queue.get)  # type: ignore[attr-defined]\n\n    async def send_result(\n        self,\n        results_queue: multiprocessing.Queue,\n        result: WorkerProcessResult[RequestT, ResponseT],\n    ):\n        await asyncio.to_thread(results_queue.put, result)  # type: ignore[attr-defined]\n\n    async def resolve_scheduler_request(\n        self,\n        request: Any,\n        queued_time: float,\n        dequeued_time: float,\n        start_time: float,\n        timeout_time: float,\n        results_queue: multiprocessing.Queue,\n        process_id: int,\n    ):\n        info = SchedulerRequestInfo(\n            targeted_start_time=start_time,\n            queued_time=queued_time,\n            dequeued_time=dequeued_time,\n            scheduled_time=time.time(),\n            process_id=process_id,\n        )\n        result: WorkerProcessResult[RequestT, ResponseT] = WorkerProcessResult(\n            type_=\"request_scheduled\",\n            request=request,\n            response=None,\n            info=info,\n        )\n        asyncio.create_task(self.send_result(results_queue, result))\n\n        if (wait_time := start_time - time.time()) &gt; 0:\n            await asyncio.sleep(wait_time)\n\n        info.worker_start = time.time()\n        result = WorkerProcessResult(\n            type_=\"request_start\",\n            request=request,\n            response=None,\n            info=info,\n        )\n        asyncio.create_task(self.send_result(results_queue, result))\n\n        status, response = await self.resolve(request, timeout_time)\n        info.worker_end = time.time()\n        info.requested = status.requested\n        info.completed = status.completed\n        info.errored = status.errored\n        info.canceled = status.canceled\n        info.request_start = status.request_start\n        info.request_end = status.request_end\n        result = WorkerProcessResult(\n            type_=\"request_complete\",\n            request=request,\n            response=response,\n            info=info,\n        )\n        asyncio.create_task(self.send_result(results_queue, result))\n\n    def process_loop_synchronous(\n        self,\n        requests_queue: multiprocessing.Queue,\n        results_queue: multiprocessing.Queue,\n        process_id: int,\n    ):\n        async def _process_runner():\n            while (\n                process_request := await self.get_request(requests_queue)\n            ) is not None:\n                dequeued_time = time.time()\n\n                await self.resolve_scheduler_request(\n                    request=process_request.request,\n                    queued_time=process_request.queued_time,\n                    dequeued_time=dequeued_time,\n                    start_time=process_request.start_time,\n                    timeout_time=process_request.timeout_time,\n                    results_queue=results_queue,\n                    process_id=process_id,\n                )\n\n        try:\n            asyncio.run(_process_runner())\n        except Exception as exc:  # noqa: BLE001\n            logger.error(\n                f\"Error in worker process {process_id}: {exc}\",\n                exc_info=True,\n                stack_info=True,\n            )\n\n    def process_loop_asynchronous(\n        self,\n        requests_queue: multiprocessing.Queue,\n        results_queue: multiprocessing.Queue,\n        max_concurrency: int,\n        process_id: int,\n    ):\n        async def _process_runner():\n            pending = asyncio.Semaphore(max_concurrency)\n\n            if pending.locked():\n                raise ValueError(\"Async worker called with max_concurrency &lt; 1\")\n\n            while (\n                process_request := await self.get_request(requests_queue)\n            ) is not None:\n                dequeued_time = time.time()\n\n                await pending.acquire()\n\n                def _task_done(_: asyncio.Task):\n                    nonlocal pending\n                    pending.release()\n\n                task = asyncio.create_task(\n                    self.resolve_scheduler_request(\n                        request=process_request.request,\n                        queued_time=process_request.queued_time,\n                        dequeued_time=dequeued_time,\n                        start_time=process_request.start_time,\n                        timeout_time=process_request.timeout_time,\n                        results_queue=results_queue,\n                        process_id=process_id,\n                    )\n                )\n                task.add_done_callback(_task_done)\n                await asyncio.sleep(0)  # enable start task immediately\n\n        try:\n            asyncio.run(_process_runner())\n        except Exception as exc:  # noqa: BLE001\n            logger.error(\n                f\"Error in worker process {process_id}: {exc}\",\n                exc_info=True,\n                stack_info=True,\n            )\n</code></pre>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.RequestsWorker.description","title":"<code>description</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>An abstract property that must be implemented by subclasses. This property should return a Serializable class representing the information about the worker instance.</p>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.RequestsWorker.prepare_multiprocessing","title":"<code>prepare_multiprocessing()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>An abstract method that must be implemented by subclasses. This is useful for workers that have instance state that can not be shared across processes and should be cleared out and re-initialized for each new process.</p> Source code in <code>src/guidellm/scheduler/worker.py</code> <pre><code>@abstractmethod\nasync def prepare_multiprocessing(self):\n    \"\"\"\n    An abstract method that must be implemented by subclasses.\n    This is useful for workers that have instance state that can not\n    be shared across processes and should be cleared out and re-initialized\n    for each new process.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.RequestsWorker.resolve","title":"<code>resolve(request, timeout_time)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>An abstract method that must be implemented by subclasses. This method should handle the resolution of a request through asyncio, including any necessary backend processing and response handling.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>RequestT</code> <p>The request to be resolved generated by the load generator.</p> required <code>timeout_time</code> <code>float</code> <p>The timeout time for the request, if there is no timeout given, then this will be math.inf.</p> required <p>Returns:</p> Type Description <code>tuple[ResolveStatus, ResponseT]</code> <p>The response from the worker.</p> Source code in <code>src/guidellm/scheduler/worker.py</code> <pre><code>@abstractmethod\nasync def resolve(\n    self,\n    request: RequestT,\n    timeout_time: float,\n) -&gt; tuple[ResolveStatus, ResponseT]:\n    \"\"\"\n    An abstract method that must be implemented by subclasses.\n    This method should handle the resolution of a request through asyncio,\n    including any necessary backend processing and response handling.\n\n    :param request: The request to be resolved generated by the load generator.\n    :param timeout_time: The timeout time for the request, if there is no timeout\n        given, then this will be math.inf.\n    :return: The response from the worker.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.Scheduler","title":"<code>Scheduler</code>","text":"<p>               Bases: <code>Generic[RequestT, ResponseT]</code></p> <p>A class that handles the scheduling of requests to a worker. This class is responsible for managing the lifecycle of the requests, including their creation, queuing, and processing. It uses a multiprocessing approach to handle requests concurrently and efficiently, based on the specified scheduling strategy. The Scheduler class is designed to work with a RequestsWorker, which is an abstract base class that defines the interface for a worker that can resolve requests asynchronously or synchronously. The Scheduler class also supports different scheduling strategies, including synchronous, throughput, and concurrent strategies.</p> <p>Parameters:</p> Name Type Description Default <code>worker</code> <code>RequestsWorker[RequestT, ResponseT]</code> <p>The worker that will process the requests. This should be an instance of RequestsWorker.</p> required <code>request_loader</code> <code>Iterable[RequestT]</code> <p>An iterable that generates requests. This can be a list, generator, or any other iterable. The requests will be processed by the worker.</p> required Source code in <code>src/guidellm/scheduler/scheduler.py</code> <pre><code>class Scheduler(Generic[RequestT, ResponseT]):\n    \"\"\"\n    A class that handles the scheduling of requests to a worker.\n    This class is responsible for managing the lifecycle of the requests,\n    including their creation, queuing, and processing.\n    It uses a multiprocessing approach to handle requests concurrently\n    and efficiently, based on the specified scheduling strategy.\n    The Scheduler class is designed to work with a RequestsWorker,\n    which is an abstract base class that defines the interface for a worker\n    that can resolve requests asynchronously or synchronously.\n    The Scheduler class also supports different scheduling strategies,\n    including synchronous, throughput, and concurrent strategies.\n\n    :param worker: The worker that will process the requests.\n        This should be an instance of RequestsWorker.\n    :param request_loader: An iterable that generates requests.\n        This can be a list, generator, or any other iterable.\n        The requests will be processed by the worker.\n    \"\"\"\n\n    def __init__(\n        self,\n        worker: RequestsWorker[RequestT, ResponseT],\n        request_loader: Iterable[RequestT],\n    ):\n        if not isinstance(worker, RequestsWorker):\n            raise ValueError(f\"Invalid worker: {worker}\")\n\n        if not isinstance(request_loader, Iterable):\n            raise ValueError(f\"Invalid request_loader: {request_loader}\")\n\n        self.worker = worker\n        self.request_loader = request_loader\n\n    async def run(\n        self,\n        scheduling_strategy: SchedulingStrategy,\n        max_number: Optional[int] = None,\n        max_duration: Optional[float] = None,\n    ) -&gt; AsyncGenerator[\n        Union[SchedulerResult, SchedulerRequestResult[RequestT, ResponseT]], None\n    ]:\n        \"\"\"\n        The main method that runs the scheduler.\n        This method is a generator that yields SchedulerResult objects\n        at the start and end of the run, as well as at the start and end\n        of each request.\n        It uses multiprocessing to handle requests concurrently\n        and efficiently, based on the specified scheduling strategy.\n        The method also handles the lifecycle of the requests,\n        including their creation, queuing, and processing.\n        The method is designed to be used as an asynchronous generator,\n        allowing it to be used with asyncio and other asynchronous frameworks.\n\n        :param scheduling_strategy: The scheduling strategy to use.\n            Specifies the times at which requests will be sent as well how many\n            worker processes are used and if requests are scheduled sync or async.\n            This can be one of the following:\n            - \"synchronous\": Requests are sent synchronously.\n            - \"throughput\": Requests are sent at the maximum rate possible.\n            - An instance of SchedulingStrategy.\n        :param max_number: The maximum number of requests to process.\n            If None, then no limit is set and either the iterator must be exhaustible\n            or the max_duration must be set.\n        :param max_duration: The maximum duration for the scheduling run.\n            If None, then no limit is set and either the iterator must be exhaustible\n            or the max_number must be set.\n        :return: An asynchronous generator that yields SchedulerResult objects.\n            Each SchedulerResult object contains information about the request,\n            the response, and the run information.\n        \"\"\"\n        if scheduling_strategy is None or not isinstance(\n            scheduling_strategy, SchedulingStrategy\n        ):\n            raise ValueError(f\"Invalid scheduling strategy: {scheduling_strategy}\")\n\n        if max_number is not None and max_number &lt; 1:\n            raise ValueError(f\"Invalid max_number: {max_number}\")\n\n        if max_duration is not None and max_duration &lt; 0:\n            raise ValueError(f\"Invalid max_duration: {max_duration}\")\n\n        with (\n            multiprocessing.Manager() as manager,\n            ProcessPoolExecutor(\n                max_workers=scheduling_strategy.processes_limit\n            ) as executor,\n        ):\n            requests_iter: Optional[Iterator[Any]] = None\n            futures, requests_queue, responses_queue = await self._start_processes(\n                manager, executor, scheduling_strategy\n            )\n            run_info, requests_iter, times_iter = self._run_setup(\n                futures, scheduling_strategy, max_number, max_duration\n            )\n            yield SchedulerResult(\n                type_=\"run_start\",\n                run_info=run_info,\n            )\n\n            try:\n                while True:\n                    # check errors and raise them\n                    for future in futures:\n                        if future.done() and (err := future.exception()) is not None:\n                            raise err\n\n                    if (\n                        requests_iter is None\n                        and run_info.completed_requests &gt;= run_info.created_requests\n                    ):\n                        # we've exhausted all requests we've wanted to run\n                        # and yielded all responses\n                        break\n\n                    requests_iter = self._add_requests(\n                        requests_iter,\n                        times_iter,\n                        requests_queue,\n                        run_info,\n                    )\n                    await asyncio.sleep(0)  # enable requests to start\n\n                    iter_result = self._check_result_ready(\n                        responses_queue,\n                        run_info,\n                    )\n                    if iter_result is not None:\n                        yield iter_result\n\n                    # yield control to the event loop\n                    await asyncio.sleep(settings.default_async_loop_sleep)\n            except Exception as err:\n                raise RuntimeError(f\"Scheduler run failed: {err}\") from err\n\n            yield SchedulerResult(\n                type_=\"run_complete\",\n                run_info=run_info,\n            )\n\n            await self._stop_processes(futures, requests_queue)\n\n    async def _start_processes(\n        self,\n        manager,\n        executor: ProcessPoolExecutor,\n        scheduling_strategy: SchedulingStrategy,\n    ) -&gt; tuple[\n        list[asyncio.Future],\n        multiprocessing.Queue,\n        multiprocessing.Queue,\n    ]:\n        await self.worker.prepare_multiprocessing()\n        requests_queue = manager.Queue(\n            maxsize=scheduling_strategy.queued_requests_limit\n        )\n        responses_queue = manager.Queue()\n\n        num_processes = min(\n            scheduling_strategy.processes_limit,\n            scheduling_strategy.processing_requests_limit,\n        )\n        requests_limit_split = (\n            scheduling_strategy.processing_requests_limit\n            // scheduling_strategy.processes_limit\n        )\n        requests_limit_remain = (\n            scheduling_strategy.processing_requests_limit\n            % scheduling_strategy.processes_limit\n        )\n        process_ids = (id_ for id_ in range(num_processes))\n        process_requests_limits = (\n            requests_limit_split + 1\n            if i &lt; requests_limit_remain\n            else requests_limit_split\n            for i in range(num_processes)\n        )\n\n        futures = []\n        loop = asyncio.get_event_loop()\n        for id_, requests_limit in zip(process_ids, process_requests_limits):\n            if scheduling_strategy.processing_mode == \"sync\":\n                futures.append(\n                    loop.run_in_executor(\n                        executor,\n                        self.worker.process_loop_synchronous,\n                        requests_queue,\n                        responses_queue,\n                        id_,\n                    )\n                )\n            elif scheduling_strategy.processing_mode == \"async\":\n                futures.append(\n                    loop.run_in_executor(\n                        executor,\n                        self.worker.process_loop_asynchronous,\n                        requests_queue,\n                        responses_queue,\n                        requests_limit,\n                        id_,\n                    )\n                )\n            else:\n                raise ValueError(\n                    f\"Invalid processing mode: {scheduling_strategy.processing_mode} \"\n                    f\"for strategy: {scheduling_strategy}\"\n                )\n\n        await asyncio.sleep(0.1)  # give time for processes to start\n\n        return futures, requests_queue, responses_queue\n\n    def _run_setup(\n        self,\n        processes: list[asyncio.Future],\n        scheduling_strategy: SchedulingStrategy,\n        max_number: Optional[int],\n        max_duration: Optional[float],\n    ) -&gt; tuple[SchedulerRunInfo, Iterator[Any], Iterator[float]]:\n        requests_iter = iter(self.request_loader)\n        start_time = time.time()\n        times_iter = iter(scheduling_strategy.request_times())\n        end_time = time.time() + (max_duration or math.inf)\n        end_number = max_number or math.inf\n\n        try:\n            # update end number if the request loader is finite and less than max\n            iter_length = len(self.request_loader)  # type: ignore[arg-type]\n            if 0 &lt; iter_length &lt; end_number:\n                end_number = iter_length\n        except Exception:  # noqa: BLE001, S110\n            pass\n\n        if end_number == math.inf and end_time is None:\n            logger.warning(\n                \"No end number or end time set, \"\n                \"scheduler will run indefinitely until the request loader is exhausted.\"\n            )\n\n        info = SchedulerRunInfo(\n            start_time=start_time,\n            end_time=end_time,\n            end_number=end_number,\n            processes=len(processes),\n            strategy=scheduling_strategy,\n        )\n\n        return info, requests_iter, times_iter\n\n    def _add_requests(\n        self,\n        requests_iter: Optional[Iterator[Any]],\n        times_iter: Iterator[float],\n        requests_queue: multiprocessing.Queue,\n        run_info: SchedulerRunInfo,\n    ) -&gt; Optional[Iterator[Any]]:\n        if requests_iter is not None:\n            try:\n                added_count = 0\n\n                while (\n                    not requests_queue.full()\n                    and added_count &lt; settings.max_add_requests_per_loop\n                ):\n                    if run_info.created_requests &gt;= run_info.end_number:\n                        raise StopIteration\n\n                    if (\n                        request_time := next(times_iter)\n                    ) &gt;= run_info.end_time or time.time() &gt;= run_info.end_time:\n                        raise StopIteration\n\n                    request = next(requests_iter)\n                    work_req: WorkerProcessRequest[RequestT] = WorkerProcessRequest(\n                        request=request,\n                        start_time=request_time,\n                        timeout_time=run_info.end_time,\n                        queued_time=time.time(),\n                    )\n                    requests_queue.put(work_req)\n\n                    run_info.created_requests += 1\n                    run_info.queued_requests += 1\n                    added_count += 1\n            except StopIteration:\n                # we've reached the limit number, limit time, or exhausted the requests\n                # set to None to stop adding more and tell the loop no more requests\n                requests_iter = None\n\n        return requests_iter\n\n    def _check_result_ready(\n        self,\n        responses_queue: multiprocessing.Queue,\n        run_info: SchedulerRunInfo,\n    ) -&gt; Optional[SchedulerRequestResult[RequestT, ResponseT]]:\n        try:\n            process_response: WorkerProcessResult[RequestT, ResponseT] = (\n                responses_queue.get_nowait()\n            )\n        except multiprocessing.queues.Empty:  # type: ignore[attr-defined]\n            return None\n\n        if process_response.type_ == \"request_scheduled\":\n            run_info.queued_requests -= 1\n            run_info.scheduled_requests += 1\n\n            return SchedulerRequestResult(\n                type_=\"request_scheduled\",\n                run_info=run_info,\n                request=process_response.request,\n                request_info=process_response.info,\n                response=None,\n            )\n\n        if process_response.type_ == \"request_start\":\n            run_info.scheduled_requests -= 1\n            run_info.processing_requests += 1\n\n            return SchedulerRequestResult(\n                type_=\"request_start\",\n                run_info=run_info,\n                request=process_response.request,\n                request_info=process_response.info,\n                response=None,\n            )\n\n        if process_response.type_ == \"request_complete\":\n            run_info.processing_requests -= 1\n            run_info.completed_requests += 1\n\n            return SchedulerRequestResult(\n                type_=\"request_complete\",\n                run_info=run_info,\n                request=process_response.request,\n                request_info=process_response.info,\n                response=process_response.response,\n            )\n        raise ValueError(f\"Invalid process response type: {process_response}\")\n\n    async def _stop_processes(\n        self,\n        futures: list[asyncio.Future],\n        requests_queue: multiprocessing.Queue,\n    ):\n        for _ in futures:\n            requests_queue.put(None)\n\n        await asyncio.gather(*futures)\n</code></pre>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.Scheduler.run","title":"<code>run(scheduling_strategy, max_number=None, max_duration=None)</code>  <code>async</code>","text":"<p>The main method that runs the scheduler. This method is a generator that yields SchedulerResult objects at the start and end of the run, as well as at the start and end of each request. It uses multiprocessing to handle requests concurrently and efficiently, based on the specified scheduling strategy. The method also handles the lifecycle of the requests, including their creation, queuing, and processing. The method is designed to be used as an asynchronous generator, allowing it to be used with asyncio and other asynchronous frameworks.</p> <p>Parameters:</p> Name Type Description Default <code>scheduling_strategy</code> <code>SchedulingStrategy</code> <p>The scheduling strategy to use. Specifies the times at which requests will be sent as well how many worker processes are used and if requests are scheduled sync or async. This can be one of the following: - \"synchronous\": Requests are sent synchronously. - \"throughput\": Requests are sent at the maximum rate possible. - An instance of SchedulingStrategy.</p> required <code>max_number</code> <code>Optional[int]</code> <p>The maximum number of requests to process. If None, then no limit is set and either the iterator must be exhaustible or the max_duration must be set.</p> <code>None</code> <code>max_duration</code> <code>Optional[float]</code> <p>The maximum duration for the scheduling run. If None, then no limit is set and either the iterator must be exhaustible or the max_number must be set.</p> <code>None</code> <p>Returns:</p> Type Description <code>AsyncGenerator[Union[SchedulerResult, SchedulerRequestResult[RequestT, ResponseT]], None]</code> <p>An asynchronous generator that yields SchedulerResult objects. Each SchedulerResult object contains information about the request, the response, and the run information.</p> Source code in <code>src/guidellm/scheduler/scheduler.py</code> <pre><code>async def run(\n    self,\n    scheduling_strategy: SchedulingStrategy,\n    max_number: Optional[int] = None,\n    max_duration: Optional[float] = None,\n) -&gt; AsyncGenerator[\n    Union[SchedulerResult, SchedulerRequestResult[RequestT, ResponseT]], None\n]:\n    \"\"\"\n    The main method that runs the scheduler.\n    This method is a generator that yields SchedulerResult objects\n    at the start and end of the run, as well as at the start and end\n    of each request.\n    It uses multiprocessing to handle requests concurrently\n    and efficiently, based on the specified scheduling strategy.\n    The method also handles the lifecycle of the requests,\n    including their creation, queuing, and processing.\n    The method is designed to be used as an asynchronous generator,\n    allowing it to be used with asyncio and other asynchronous frameworks.\n\n    :param scheduling_strategy: The scheduling strategy to use.\n        Specifies the times at which requests will be sent as well how many\n        worker processes are used and if requests are scheduled sync or async.\n        This can be one of the following:\n        - \"synchronous\": Requests are sent synchronously.\n        - \"throughput\": Requests are sent at the maximum rate possible.\n        - An instance of SchedulingStrategy.\n    :param max_number: The maximum number of requests to process.\n        If None, then no limit is set and either the iterator must be exhaustible\n        or the max_duration must be set.\n    :param max_duration: The maximum duration for the scheduling run.\n        If None, then no limit is set and either the iterator must be exhaustible\n        or the max_number must be set.\n    :return: An asynchronous generator that yields SchedulerResult objects.\n        Each SchedulerResult object contains information about the request,\n        the response, and the run information.\n    \"\"\"\n    if scheduling_strategy is None or not isinstance(\n        scheduling_strategy, SchedulingStrategy\n    ):\n        raise ValueError(f\"Invalid scheduling strategy: {scheduling_strategy}\")\n\n    if max_number is not None and max_number &lt; 1:\n        raise ValueError(f\"Invalid max_number: {max_number}\")\n\n    if max_duration is not None and max_duration &lt; 0:\n        raise ValueError(f\"Invalid max_duration: {max_duration}\")\n\n    with (\n        multiprocessing.Manager() as manager,\n        ProcessPoolExecutor(\n            max_workers=scheduling_strategy.processes_limit\n        ) as executor,\n    ):\n        requests_iter: Optional[Iterator[Any]] = None\n        futures, requests_queue, responses_queue = await self._start_processes(\n            manager, executor, scheduling_strategy\n        )\n        run_info, requests_iter, times_iter = self._run_setup(\n            futures, scheduling_strategy, max_number, max_duration\n        )\n        yield SchedulerResult(\n            type_=\"run_start\",\n            run_info=run_info,\n        )\n\n        try:\n            while True:\n                # check errors and raise them\n                for future in futures:\n                    if future.done() and (err := future.exception()) is not None:\n                        raise err\n\n                if (\n                    requests_iter is None\n                    and run_info.completed_requests &gt;= run_info.created_requests\n                ):\n                    # we've exhausted all requests we've wanted to run\n                    # and yielded all responses\n                    break\n\n                requests_iter = self._add_requests(\n                    requests_iter,\n                    times_iter,\n                    requests_queue,\n                    run_info,\n                )\n                await asyncio.sleep(0)  # enable requests to start\n\n                iter_result = self._check_result_ready(\n                    responses_queue,\n                    run_info,\n                )\n                if iter_result is not None:\n                    yield iter_result\n\n                # yield control to the event loop\n                await asyncio.sleep(settings.default_async_loop_sleep)\n        except Exception as err:\n            raise RuntimeError(f\"Scheduler run failed: {err}\") from err\n\n        yield SchedulerResult(\n            type_=\"run_complete\",\n            run_info=run_info,\n        )\n\n        await self._stop_processes(futures, requests_queue)\n</code></pre>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.SchedulerRequestInfo","title":"<code>SchedulerRequestInfo</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>Information about a specific request run through the scheduler. This class holds metadata about the request, including the targeted start time, queued time, start time, end time, and the process ID that handled the request.</p> <p>Parameters:</p> Name Type Description Default <code>targeted_start_time</code> <p>The targeted start time for the request (time.time()).</p> required <code>queued_time</code> <p>The time the request was queued (time.time()).</p> required <code>scheduled_time</code> <p>The time the request was scheduled (time.time()) (any sleep time before the request was sent to the worker).</p> required <code>worker_start</code> <p>The time the worker started processing request (time.time()).</p> required <code>worker_end</code> <p>The time the worker finished processing request. (time.time()).</p> required <code>process_id</code> <p>The ID of the underlying process that handled the request.</p> required Source code in <code>src/guidellm/scheduler/result.py</code> <pre><code>class SchedulerRequestInfo(StandardBaseModel):\n    \"\"\"\n    Information about a specific request run through the scheduler.\n    This class holds metadata about the request, including\n    the targeted start time, queued time, start time, end time,\n    and the process ID that handled the request.\n\n    :param targeted_start_time: The targeted start time for the request (time.time()).\n    :param queued_time: The time the request was queued (time.time()).\n    :param scheduled_time: The time the request was scheduled (time.time())\n        (any sleep time before the request was sent to the worker).\n    :param worker_start: The time the worker started processing request (time.time()).\n    :param worker_end: The time the worker finished processing request. (time.time()).\n    :param process_id: The ID of the underlying process that handled the request.\n    \"\"\"\n\n    requested: bool = False\n    completed: bool = False\n    errored: bool = False\n    canceled: bool = False\n\n    targeted_start_time: float = -1\n    queued_time: float = -1\n    dequeued_time: float = -1\n    scheduled_time: float = -1\n    worker_start: float = -1\n    request_start: float = -1\n    request_end: float = -1\n    worker_end: float = -1\n    process_id: int = -1\n</code></pre>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.SchedulerResult","title":"<code>SchedulerResult</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>The yielded, iterative result for a scheduler run. These are triggered on the start and end of the run, as well as on the start and end of each request. Depending on the type, it will hold the request and response along with information and statistics about the request and general run.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <p>The type of the result, which can be one of: - \"run_start\": Indicates the start of the run. - \"run_complete\": Indicates the completion of the run (teardown happens after). - \"request_start\": Indicates the start of a request. - \"request_complete\": Indicates the completion of a request.</p> required <code>request</code> <p>The request that was processed.</p> required <code>response</code> <p>The response from the worker for the request.</p> required <code>request_info</code> <p>Information about the request, including the targeted start time, queued time, start time, end time, and the process ID that handled the request.</p> required <code>run_info</code> <p>Information about the current run of the scheduler, including the start and end times, the number of processes, and the scheduling strategy used. It also tracks the number of requests created, queued, pending, and completed during the run.</p> required Source code in <code>src/guidellm/scheduler/result.py</code> <pre><code>class SchedulerResult(StandardBaseModel):\n    \"\"\"\n    The yielded, iterative result for a scheduler run.\n    These are triggered on the start and end of the run,\n    as well as on the start and end of each request.\n    Depending on the type, it will hold the request and response\n    along with information and statistics about the request and general run.\n\n    :param type_: The type of the result, which can be one of:\n        - \"run_start\": Indicates the start of the run.\n        - \"run_complete\": Indicates the completion of the run (teardown happens after).\n        - \"request_start\": Indicates the start of a request.\n        - \"request_complete\": Indicates the completion of a request.\n    :param request: The request that was processed.\n    :param response: The response from the worker for the request.\n    :param request_info: Information about the request, including\n        the targeted start time, queued time, start time, end time,\n        and the process ID that handled the request.\n    :param run_info: Information about the current run of the scheduler,\n        including the start and end times, the number of processes,\n        and the scheduling strategy used.\n        It also tracks the number of requests created, queued, pending,\n        and completed during the run.\n    \"\"\"\n\n    pydantic_type: Literal[\"scheduler_result\"] = \"scheduler_result\"\n    type_: Literal[\n        \"run_start\",\n        \"run_complete\",\n        \"request_scheduled\",\n        \"request_start\",\n        \"request_complete\",\n    ]\n    run_info: SchedulerRunInfo\n</code></pre>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.SchedulerRunInfo","title":"<code>SchedulerRunInfo</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>Information about the current run of the scheduler. This class holds metadata about the scheduling run, including the start and end times, the number of processes, and the scheduling strategy used. It also tracks the number of requests created, queued, pending, and completed during the run.</p> <p>Parameters:</p> Name Type Description Default <code>start_time</code> <p>The start time of the scheduling run.</p> required <code>end_time</code> <p>The end time of the scheduling run; if None, then this will be math.inf.</p> required <code>end_number</code> <p>The maximum number of requests to be processed; if None, then this will be math.inf.</p> required <code>processes</code> <p>The number of processes used in the scheduling run.</p> required <code>strategy</code> <p>The scheduling strategy used in the run. This should be an instance of SchedulingStrategy.</p> required <code>created_requests</code> <p>The number of requests created during the run.</p> required <code>queued_requests</code> <p>The number of requests queued during the run.</p> required <code>scheduled_requests</code> <p>The number of requests scheduled during the run. (requests pending being sent to the worker but recieved by a process)</p> required <code>processing_requests</code> <p>The number of requests actively being run.</p> required <code>completed_requests</code> <p>The number of requests completed during the run.</p> required Source code in <code>src/guidellm/scheduler/result.py</code> <pre><code>class SchedulerRunInfo(StandardBaseModel):\n    \"\"\"\n    Information about the current run of the scheduler.\n    This class holds metadata about the scheduling run,\n    including the start and end times, the number of processes,\n    and the scheduling strategy used.\n    It also tracks the number of requests created, queued, pending,\n    and completed during the run.\n\n    :param start_time: The start time of the scheduling run.\n    :param end_time: The end time of the scheduling run;\n        if None, then this will be math.inf.\n    :param end_number: The maximum number of requests to be processed;\n        if None, then this will be math.inf.\n    :param processes: The number of processes used in the scheduling run.\n    :param strategy: The scheduling strategy used in the run.\n        This should be an instance of SchedulingStrategy.\n    :param created_requests: The number of requests created during the run.\n    :param queued_requests: The number of requests queued during the run.\n    :param scheduled_requests: The number of requests scheduled during the run.\n        (requests pending being sent to the worker but recieved by a process)\n    :param processing_requests: The number of requests actively being run.\n    :param completed_requests: The number of requests completed during the run.\n    \"\"\"\n\n    start_time: float\n    end_time: float\n    end_number: float\n    processes: int\n    strategy: SchedulingStrategy\n\n    created_requests: int = 0\n    queued_requests: int = 0\n    scheduled_requests: int = 0\n    processing_requests: int = 0\n    completed_requests: int = 0\n</code></pre>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.SchedulingStrategy","title":"<code>SchedulingStrategy</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>An abstract base class for scheduling strategies. This class defines the interface for scheduling requests and provides a common structure for all scheduling strategies. Subclasses should implement the <code>request_times</code> method to provide specific scheduling behavior.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <p>The type of scheduling strategy to use. This should be one of the predefined strategy types.</p> required Source code in <code>src/guidellm/scheduler/strategy.py</code> <pre><code>class SchedulingStrategy(StandardBaseModel):\n    \"\"\"\n    An abstract base class for scheduling strategies.\n    This class defines the interface for scheduling requests and provides\n    a common structure for all scheduling strategies.\n    Subclasses should implement the `request_times` method to provide\n    specific scheduling behavior.\n\n    :param type_: The type of scheduling strategy to use.\n        This should be one of the predefined strategy types.\n    \"\"\"\n\n    type_: Literal[\"strategy\"] = Field(\n        description=\"The type of scheduling strategy schedule requests with.\",\n    )\n\n    @property\n    def processing_mode(self) -&gt; Literal[\"sync\", \"async\"]:\n        \"\"\"\n        The processing mode for the scheduling strategy, either 'sync' or 'async'.\n        This property determines how the worker processes are setup:\n        either to run synchronously with one request at a time or asynchronously.\n        This property should be implemented by subclasses to return\n        the appropriate processing mode.\n\n        :return: The processing mode for the scheduling strategy,\n            either 'sync' or 'async'.\n        \"\"\"\n        return \"async\"\n\n    @property\n    def processes_limit(self) -&gt; int:\n        \"\"\"\n        The limit on the number of worker processes for the scheduling strategy.\n        It determines how many worker processes are created\n        for the scheduling strategy and must be implemented by subclasses.\n\n        :return: The number of processes for the scheduling strategy.\n        \"\"\"\n        cpu_cores = os.cpu_count() or 1\n\n        return min(max(1, cpu_cores - 1), settings.max_worker_processes)\n\n    @property\n    def queued_requests_limit(self) -&gt; Optional[int]:\n        \"\"\"\n        The maximum number of queued requests for the scheduling strategy.\n        It determines how many requests can be queued at one time\n        for the scheduling strategy and must be implemented by subclasses.\n\n        :return: The maximum number of queued requests for the scheduling strategy.\n        \"\"\"\n        return settings.max_concurrency\n\n    @property\n    def processing_requests_limit(self) -&gt; int:\n        \"\"\"\n        The maximum number of processing requests for the scheduling strategy.\n        It determines how many requests can be processed at one time\n        for the scheduling strategy and must be implemented by subclasses.\n\n        :return: The maximum number of processing requests for the scheduling strategy.\n        \"\"\"\n        return settings.max_concurrency\n\n    def request_times(self) -&gt; Generator[float, None, None]:\n        \"\"\"\n        A generator that yields timestamps for when requests should be sent.\n        This method should be implemented by subclasses to provide specific\n        scheduling behavior.\n\n        :return: A generator that yields timestamps for request scheduling\n            or -1 for requests that should be sent immediately.\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement request_times() method.\")\n</code></pre>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.SchedulingStrategy.processes_limit","title":"<code>processes_limit</code>  <code>property</code>","text":"<p>The limit on the number of worker processes for the scheduling strategy. It determines how many worker processes are created for the scheduling strategy and must be implemented by subclasses.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of processes for the scheduling strategy.</p>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.SchedulingStrategy.processing_mode","title":"<code>processing_mode</code>  <code>property</code>","text":"<p>The processing mode for the scheduling strategy, either 'sync' or 'async'. This property determines how the worker processes are setup: either to run synchronously with one request at a time or asynchronously. This property should be implemented by subclasses to return the appropriate processing mode.</p> <p>Returns:</p> Type Description <code>Literal['sync', 'async']</code> <p>The processing mode for the scheduling strategy, either 'sync' or 'async'.</p>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.SchedulingStrategy.processing_requests_limit","title":"<code>processing_requests_limit</code>  <code>property</code>","text":"<p>The maximum number of processing requests for the scheduling strategy. It determines how many requests can be processed at one time for the scheduling strategy and must be implemented by subclasses.</p> <p>Returns:</p> Type Description <code>int</code> <p>The maximum number of processing requests for the scheduling strategy.</p>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.SchedulingStrategy.queued_requests_limit","title":"<code>queued_requests_limit</code>  <code>property</code>","text":"<p>The maximum number of queued requests for the scheduling strategy. It determines how many requests can be queued at one time for the scheduling strategy and must be implemented by subclasses.</p> <p>Returns:</p> Type Description <code>Optional[int]</code> <p>The maximum number of queued requests for the scheduling strategy.</p>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.SchedulingStrategy.request_times","title":"<code>request_times()</code>","text":"<p>A generator that yields timestamps for when requests should be sent. This method should be implemented by subclasses to provide specific scheduling behavior.</p> <p>Returns:</p> Type Description <code>Generator[float, None, None]</code> <p>A generator that yields timestamps for request scheduling or -1 for requests that should be sent immediately.</p> Source code in <code>src/guidellm/scheduler/strategy.py</code> <pre><code>def request_times(self) -&gt; Generator[float, None, None]:\n    \"\"\"\n    A generator that yields timestamps for when requests should be sent.\n    This method should be implemented by subclasses to provide specific\n    scheduling behavior.\n\n    :return: A generator that yields timestamps for request scheduling\n        or -1 for requests that should be sent immediately.\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement request_times() method.\")\n</code></pre>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.SynchronousStrategy","title":"<code>SynchronousStrategy</code>","text":"<p>               Bases: <code>SchedulingStrategy</code></p> <p>A class representing a synchronous scheduling strategy. This strategy schedules requests synchronously, one at a time, with the maximum rate possible. It inherits from the <code>SchedulingStrategy</code> base class and implements the <code>request_times</code> method to provide the specific behavior for synchronous scheduling.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <p>The synchronous StrategyType to schedule requests synchronously.</p> required Source code in <code>src/guidellm/scheduler/strategy.py</code> <pre><code>class SynchronousStrategy(SchedulingStrategy):\n    \"\"\"\n    A class representing a synchronous scheduling strategy.\n    This strategy schedules requests synchronously, one at a time,\n    with the maximum rate possible.\n    It inherits from the `SchedulingStrategy` base class and\n    implements the `request_times` method to provide the specific\n    behavior for synchronous scheduling.\n\n    :param type_: The synchronous StrategyType to schedule requests synchronously.\n    \"\"\"\n\n    type_: Literal[\"synchronous\"] = \"synchronous\"  # type: ignore[assignment]\n\n    @property\n    def processing_mode(self) -&gt; Literal[\"sync\"]:\n        \"\"\"\n        The processing mode for the scheduling strategy, either 'sync' or 'async'.\n        This property determines how the worker processes are setup:\n        either to run synchronously with one request at a time or asynchronously.\n\n        :return: 'sync' for synchronous scheduling strategy\n            for the single worker process.\n        \"\"\"\n        return \"sync\"\n\n    @property\n    def processes_limit(self) -&gt; int:\n        \"\"\"\n        The limit on the number of worker processes for the scheduling strategy.\n        It determines how many worker processes are created\n        for the scheduling strategy and must be implemented by subclasses.\n\n        :return: 1 for the synchronous scheduling strategy to limit\n            the worker processes to one.\n        \"\"\"\n        return 1\n\n    @property\n    def queued_requests_limit(self) -&gt; int:\n        \"\"\"\n        The maximum number of queued requests for the scheduling strategy.\n        It determines how many requests can be queued at one time\n        for the scheduling strategy and must be implemented by subclasses.\n\n        :return: 1 for the synchronous scheduling strategy to limit\n            the queued requests to one that is ready to be processed.\n        \"\"\"\n        return 1\n\n    @property\n    def processing_requests_limit(self) -&gt; int:\n        \"\"\"\n        The maximum number of processing requests for the scheduling strategy.\n        It determines how many requests can be processed at one time\n        for the scheduling strategy and must be implemented by subclasses.\n\n        :return: 1 for the synchronous scheduling strategy to limit\n            the processing requests to one that is ready to be processed.\n        \"\"\"\n        return 1\n\n    def request_times(self) -&gt; Generator[float, None, None]:\n        \"\"\"\n        A generator that yields time.time() so requests are sent immediately,\n            while scheduling them synchronously.\n\n        :return: A generator that yields time.time() for immediate request scheduling.\n        \"\"\"\n        while True:\n            yield time.time()\n</code></pre>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.SynchronousStrategy.processes_limit","title":"<code>processes_limit</code>  <code>property</code>","text":"<p>The limit on the number of worker processes for the scheduling strategy. It determines how many worker processes are created for the scheduling strategy and must be implemented by subclasses.</p> <p>Returns:</p> Type Description <code>int</code> <p>1 for the synchronous scheduling strategy to limit the worker processes to one.</p>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.SynchronousStrategy.processing_mode","title":"<code>processing_mode</code>  <code>property</code>","text":"<p>The processing mode for the scheduling strategy, either 'sync' or 'async'. This property determines how the worker processes are setup: either to run synchronously with one request at a time or asynchronously.</p> <p>Returns:</p> Type Description <code>Literal['sync']</code> <p>'sync' for synchronous scheduling strategy for the single worker process.</p>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.SynchronousStrategy.processing_requests_limit","title":"<code>processing_requests_limit</code>  <code>property</code>","text":"<p>The maximum number of processing requests for the scheduling strategy. It determines how many requests can be processed at one time for the scheduling strategy and must be implemented by subclasses.</p> <p>Returns:</p> Type Description <code>int</code> <p>1 for the synchronous scheduling strategy to limit the processing requests to one that is ready to be processed.</p>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.SynchronousStrategy.queued_requests_limit","title":"<code>queued_requests_limit</code>  <code>property</code>","text":"<p>The maximum number of queued requests for the scheduling strategy. It determines how many requests can be queued at one time for the scheduling strategy and must be implemented by subclasses.</p> <p>Returns:</p> Type Description <code>int</code> <p>1 for the synchronous scheduling strategy to limit the queued requests to one that is ready to be processed.</p>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.SynchronousStrategy.request_times","title":"<code>request_times()</code>","text":"<p>A generator that yields time.time() so requests are sent immediately,     while scheduling them synchronously.</p> <p>Returns:</p> Type Description <code>Generator[float, None, None]</code> <p>A generator that yields time.time() for immediate request scheduling.</p> Source code in <code>src/guidellm/scheduler/strategy.py</code> <pre><code>def request_times(self) -&gt; Generator[float, None, None]:\n    \"\"\"\n    A generator that yields time.time() so requests are sent immediately,\n        while scheduling them synchronously.\n\n    :return: A generator that yields time.time() for immediate request scheduling.\n    \"\"\"\n    while True:\n        yield time.time()\n</code></pre>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.ThroughputStrategy","title":"<code>ThroughputStrategy</code>","text":"<p>               Bases: <code>SchedulingStrategy</code></p> <p>A class representing a throughput scheduling strategy. This strategy schedules as many requests asynchronously as possible, with the maximum rate possible. It inherits from the <code>SchedulingStrategy</code> base class and implements the <code>request_times</code> method to provide the specific behavior for throughput scheduling.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <p>The throughput StrategyType to schedule requests asynchronously.</p> required Source code in <code>src/guidellm/scheduler/strategy.py</code> <pre><code>class ThroughputStrategy(SchedulingStrategy):\n    \"\"\"\n    A class representing a throughput scheduling strategy.\n    This strategy schedules as many requests asynchronously as possible,\n    with the maximum rate possible.\n    It inherits from the `SchedulingStrategy` base class and\n    implements the `request_times` method to provide the specific\n    behavior for throughput scheduling.\n\n    :param type_: The throughput StrategyType to schedule requests asynchronously.\n    \"\"\"\n\n    type_: Literal[\"throughput\"] = \"throughput\"  # type: ignore[assignment]\n    max_concurrency: Optional[int] = Field(\n        default=None,\n        description=(\n            \"The maximum number of concurrent requests to schedule. \"\n            \"If set to None, the concurrency value from settings will be used. \"\n            \"This must be a positive integer greater than 0.\"\n        ),\n        gt=0,\n    )\n\n    @property\n    def processing_mode(self) -&gt; Literal[\"async\"]:\n        \"\"\"\n        The processing mode for the scheduling strategy, either 'sync' or 'async'.\n        This property determines how the worker processes are setup:\n        either to run synchronously with one request at a time or asynchronously.\n\n        :return: 'async' for asynchronous scheduling strategy\n            for the multiple worker processes handling requests.\n        \"\"\"\n        return \"async\"\n\n    @property\n    def queued_requests_limit(self) -&gt; int:\n        \"\"\"\n        The maximum number of queued requests for the scheduling strategy.\n        It determines how many requests can be queued at one time\n        for the scheduling strategy and must be implemented by subclasses.\n\n        :return: The processing requests limit to ensure that there are enough\n            requests even for the worst case scenario where the max concurrent\n            requests are pulled at once for processing.\n        \"\"\"\n        return self.processing_requests_limit\n\n    @property\n    def processing_requests_limit(self) -&gt; int:\n        \"\"\"\n        The maximum number of processing requests for the scheduling strategy.\n        It determines how many requests can be processed at one time\n        for the scheduling strategy and must be implemented by subclasses.\n\n        :return: {self.max_concurrency} for the throughput scheduling strategy to limit\n            the processing requests to the maximum concurrency.\n            If max_concurrency is None, then the default processing requests limit\n            will be used.\n        \"\"\"\n        return self.max_concurrency or super().processing_requests_limit\n\n    def request_times(self) -&gt; Generator[float, None, None]:\n        \"\"\"\n        A generator that yields the start time.time() so requests are sent\n        immediately, while scheduling as many asynchronously as possible.\n\n        :return: A generator that yields the start time.time()\n            for immediate request scheduling.\n        \"\"\"\n        start_time = time.time()\n\n        while True:\n            yield start_time\n</code></pre>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.ThroughputStrategy.processing_mode","title":"<code>processing_mode</code>  <code>property</code>","text":"<p>The processing mode for the scheduling strategy, either 'sync' or 'async'. This property determines how the worker processes are setup: either to run synchronously with one request at a time or asynchronously.</p> <p>Returns:</p> Type Description <code>Literal['async']</code> <p>'async' for asynchronous scheduling strategy for the multiple worker processes handling requests.</p>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.ThroughputStrategy.processing_requests_limit","title":"<code>processing_requests_limit</code>  <code>property</code>","text":"<p>The maximum number of processing requests for the scheduling strategy. It determines how many requests can be processed at one time for the scheduling strategy and must be implemented by subclasses.</p> <p>Returns:</p> Type Description <code>int</code> <p>{self.max_concurrency} for the throughput scheduling strategy to limit the processing requests to the maximum concurrency. If max_concurrency is None, then the default processing requests limit will be used.</p>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.ThroughputStrategy.queued_requests_limit","title":"<code>queued_requests_limit</code>  <code>property</code>","text":"<p>The maximum number of queued requests for the scheduling strategy. It determines how many requests can be queued at one time for the scheduling strategy and must be implemented by subclasses.</p> <p>Returns:</p> Type Description <code>int</code> <p>The processing requests limit to ensure that there are enough requests even for the worst case scenario where the max concurrent requests are pulled at once for processing.</p>"},{"location":"reference/guidellm/scheduler/#guidellm.scheduler.ThroughputStrategy.request_times","title":"<code>request_times()</code>","text":"<p>A generator that yields the start time.time() so requests are sent immediately, while scheduling as many asynchronously as possible.</p> <p>Returns:</p> Type Description <code>Generator[float, None, None]</code> <p>A generator that yields the start time.time() for immediate request scheduling.</p> Source code in <code>src/guidellm/scheduler/strategy.py</code> <pre><code>def request_times(self) -&gt; Generator[float, None, None]:\n    \"\"\"\n    A generator that yields the start time.time() so requests are sent\n    immediately, while scheduling as many asynchronously as possible.\n\n    :return: A generator that yields the start time.time()\n        for immediate request scheduling.\n    \"\"\"\n    start_time = time.time()\n\n    while True:\n        yield start_time\n</code></pre>"},{"location":"reference/guidellm/scheduler/result/","title":"guidellm.scheduler.result","text":""},{"location":"reference/guidellm/scheduler/result/#guidellm.scheduler.result.SchedulerRequestInfo","title":"<code>SchedulerRequestInfo</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>Information about a specific request run through the scheduler. This class holds metadata about the request, including the targeted start time, queued time, start time, end time, and the process ID that handled the request.</p> <p>Parameters:</p> Name Type Description Default <code>targeted_start_time</code> <p>The targeted start time for the request (time.time()).</p> required <code>queued_time</code> <p>The time the request was queued (time.time()).</p> required <code>scheduled_time</code> <p>The time the request was scheduled (time.time()) (any sleep time before the request was sent to the worker).</p> required <code>worker_start</code> <p>The time the worker started processing request (time.time()).</p> required <code>worker_end</code> <p>The time the worker finished processing request. (time.time()).</p> required <code>process_id</code> <p>The ID of the underlying process that handled the request.</p> required Source code in <code>src/guidellm/scheduler/result.py</code> <pre><code>class SchedulerRequestInfo(StandardBaseModel):\n    \"\"\"\n    Information about a specific request run through the scheduler.\n    This class holds metadata about the request, including\n    the targeted start time, queued time, start time, end time,\n    and the process ID that handled the request.\n\n    :param targeted_start_time: The targeted start time for the request (time.time()).\n    :param queued_time: The time the request was queued (time.time()).\n    :param scheduled_time: The time the request was scheduled (time.time())\n        (any sleep time before the request was sent to the worker).\n    :param worker_start: The time the worker started processing request (time.time()).\n    :param worker_end: The time the worker finished processing request. (time.time()).\n    :param process_id: The ID of the underlying process that handled the request.\n    \"\"\"\n\n    requested: bool = False\n    completed: bool = False\n    errored: bool = False\n    canceled: bool = False\n\n    targeted_start_time: float = -1\n    queued_time: float = -1\n    dequeued_time: float = -1\n    scheduled_time: float = -1\n    worker_start: float = -1\n    request_start: float = -1\n    request_end: float = -1\n    worker_end: float = -1\n    process_id: int = -1\n</code></pre>"},{"location":"reference/guidellm/scheduler/result/#guidellm.scheduler.result.SchedulerResult","title":"<code>SchedulerResult</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>The yielded, iterative result for a scheduler run. These are triggered on the start and end of the run, as well as on the start and end of each request. Depending on the type, it will hold the request and response along with information and statistics about the request and general run.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <p>The type of the result, which can be one of: - \"run_start\": Indicates the start of the run. - \"run_complete\": Indicates the completion of the run (teardown happens after). - \"request_start\": Indicates the start of a request. - \"request_complete\": Indicates the completion of a request.</p> required <code>request</code> <p>The request that was processed.</p> required <code>response</code> <p>The response from the worker for the request.</p> required <code>request_info</code> <p>Information about the request, including the targeted start time, queued time, start time, end time, and the process ID that handled the request.</p> required <code>run_info</code> <p>Information about the current run of the scheduler, including the start and end times, the number of processes, and the scheduling strategy used. It also tracks the number of requests created, queued, pending, and completed during the run.</p> required Source code in <code>src/guidellm/scheduler/result.py</code> <pre><code>class SchedulerResult(StandardBaseModel):\n    \"\"\"\n    The yielded, iterative result for a scheduler run.\n    These are triggered on the start and end of the run,\n    as well as on the start and end of each request.\n    Depending on the type, it will hold the request and response\n    along with information and statistics about the request and general run.\n\n    :param type_: The type of the result, which can be one of:\n        - \"run_start\": Indicates the start of the run.\n        - \"run_complete\": Indicates the completion of the run (teardown happens after).\n        - \"request_start\": Indicates the start of a request.\n        - \"request_complete\": Indicates the completion of a request.\n    :param request: The request that was processed.\n    :param response: The response from the worker for the request.\n    :param request_info: Information about the request, including\n        the targeted start time, queued time, start time, end time,\n        and the process ID that handled the request.\n    :param run_info: Information about the current run of the scheduler,\n        including the start and end times, the number of processes,\n        and the scheduling strategy used.\n        It also tracks the number of requests created, queued, pending,\n        and completed during the run.\n    \"\"\"\n\n    pydantic_type: Literal[\"scheduler_result\"] = \"scheduler_result\"\n    type_: Literal[\n        \"run_start\",\n        \"run_complete\",\n        \"request_scheduled\",\n        \"request_start\",\n        \"request_complete\",\n    ]\n    run_info: SchedulerRunInfo\n</code></pre>"},{"location":"reference/guidellm/scheduler/result/#guidellm.scheduler.result.SchedulerRunInfo","title":"<code>SchedulerRunInfo</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>Information about the current run of the scheduler. This class holds metadata about the scheduling run, including the start and end times, the number of processes, and the scheduling strategy used. It also tracks the number of requests created, queued, pending, and completed during the run.</p> <p>Parameters:</p> Name Type Description Default <code>start_time</code> <p>The start time of the scheduling run.</p> required <code>end_time</code> <p>The end time of the scheduling run; if None, then this will be math.inf.</p> required <code>end_number</code> <p>The maximum number of requests to be processed; if None, then this will be math.inf.</p> required <code>processes</code> <p>The number of processes used in the scheduling run.</p> required <code>strategy</code> <p>The scheduling strategy used in the run. This should be an instance of SchedulingStrategy.</p> required <code>created_requests</code> <p>The number of requests created during the run.</p> required <code>queued_requests</code> <p>The number of requests queued during the run.</p> required <code>scheduled_requests</code> <p>The number of requests scheduled during the run. (requests pending being sent to the worker but recieved by a process)</p> required <code>processing_requests</code> <p>The number of requests actively being run.</p> required <code>completed_requests</code> <p>The number of requests completed during the run.</p> required Source code in <code>src/guidellm/scheduler/result.py</code> <pre><code>class SchedulerRunInfo(StandardBaseModel):\n    \"\"\"\n    Information about the current run of the scheduler.\n    This class holds metadata about the scheduling run,\n    including the start and end times, the number of processes,\n    and the scheduling strategy used.\n    It also tracks the number of requests created, queued, pending,\n    and completed during the run.\n\n    :param start_time: The start time of the scheduling run.\n    :param end_time: The end time of the scheduling run;\n        if None, then this will be math.inf.\n    :param end_number: The maximum number of requests to be processed;\n        if None, then this will be math.inf.\n    :param processes: The number of processes used in the scheduling run.\n    :param strategy: The scheduling strategy used in the run.\n        This should be an instance of SchedulingStrategy.\n    :param created_requests: The number of requests created during the run.\n    :param queued_requests: The number of requests queued during the run.\n    :param scheduled_requests: The number of requests scheduled during the run.\n        (requests pending being sent to the worker but recieved by a process)\n    :param processing_requests: The number of requests actively being run.\n    :param completed_requests: The number of requests completed during the run.\n    \"\"\"\n\n    start_time: float\n    end_time: float\n    end_number: float\n    processes: int\n    strategy: SchedulingStrategy\n\n    created_requests: int = 0\n    queued_requests: int = 0\n    scheduled_requests: int = 0\n    processing_requests: int = 0\n    completed_requests: int = 0\n</code></pre>"},{"location":"reference/guidellm/scheduler/scheduler/","title":"guidellm.scheduler.scheduler","text":""},{"location":"reference/guidellm/scheduler/scheduler/#guidellm.scheduler.scheduler.Scheduler","title":"<code>Scheduler</code>","text":"<p>               Bases: <code>Generic[RequestT, ResponseT]</code></p> <p>A class that handles the scheduling of requests to a worker. This class is responsible for managing the lifecycle of the requests, including their creation, queuing, and processing. It uses a multiprocessing approach to handle requests concurrently and efficiently, based on the specified scheduling strategy. The Scheduler class is designed to work with a RequestsWorker, which is an abstract base class that defines the interface for a worker that can resolve requests asynchronously or synchronously. The Scheduler class also supports different scheduling strategies, including synchronous, throughput, and concurrent strategies.</p> <p>Parameters:</p> Name Type Description Default <code>worker</code> <code>RequestsWorker[RequestT, ResponseT]</code> <p>The worker that will process the requests. This should be an instance of RequestsWorker.</p> required <code>request_loader</code> <code>Iterable[RequestT]</code> <p>An iterable that generates requests. This can be a list, generator, or any other iterable. The requests will be processed by the worker.</p> required Source code in <code>src/guidellm/scheduler/scheduler.py</code> <pre><code>class Scheduler(Generic[RequestT, ResponseT]):\n    \"\"\"\n    A class that handles the scheduling of requests to a worker.\n    This class is responsible for managing the lifecycle of the requests,\n    including their creation, queuing, and processing.\n    It uses a multiprocessing approach to handle requests concurrently\n    and efficiently, based on the specified scheduling strategy.\n    The Scheduler class is designed to work with a RequestsWorker,\n    which is an abstract base class that defines the interface for a worker\n    that can resolve requests asynchronously or synchronously.\n    The Scheduler class also supports different scheduling strategies,\n    including synchronous, throughput, and concurrent strategies.\n\n    :param worker: The worker that will process the requests.\n        This should be an instance of RequestsWorker.\n    :param request_loader: An iterable that generates requests.\n        This can be a list, generator, or any other iterable.\n        The requests will be processed by the worker.\n    \"\"\"\n\n    def __init__(\n        self,\n        worker: RequestsWorker[RequestT, ResponseT],\n        request_loader: Iterable[RequestT],\n    ):\n        if not isinstance(worker, RequestsWorker):\n            raise ValueError(f\"Invalid worker: {worker}\")\n\n        if not isinstance(request_loader, Iterable):\n            raise ValueError(f\"Invalid request_loader: {request_loader}\")\n\n        self.worker = worker\n        self.request_loader = request_loader\n\n    async def run(\n        self,\n        scheduling_strategy: SchedulingStrategy,\n        max_number: Optional[int] = None,\n        max_duration: Optional[float] = None,\n    ) -&gt; AsyncGenerator[\n        Union[SchedulerResult, SchedulerRequestResult[RequestT, ResponseT]], None\n    ]:\n        \"\"\"\n        The main method that runs the scheduler.\n        This method is a generator that yields SchedulerResult objects\n        at the start and end of the run, as well as at the start and end\n        of each request.\n        It uses multiprocessing to handle requests concurrently\n        and efficiently, based on the specified scheduling strategy.\n        The method also handles the lifecycle of the requests,\n        including their creation, queuing, and processing.\n        The method is designed to be used as an asynchronous generator,\n        allowing it to be used with asyncio and other asynchronous frameworks.\n\n        :param scheduling_strategy: The scheduling strategy to use.\n            Specifies the times at which requests will be sent as well how many\n            worker processes are used and if requests are scheduled sync or async.\n            This can be one of the following:\n            - \"synchronous\": Requests are sent synchronously.\n            - \"throughput\": Requests are sent at the maximum rate possible.\n            - An instance of SchedulingStrategy.\n        :param max_number: The maximum number of requests to process.\n            If None, then no limit is set and either the iterator must be exhaustible\n            or the max_duration must be set.\n        :param max_duration: The maximum duration for the scheduling run.\n            If None, then no limit is set and either the iterator must be exhaustible\n            or the max_number must be set.\n        :return: An asynchronous generator that yields SchedulerResult objects.\n            Each SchedulerResult object contains information about the request,\n            the response, and the run information.\n        \"\"\"\n        if scheduling_strategy is None or not isinstance(\n            scheduling_strategy, SchedulingStrategy\n        ):\n            raise ValueError(f\"Invalid scheduling strategy: {scheduling_strategy}\")\n\n        if max_number is not None and max_number &lt; 1:\n            raise ValueError(f\"Invalid max_number: {max_number}\")\n\n        if max_duration is not None and max_duration &lt; 0:\n            raise ValueError(f\"Invalid max_duration: {max_duration}\")\n\n        with (\n            multiprocessing.Manager() as manager,\n            ProcessPoolExecutor(\n                max_workers=scheduling_strategy.processes_limit\n            ) as executor,\n        ):\n            requests_iter: Optional[Iterator[Any]] = None\n            futures, requests_queue, responses_queue = await self._start_processes(\n                manager, executor, scheduling_strategy\n            )\n            run_info, requests_iter, times_iter = self._run_setup(\n                futures, scheduling_strategy, max_number, max_duration\n            )\n            yield SchedulerResult(\n                type_=\"run_start\",\n                run_info=run_info,\n            )\n\n            try:\n                while True:\n                    # check errors and raise them\n                    for future in futures:\n                        if future.done() and (err := future.exception()) is not None:\n                            raise err\n\n                    if (\n                        requests_iter is None\n                        and run_info.completed_requests &gt;= run_info.created_requests\n                    ):\n                        # we've exhausted all requests we've wanted to run\n                        # and yielded all responses\n                        break\n\n                    requests_iter = self._add_requests(\n                        requests_iter,\n                        times_iter,\n                        requests_queue,\n                        run_info,\n                    )\n                    await asyncio.sleep(0)  # enable requests to start\n\n                    iter_result = self._check_result_ready(\n                        responses_queue,\n                        run_info,\n                    )\n                    if iter_result is not None:\n                        yield iter_result\n\n                    # yield control to the event loop\n                    await asyncio.sleep(settings.default_async_loop_sleep)\n            except Exception as err:\n                raise RuntimeError(f\"Scheduler run failed: {err}\") from err\n\n            yield SchedulerResult(\n                type_=\"run_complete\",\n                run_info=run_info,\n            )\n\n            await self._stop_processes(futures, requests_queue)\n\n    async def _start_processes(\n        self,\n        manager,\n        executor: ProcessPoolExecutor,\n        scheduling_strategy: SchedulingStrategy,\n    ) -&gt; tuple[\n        list[asyncio.Future],\n        multiprocessing.Queue,\n        multiprocessing.Queue,\n    ]:\n        await self.worker.prepare_multiprocessing()\n        requests_queue = manager.Queue(\n            maxsize=scheduling_strategy.queued_requests_limit\n        )\n        responses_queue = manager.Queue()\n\n        num_processes = min(\n            scheduling_strategy.processes_limit,\n            scheduling_strategy.processing_requests_limit,\n        )\n        requests_limit_split = (\n            scheduling_strategy.processing_requests_limit\n            // scheduling_strategy.processes_limit\n        )\n        requests_limit_remain = (\n            scheduling_strategy.processing_requests_limit\n            % scheduling_strategy.processes_limit\n        )\n        process_ids = (id_ for id_ in range(num_processes))\n        process_requests_limits = (\n            requests_limit_split + 1\n            if i &lt; requests_limit_remain\n            else requests_limit_split\n            for i in range(num_processes)\n        )\n\n        futures = []\n        loop = asyncio.get_event_loop()\n        for id_, requests_limit in zip(process_ids, process_requests_limits):\n            if scheduling_strategy.processing_mode == \"sync\":\n                futures.append(\n                    loop.run_in_executor(\n                        executor,\n                        self.worker.process_loop_synchronous,\n                        requests_queue,\n                        responses_queue,\n                        id_,\n                    )\n                )\n            elif scheduling_strategy.processing_mode == \"async\":\n                futures.append(\n                    loop.run_in_executor(\n                        executor,\n                        self.worker.process_loop_asynchronous,\n                        requests_queue,\n                        responses_queue,\n                        requests_limit,\n                        id_,\n                    )\n                )\n            else:\n                raise ValueError(\n                    f\"Invalid processing mode: {scheduling_strategy.processing_mode} \"\n                    f\"for strategy: {scheduling_strategy}\"\n                )\n\n        await asyncio.sleep(0.1)  # give time for processes to start\n\n        return futures, requests_queue, responses_queue\n\n    def _run_setup(\n        self,\n        processes: list[asyncio.Future],\n        scheduling_strategy: SchedulingStrategy,\n        max_number: Optional[int],\n        max_duration: Optional[float],\n    ) -&gt; tuple[SchedulerRunInfo, Iterator[Any], Iterator[float]]:\n        requests_iter = iter(self.request_loader)\n        start_time = time.time()\n        times_iter = iter(scheduling_strategy.request_times())\n        end_time = time.time() + (max_duration or math.inf)\n        end_number = max_number or math.inf\n\n        try:\n            # update end number if the request loader is finite and less than max\n            iter_length = len(self.request_loader)  # type: ignore[arg-type]\n            if 0 &lt; iter_length &lt; end_number:\n                end_number = iter_length\n        except Exception:  # noqa: BLE001, S110\n            pass\n\n        if end_number == math.inf and end_time is None:\n            logger.warning(\n                \"No end number or end time set, \"\n                \"scheduler will run indefinitely until the request loader is exhausted.\"\n            )\n\n        info = SchedulerRunInfo(\n            start_time=start_time,\n            end_time=end_time,\n            end_number=end_number,\n            processes=len(processes),\n            strategy=scheduling_strategy,\n        )\n\n        return info, requests_iter, times_iter\n\n    def _add_requests(\n        self,\n        requests_iter: Optional[Iterator[Any]],\n        times_iter: Iterator[float],\n        requests_queue: multiprocessing.Queue,\n        run_info: SchedulerRunInfo,\n    ) -&gt; Optional[Iterator[Any]]:\n        if requests_iter is not None:\n            try:\n                added_count = 0\n\n                while (\n                    not requests_queue.full()\n                    and added_count &lt; settings.max_add_requests_per_loop\n                ):\n                    if run_info.created_requests &gt;= run_info.end_number:\n                        raise StopIteration\n\n                    if (\n                        request_time := next(times_iter)\n                    ) &gt;= run_info.end_time or time.time() &gt;= run_info.end_time:\n                        raise StopIteration\n\n                    request = next(requests_iter)\n                    work_req: WorkerProcessRequest[RequestT] = WorkerProcessRequest(\n                        request=request,\n                        start_time=request_time,\n                        timeout_time=run_info.end_time,\n                        queued_time=time.time(),\n                    )\n                    requests_queue.put(work_req)\n\n                    run_info.created_requests += 1\n                    run_info.queued_requests += 1\n                    added_count += 1\n            except StopIteration:\n                # we've reached the limit number, limit time, or exhausted the requests\n                # set to None to stop adding more and tell the loop no more requests\n                requests_iter = None\n\n        return requests_iter\n\n    def _check_result_ready(\n        self,\n        responses_queue: multiprocessing.Queue,\n        run_info: SchedulerRunInfo,\n    ) -&gt; Optional[SchedulerRequestResult[RequestT, ResponseT]]:\n        try:\n            process_response: WorkerProcessResult[RequestT, ResponseT] = (\n                responses_queue.get_nowait()\n            )\n        except multiprocessing.queues.Empty:  # type: ignore[attr-defined]\n            return None\n\n        if process_response.type_ == \"request_scheduled\":\n            run_info.queued_requests -= 1\n            run_info.scheduled_requests += 1\n\n            return SchedulerRequestResult(\n                type_=\"request_scheduled\",\n                run_info=run_info,\n                request=process_response.request,\n                request_info=process_response.info,\n                response=None,\n            )\n\n        if process_response.type_ == \"request_start\":\n            run_info.scheduled_requests -= 1\n            run_info.processing_requests += 1\n\n            return SchedulerRequestResult(\n                type_=\"request_start\",\n                run_info=run_info,\n                request=process_response.request,\n                request_info=process_response.info,\n                response=None,\n            )\n\n        if process_response.type_ == \"request_complete\":\n            run_info.processing_requests -= 1\n            run_info.completed_requests += 1\n\n            return SchedulerRequestResult(\n                type_=\"request_complete\",\n                run_info=run_info,\n                request=process_response.request,\n                request_info=process_response.info,\n                response=process_response.response,\n            )\n        raise ValueError(f\"Invalid process response type: {process_response}\")\n\n    async def _stop_processes(\n        self,\n        futures: list[asyncio.Future],\n        requests_queue: multiprocessing.Queue,\n    ):\n        for _ in futures:\n            requests_queue.put(None)\n\n        await asyncio.gather(*futures)\n</code></pre>"},{"location":"reference/guidellm/scheduler/scheduler/#guidellm.scheduler.scheduler.Scheduler.run","title":"<code>run(scheduling_strategy, max_number=None, max_duration=None)</code>  <code>async</code>","text":"<p>The main method that runs the scheduler. This method is a generator that yields SchedulerResult objects at the start and end of the run, as well as at the start and end of each request. It uses multiprocessing to handle requests concurrently and efficiently, based on the specified scheduling strategy. The method also handles the lifecycle of the requests, including their creation, queuing, and processing. The method is designed to be used as an asynchronous generator, allowing it to be used with asyncio and other asynchronous frameworks.</p> <p>Parameters:</p> Name Type Description Default <code>scheduling_strategy</code> <code>SchedulingStrategy</code> <p>The scheduling strategy to use. Specifies the times at which requests will be sent as well how many worker processes are used and if requests are scheduled sync or async. This can be one of the following: - \"synchronous\": Requests are sent synchronously. - \"throughput\": Requests are sent at the maximum rate possible. - An instance of SchedulingStrategy.</p> required <code>max_number</code> <code>Optional[int]</code> <p>The maximum number of requests to process. If None, then no limit is set and either the iterator must be exhaustible or the max_duration must be set.</p> <code>None</code> <code>max_duration</code> <code>Optional[float]</code> <p>The maximum duration for the scheduling run. If None, then no limit is set and either the iterator must be exhaustible or the max_number must be set.</p> <code>None</code> <p>Returns:</p> Type Description <code>AsyncGenerator[Union[SchedulerResult, SchedulerRequestResult[RequestT, ResponseT]], None]</code> <p>An asynchronous generator that yields SchedulerResult objects. Each SchedulerResult object contains information about the request, the response, and the run information.</p> Source code in <code>src/guidellm/scheduler/scheduler.py</code> <pre><code>async def run(\n    self,\n    scheduling_strategy: SchedulingStrategy,\n    max_number: Optional[int] = None,\n    max_duration: Optional[float] = None,\n) -&gt; AsyncGenerator[\n    Union[SchedulerResult, SchedulerRequestResult[RequestT, ResponseT]], None\n]:\n    \"\"\"\n    The main method that runs the scheduler.\n    This method is a generator that yields SchedulerResult objects\n    at the start and end of the run, as well as at the start and end\n    of each request.\n    It uses multiprocessing to handle requests concurrently\n    and efficiently, based on the specified scheduling strategy.\n    The method also handles the lifecycle of the requests,\n    including their creation, queuing, and processing.\n    The method is designed to be used as an asynchronous generator,\n    allowing it to be used with asyncio and other asynchronous frameworks.\n\n    :param scheduling_strategy: The scheduling strategy to use.\n        Specifies the times at which requests will be sent as well how many\n        worker processes are used and if requests are scheduled sync or async.\n        This can be one of the following:\n        - \"synchronous\": Requests are sent synchronously.\n        - \"throughput\": Requests are sent at the maximum rate possible.\n        - An instance of SchedulingStrategy.\n    :param max_number: The maximum number of requests to process.\n        If None, then no limit is set and either the iterator must be exhaustible\n        or the max_duration must be set.\n    :param max_duration: The maximum duration for the scheduling run.\n        If None, then no limit is set and either the iterator must be exhaustible\n        or the max_number must be set.\n    :return: An asynchronous generator that yields SchedulerResult objects.\n        Each SchedulerResult object contains information about the request,\n        the response, and the run information.\n    \"\"\"\n    if scheduling_strategy is None or not isinstance(\n        scheduling_strategy, SchedulingStrategy\n    ):\n        raise ValueError(f\"Invalid scheduling strategy: {scheduling_strategy}\")\n\n    if max_number is not None and max_number &lt; 1:\n        raise ValueError(f\"Invalid max_number: {max_number}\")\n\n    if max_duration is not None and max_duration &lt; 0:\n        raise ValueError(f\"Invalid max_duration: {max_duration}\")\n\n    with (\n        multiprocessing.Manager() as manager,\n        ProcessPoolExecutor(\n            max_workers=scheduling_strategy.processes_limit\n        ) as executor,\n    ):\n        requests_iter: Optional[Iterator[Any]] = None\n        futures, requests_queue, responses_queue = await self._start_processes(\n            manager, executor, scheduling_strategy\n        )\n        run_info, requests_iter, times_iter = self._run_setup(\n            futures, scheduling_strategy, max_number, max_duration\n        )\n        yield SchedulerResult(\n            type_=\"run_start\",\n            run_info=run_info,\n        )\n\n        try:\n            while True:\n                # check errors and raise them\n                for future in futures:\n                    if future.done() and (err := future.exception()) is not None:\n                        raise err\n\n                if (\n                    requests_iter is None\n                    and run_info.completed_requests &gt;= run_info.created_requests\n                ):\n                    # we've exhausted all requests we've wanted to run\n                    # and yielded all responses\n                    break\n\n                requests_iter = self._add_requests(\n                    requests_iter,\n                    times_iter,\n                    requests_queue,\n                    run_info,\n                )\n                await asyncio.sleep(0)  # enable requests to start\n\n                iter_result = self._check_result_ready(\n                    responses_queue,\n                    run_info,\n                )\n                if iter_result is not None:\n                    yield iter_result\n\n                # yield control to the event loop\n                await asyncio.sleep(settings.default_async_loop_sleep)\n        except Exception as err:\n            raise RuntimeError(f\"Scheduler run failed: {err}\") from err\n\n        yield SchedulerResult(\n            type_=\"run_complete\",\n            run_info=run_info,\n        )\n\n        await self._stop_processes(futures, requests_queue)\n</code></pre>"},{"location":"reference/guidellm/scheduler/strategy/","title":"guidellm.scheduler.strategy","text":""},{"location":"reference/guidellm/scheduler/strategy/#guidellm.scheduler.strategy.AsyncConstantStrategy","title":"<code>AsyncConstantStrategy</code>","text":"<p>               Bases: <code>ThroughputStrategy</code></p> <p>A class representing an asynchronous constant scheduling strategy. This strategy schedules requests asynchronously at a constant request rate in requests per second. If initial_burst is set, it will send an initial burst of math.floor(rate) requests to reach the target rate. This is useful to ensure that the target rate is reached quickly and then maintained. It inherits from the <code>SchedulingStrategy</code> base class and implements the <code>request_times</code> method to provide the specific behavior for asynchronous constant scheduling.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <p>The constant StrategyType to schedule requests asynchronously.</p> required <code>rate</code> <p>The rate at which to schedule requests asynchronously in requests per second. This must be a positive float.</p> required <code>initial_burst</code> <p>True to send an initial burst of requests (math.floor(self.rate)) to reach target rate. False to not send an initial burst.</p> required Source code in <code>src/guidellm/scheduler/strategy.py</code> <pre><code>class AsyncConstantStrategy(ThroughputStrategy):\n    \"\"\"\n    A class representing an asynchronous constant scheduling strategy.\n    This strategy schedules requests asynchronously at a constant request rate\n    in requests per second.\n    If initial_burst is set, it will send an initial burst of math.floor(rate)\n    requests to reach the target rate.\n    This is useful to ensure that the target rate is reached quickly\n    and then maintained.\n    It inherits from the `SchedulingStrategy` base class and\n    implements the `request_times` method to provide the specific\n    behavior for asynchronous constant scheduling.\n\n    :param type_: The constant StrategyType to schedule requests asynchronously.\n    :param rate: The rate at which to schedule requests asynchronously in\n        requests per second. This must be a positive float.\n    :param initial_burst: True to send an initial burst of requests\n        (math.floor(self.rate)) to reach target rate.\n        False to not send an initial burst.\n    \"\"\"\n\n    type_: Literal[\"constant\"] = \"constant\"  # type: ignore[assignment]\n    rate: float = Field(\n        description=(\n            \"The rate at which to schedule requests asynchronously in \"\n            \"requests per second. This must be a positive float.\"\n        ),\n        gt=0,\n    )\n    initial_burst: bool = Field(\n        default=True,\n        description=(\n            \"True to send an initial burst of requests (math.floor(self.rate)) \"\n            \"to reach target rate. False to not send an initial burst.\"\n        ),\n    )\n\n    def request_times(self) -&gt; Generator[float, None, None]:\n        \"\"\"\n        A generator that yields timestamps for when requests should be sent.\n        This method schedules requests asynchronously at a constant rate\n        in requests per second.\n        If burst_time is set, it will send an initial burst of requests\n        to reach the target rate.\n        This is useful to ensure that the target rate is reached quickly\n        and then maintained.\n\n        :return: A generator that yields timestamps for request scheduling.\n        \"\"\"\n        start_time = time.time()\n        constant_increment = 1.0 / self.rate\n\n        # handle bursts first to get to the desired rate\n        if self.initial_burst is not None:\n            # send an initial burst equal to the rate\n            # to reach the target rate\n            burst_count = math.floor(self.rate)\n            for _ in range(burst_count):\n                yield start_time\n\n            start_time += constant_increment\n\n        counter = 0\n\n        # continue with constant rate after bursting\n        while True:\n            yield start_time + constant_increment * counter\n            counter += 1\n</code></pre>"},{"location":"reference/guidellm/scheduler/strategy/#guidellm.scheduler.strategy.AsyncConstantStrategy.request_times","title":"<code>request_times()</code>","text":"<p>A generator that yields timestamps for when requests should be sent. This method schedules requests asynchronously at a constant rate in requests per second. If burst_time is set, it will send an initial burst of requests to reach the target rate. This is useful to ensure that the target rate is reached quickly and then maintained.</p> <p>Returns:</p> Type Description <code>Generator[float, None, None]</code> <p>A generator that yields timestamps for request scheduling.</p> Source code in <code>src/guidellm/scheduler/strategy.py</code> <pre><code>def request_times(self) -&gt; Generator[float, None, None]:\n    \"\"\"\n    A generator that yields timestamps for when requests should be sent.\n    This method schedules requests asynchronously at a constant rate\n    in requests per second.\n    If burst_time is set, it will send an initial burst of requests\n    to reach the target rate.\n    This is useful to ensure that the target rate is reached quickly\n    and then maintained.\n\n    :return: A generator that yields timestamps for request scheduling.\n    \"\"\"\n    start_time = time.time()\n    constant_increment = 1.0 / self.rate\n\n    # handle bursts first to get to the desired rate\n    if self.initial_burst is not None:\n        # send an initial burst equal to the rate\n        # to reach the target rate\n        burst_count = math.floor(self.rate)\n        for _ in range(burst_count):\n            yield start_time\n\n        start_time += constant_increment\n\n    counter = 0\n\n    # continue with constant rate after bursting\n    while True:\n        yield start_time + constant_increment * counter\n        counter += 1\n</code></pre>"},{"location":"reference/guidellm/scheduler/strategy/#guidellm.scheduler.strategy.AsyncPoissonStrategy","title":"<code>AsyncPoissonStrategy</code>","text":"<p>               Bases: <code>ThroughputStrategy</code></p> <p>A class representing an asynchronous Poisson scheduling strategy. This strategy schedules requests asynchronously at a Poisson request rate in requests per second. If initial_burst is set, it will send an initial burst of math.floor(rate) requests to reach the target rate. It inherits from the <code>SchedulingStrategy</code> base class and implements the <code>request_times</code> method to provide the specific behavior for asynchronous Poisson scheduling.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <p>The Poisson StrategyType to schedule requests asynchronously.</p> required <code>rate</code> <p>The rate at which to schedule requests asynchronously in requests per second. This must be a positive float.</p> required <code>initial_burst</code> <p>True to send an initial burst of requests (math.floor(self.rate)) to reach target rate. False to not send an initial burst.</p> required Source code in <code>src/guidellm/scheduler/strategy.py</code> <pre><code>class AsyncPoissonStrategy(ThroughputStrategy):\n    \"\"\"\n    A class representing an asynchronous Poisson scheduling strategy.\n    This strategy schedules requests asynchronously at a Poisson request rate\n    in requests per second.\n    If initial_burst is set, it will send an initial burst of math.floor(rate)\n    requests to reach the target rate.\n    It inherits from the `SchedulingStrategy` base class and\n    implements the `request_times` method to provide the specific\n    behavior for asynchronous Poisson scheduling.\n\n    :param type_: The Poisson StrategyType to schedule requests asynchronously.\n    :param rate: The rate at which to schedule requests asynchronously in\n        requests per second. This must be a positive float.\n    :param initial_burst: True to send an initial burst of requests\n        (math.floor(self.rate)) to reach target rate.\n        False to not send an initial burst.\n    \"\"\"\n\n    type_: Literal[\"poisson\"] = \"poisson\"  # type: ignore[assignment]\n    rate: float = Field(\n        description=(\n            \"The rate at which to schedule requests asynchronously in \"\n            \"requests per second. This must be a positive float.\"\n        ),\n        gt=0,\n    )\n    initial_burst: bool = Field(\n        default=True,\n        description=(\n            \"True to send an initial burst of requests (math.floor(self.rate)) \"\n            \"to reach target rate. False to not send an initial burst.\"\n        ),\n    )\n    random_seed: int = Field(\n        default=42,\n        description=(\"The random seed to use for the Poisson distribution. \"),\n    )\n\n    def request_times(self) -&gt; Generator[float, None, None]:\n        \"\"\"\n        A generator that yields timestamps for when requests should be sent.\n        This method schedules requests asynchronously at a Poisson rate\n        in requests per second.\n        The inter arrival time between requests is exponentially distributed\n        based on the rate.\n\n        :return: A generator that yields timestamps for request scheduling.\n        \"\"\"\n        start_time = time.time()\n\n        if self.initial_burst is not None:\n            # send an initial burst equal to the rate\n            # to reach the target rate\n            burst_count = math.floor(self.rate)\n            for _ in range(burst_count):\n                yield start_time\n        else:\n            yield start_time\n\n        # set the random seed for reproducibility\n        rand = random.Random(self.random_seed)  # noqa: S311\n\n        while True:\n            inter_arrival_time = rand.expovariate(self.rate)\n            start_time += inter_arrival_time\n            yield start_time\n</code></pre>"},{"location":"reference/guidellm/scheduler/strategy/#guidellm.scheduler.strategy.AsyncPoissonStrategy.request_times","title":"<code>request_times()</code>","text":"<p>A generator that yields timestamps for when requests should be sent. This method schedules requests asynchronously at a Poisson rate in requests per second. The inter arrival time between requests is exponentially distributed based on the rate.</p> <p>Returns:</p> Type Description <code>Generator[float, None, None]</code> <p>A generator that yields timestamps for request scheduling.</p> Source code in <code>src/guidellm/scheduler/strategy.py</code> <pre><code>def request_times(self) -&gt; Generator[float, None, None]:\n    \"\"\"\n    A generator that yields timestamps for when requests should be sent.\n    This method schedules requests asynchronously at a Poisson rate\n    in requests per second.\n    The inter arrival time between requests is exponentially distributed\n    based on the rate.\n\n    :return: A generator that yields timestamps for request scheduling.\n    \"\"\"\n    start_time = time.time()\n\n    if self.initial_burst is not None:\n        # send an initial burst equal to the rate\n        # to reach the target rate\n        burst_count = math.floor(self.rate)\n        for _ in range(burst_count):\n            yield start_time\n    else:\n        yield start_time\n\n    # set the random seed for reproducibility\n    rand = random.Random(self.random_seed)  # noqa: S311\n\n    while True:\n        inter_arrival_time = rand.expovariate(self.rate)\n        start_time += inter_arrival_time\n        yield start_time\n</code></pre>"},{"location":"reference/guidellm/scheduler/strategy/#guidellm.scheduler.strategy.ConcurrentStrategy","title":"<code>ConcurrentStrategy</code>","text":"<p>               Bases: <code>SchedulingStrategy</code></p> <p>A class representing a concurrent scheduling strategy. This strategy schedules requests concurrently with the specified number of streams. It inherits from the <code>SchedulingStrategy</code> base class and implements the <code>request_times</code> method to provide the specific behavior for concurrent scheduling.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <p>The concurrent StrategyType to schedule requests concurrently.</p> required <code>streams</code> <p>The number of concurrent streams to use for scheduling requests. Each stream runs synchronously with the maximum rate possible. This must be a positive integer.</p> required Source code in <code>src/guidellm/scheduler/strategy.py</code> <pre><code>class ConcurrentStrategy(SchedulingStrategy):\n    \"\"\"\n    A class representing a concurrent scheduling strategy.\n    This strategy schedules requests concurrently with the specified\n    number of streams.\n    It inherits from the `SchedulingStrategy` base class and\n    implements the `request_times` method to provide the specific\n    behavior for concurrent scheduling.\n\n    :param type_: The concurrent StrategyType to schedule requests concurrently.\n    :param streams: The number of concurrent streams to use for scheduling requests.\n        Each stream runs synchronously with the maximum rate possible.\n        This must be a positive integer.\n    \"\"\"\n\n    type_: Literal[\"concurrent\"] = \"concurrent\"  # type: ignore[assignment]\n    streams: int = Field(\n        description=(\n            \"The number of concurrent streams to use for scheduling requests. \"\n            \"Each stream runs sychronously with the maximum rate possible. \"\n            \"This must be a positive integer.\"\n        ),\n        gt=0,\n    )\n\n    @property\n    def processing_mode(self) -&gt; Literal[\"sync\"]:\n        \"\"\"\n        The processing mode for the scheduling strategy, either 'sync' or 'async'.\n        This property determines how the worker processes are setup:\n        either to run synchronously with one request at a time or asynchronously.\n\n        :return: 'sync' for synchronous scheduling strategy\n            for the multiple worker processes equal to streams.\n        \"\"\"\n        return \"sync\"\n\n    @property\n    def processes_limit(self) -&gt; int:\n        \"\"\"\n        The limit on the number of worker processes for the scheduling strategy.\n        It determines how many worker processes are created\n        for the scheduling strategy and must be implemented by subclasses.\n\n        :return: {self.streams} for the concurrent scheduling strategy to limit\n            the worker processes to the number of streams.\n        \"\"\"\n        return self.streams\n\n    @property\n    def queued_requests_limit(self) -&gt; int:\n        \"\"\"\n        The maximum number of queued requests for the scheduling strategy.\n        It determines how many requests can be queued at one time\n        for the scheduling strategy and must be implemented by subclasses.\n\n        :return: {self.streams} for the concurrent scheduling strategy to limit\n            the queued requests to the number of streams that are ready to be processed.\n        \"\"\"\n        return self.streams\n\n    @property\n    def processing_requests_limit(self) -&gt; int:\n        \"\"\"\n        The maximum number of processing requests for the scheduling strategy.\n        It determines how many requests can be processed at one time\n        for the scheduling strategy and must be implemented by subclasses.\n\n        :return: {self.streams} for the concurrent scheduling strategy to limit\n            the processing requests to the number of streams that ready to be processed.\n        \"\"\"\n        return self.streams\n\n    def request_times(self) -&gt; Generator[float, None, None]:\n        \"\"\"\n        A generator that yields time.time() so requests are sent\n        immediately, while scheduling them concurrently with the specified\n        number of streams.\n\n        :return: A generator that yields time.time() for immediate request scheduling.\n        \"\"\"\n        while True:\n            yield time.time()\n</code></pre>"},{"location":"reference/guidellm/scheduler/strategy/#guidellm.scheduler.strategy.ConcurrentStrategy.processes_limit","title":"<code>processes_limit</code>  <code>property</code>","text":"<p>The limit on the number of worker processes for the scheduling strategy. It determines how many worker processes are created for the scheduling strategy and must be implemented by subclasses.</p> <p>Returns:</p> Type Description <code>int</code> <p>{self.streams} for the concurrent scheduling strategy to limit the worker processes to the number of streams.</p>"},{"location":"reference/guidellm/scheduler/strategy/#guidellm.scheduler.strategy.ConcurrentStrategy.processing_mode","title":"<code>processing_mode</code>  <code>property</code>","text":"<p>The processing mode for the scheduling strategy, either 'sync' or 'async'. This property determines how the worker processes are setup: either to run synchronously with one request at a time or asynchronously.</p> <p>Returns:</p> Type Description <code>Literal['sync']</code> <p>'sync' for synchronous scheduling strategy for the multiple worker processes equal to streams.</p>"},{"location":"reference/guidellm/scheduler/strategy/#guidellm.scheduler.strategy.ConcurrentStrategy.processing_requests_limit","title":"<code>processing_requests_limit</code>  <code>property</code>","text":"<p>The maximum number of processing requests for the scheduling strategy. It determines how many requests can be processed at one time for the scheduling strategy and must be implemented by subclasses.</p> <p>Returns:</p> Type Description <code>int</code> <p>{self.streams} for the concurrent scheduling strategy to limit the processing requests to the number of streams that ready to be processed.</p>"},{"location":"reference/guidellm/scheduler/strategy/#guidellm.scheduler.strategy.ConcurrentStrategy.queued_requests_limit","title":"<code>queued_requests_limit</code>  <code>property</code>","text":"<p>The maximum number of queued requests for the scheduling strategy. It determines how many requests can be queued at one time for the scheduling strategy and must be implemented by subclasses.</p> <p>Returns:</p> Type Description <code>int</code> <p>{self.streams} for the concurrent scheduling strategy to limit the queued requests to the number of streams that are ready to be processed.</p>"},{"location":"reference/guidellm/scheduler/strategy/#guidellm.scheduler.strategy.ConcurrentStrategy.request_times","title":"<code>request_times()</code>","text":"<p>A generator that yields time.time() so requests are sent immediately, while scheduling them concurrently with the specified number of streams.</p> <p>Returns:</p> Type Description <code>Generator[float, None, None]</code> <p>A generator that yields time.time() for immediate request scheduling.</p> Source code in <code>src/guidellm/scheduler/strategy.py</code> <pre><code>def request_times(self) -&gt; Generator[float, None, None]:\n    \"\"\"\n    A generator that yields time.time() so requests are sent\n    immediately, while scheduling them concurrently with the specified\n    number of streams.\n\n    :return: A generator that yields time.time() for immediate request scheduling.\n    \"\"\"\n    while True:\n        yield time.time()\n</code></pre>"},{"location":"reference/guidellm/scheduler/strategy/#guidellm.scheduler.strategy.SchedulingStrategy","title":"<code>SchedulingStrategy</code>","text":"<p>               Bases: <code>StandardBaseModel</code></p> <p>An abstract base class for scheduling strategies. This class defines the interface for scheduling requests and provides a common structure for all scheduling strategies. Subclasses should implement the <code>request_times</code> method to provide specific scheduling behavior.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <p>The type of scheduling strategy to use. This should be one of the predefined strategy types.</p> required Source code in <code>src/guidellm/scheduler/strategy.py</code> <pre><code>class SchedulingStrategy(StandardBaseModel):\n    \"\"\"\n    An abstract base class for scheduling strategies.\n    This class defines the interface for scheduling requests and provides\n    a common structure for all scheduling strategies.\n    Subclasses should implement the `request_times` method to provide\n    specific scheduling behavior.\n\n    :param type_: The type of scheduling strategy to use.\n        This should be one of the predefined strategy types.\n    \"\"\"\n\n    type_: Literal[\"strategy\"] = Field(\n        description=\"The type of scheduling strategy schedule requests with.\",\n    )\n\n    @property\n    def processing_mode(self) -&gt; Literal[\"sync\", \"async\"]:\n        \"\"\"\n        The processing mode for the scheduling strategy, either 'sync' or 'async'.\n        This property determines how the worker processes are setup:\n        either to run synchronously with one request at a time or asynchronously.\n        This property should be implemented by subclasses to return\n        the appropriate processing mode.\n\n        :return: The processing mode for the scheduling strategy,\n            either 'sync' or 'async'.\n        \"\"\"\n        return \"async\"\n\n    @property\n    def processes_limit(self) -&gt; int:\n        \"\"\"\n        The limit on the number of worker processes for the scheduling strategy.\n        It determines how many worker processes are created\n        for the scheduling strategy and must be implemented by subclasses.\n\n        :return: The number of processes for the scheduling strategy.\n        \"\"\"\n        cpu_cores = os.cpu_count() or 1\n\n        return min(max(1, cpu_cores - 1), settings.max_worker_processes)\n\n    @property\n    def queued_requests_limit(self) -&gt; Optional[int]:\n        \"\"\"\n        The maximum number of queued requests for the scheduling strategy.\n        It determines how many requests can be queued at one time\n        for the scheduling strategy and must be implemented by subclasses.\n\n        :return: The maximum number of queued requests for the scheduling strategy.\n        \"\"\"\n        return settings.max_concurrency\n\n    @property\n    def processing_requests_limit(self) -&gt; int:\n        \"\"\"\n        The maximum number of processing requests for the scheduling strategy.\n        It determines how many requests can be processed at one time\n        for the scheduling strategy and must be implemented by subclasses.\n\n        :return: The maximum number of processing requests for the scheduling strategy.\n        \"\"\"\n        return settings.max_concurrency\n\n    def request_times(self) -&gt; Generator[float, None, None]:\n        \"\"\"\n        A generator that yields timestamps for when requests should be sent.\n        This method should be implemented by subclasses to provide specific\n        scheduling behavior.\n\n        :return: A generator that yields timestamps for request scheduling\n            or -1 for requests that should be sent immediately.\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement request_times() method.\")\n</code></pre>"},{"location":"reference/guidellm/scheduler/strategy/#guidellm.scheduler.strategy.SchedulingStrategy.processes_limit","title":"<code>processes_limit</code>  <code>property</code>","text":"<p>The limit on the number of worker processes for the scheduling strategy. It determines how many worker processes are created for the scheduling strategy and must be implemented by subclasses.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of processes for the scheduling strategy.</p>"},{"location":"reference/guidellm/scheduler/strategy/#guidellm.scheduler.strategy.SchedulingStrategy.processing_mode","title":"<code>processing_mode</code>  <code>property</code>","text":"<p>The processing mode for the scheduling strategy, either 'sync' or 'async'. This property determines how the worker processes are setup: either to run synchronously with one request at a time or asynchronously. This property should be implemented by subclasses to return the appropriate processing mode.</p> <p>Returns:</p> Type Description <code>Literal['sync', 'async']</code> <p>The processing mode for the scheduling strategy, either 'sync' or 'async'.</p>"},{"location":"reference/guidellm/scheduler/strategy/#guidellm.scheduler.strategy.SchedulingStrategy.processing_requests_limit","title":"<code>processing_requests_limit</code>  <code>property</code>","text":"<p>The maximum number of processing requests for the scheduling strategy. It determines how many requests can be processed at one time for the scheduling strategy and must be implemented by subclasses.</p> <p>Returns:</p> Type Description <code>int</code> <p>The maximum number of processing requests for the scheduling strategy.</p>"},{"location":"reference/guidellm/scheduler/strategy/#guidellm.scheduler.strategy.SchedulingStrategy.queued_requests_limit","title":"<code>queued_requests_limit</code>  <code>property</code>","text":"<p>The maximum number of queued requests for the scheduling strategy. It determines how many requests can be queued at one time for the scheduling strategy and must be implemented by subclasses.</p> <p>Returns:</p> Type Description <code>Optional[int]</code> <p>The maximum number of queued requests for the scheduling strategy.</p>"},{"location":"reference/guidellm/scheduler/strategy/#guidellm.scheduler.strategy.SchedulingStrategy.request_times","title":"<code>request_times()</code>","text":"<p>A generator that yields timestamps for when requests should be sent. This method should be implemented by subclasses to provide specific scheduling behavior.</p> <p>Returns:</p> Type Description <code>Generator[float, None, None]</code> <p>A generator that yields timestamps for request scheduling or -1 for requests that should be sent immediately.</p> Source code in <code>src/guidellm/scheduler/strategy.py</code> <pre><code>def request_times(self) -&gt; Generator[float, None, None]:\n    \"\"\"\n    A generator that yields timestamps for when requests should be sent.\n    This method should be implemented by subclasses to provide specific\n    scheduling behavior.\n\n    :return: A generator that yields timestamps for request scheduling\n        or -1 for requests that should be sent immediately.\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement request_times() method.\")\n</code></pre>"},{"location":"reference/guidellm/scheduler/strategy/#guidellm.scheduler.strategy.SynchronousStrategy","title":"<code>SynchronousStrategy</code>","text":"<p>               Bases: <code>SchedulingStrategy</code></p> <p>A class representing a synchronous scheduling strategy. This strategy schedules requests synchronously, one at a time, with the maximum rate possible. It inherits from the <code>SchedulingStrategy</code> base class and implements the <code>request_times</code> method to provide the specific behavior for synchronous scheduling.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <p>The synchronous StrategyType to schedule requests synchronously.</p> required Source code in <code>src/guidellm/scheduler/strategy.py</code> <pre><code>class SynchronousStrategy(SchedulingStrategy):\n    \"\"\"\n    A class representing a synchronous scheduling strategy.\n    This strategy schedules requests synchronously, one at a time,\n    with the maximum rate possible.\n    It inherits from the `SchedulingStrategy` base class and\n    implements the `request_times` method to provide the specific\n    behavior for synchronous scheduling.\n\n    :param type_: The synchronous StrategyType to schedule requests synchronously.\n    \"\"\"\n\n    type_: Literal[\"synchronous\"] = \"synchronous\"  # type: ignore[assignment]\n\n    @property\n    def processing_mode(self) -&gt; Literal[\"sync\"]:\n        \"\"\"\n        The processing mode for the scheduling strategy, either 'sync' or 'async'.\n        This property determines how the worker processes are setup:\n        either to run synchronously with one request at a time or asynchronously.\n\n        :return: 'sync' for synchronous scheduling strategy\n            for the single worker process.\n        \"\"\"\n        return \"sync\"\n\n    @property\n    def processes_limit(self) -&gt; int:\n        \"\"\"\n        The limit on the number of worker processes for the scheduling strategy.\n        It determines how many worker processes are created\n        for the scheduling strategy and must be implemented by subclasses.\n\n        :return: 1 for the synchronous scheduling strategy to limit\n            the worker processes to one.\n        \"\"\"\n        return 1\n\n    @property\n    def queued_requests_limit(self) -&gt; int:\n        \"\"\"\n        The maximum number of queued requests for the scheduling strategy.\n        It determines how many requests can be queued at one time\n        for the scheduling strategy and must be implemented by subclasses.\n\n        :return: 1 for the synchronous scheduling strategy to limit\n            the queued requests to one that is ready to be processed.\n        \"\"\"\n        return 1\n\n    @property\n    def processing_requests_limit(self) -&gt; int:\n        \"\"\"\n        The maximum number of processing requests for the scheduling strategy.\n        It determines how many requests can be processed at one time\n        for the scheduling strategy and must be implemented by subclasses.\n\n        :return: 1 for the synchronous scheduling strategy to limit\n            the processing requests to one that is ready to be processed.\n        \"\"\"\n        return 1\n\n    def request_times(self) -&gt; Generator[float, None, None]:\n        \"\"\"\n        A generator that yields time.time() so requests are sent immediately,\n            while scheduling them synchronously.\n\n        :return: A generator that yields time.time() for immediate request scheduling.\n        \"\"\"\n        while True:\n            yield time.time()\n</code></pre>"},{"location":"reference/guidellm/scheduler/strategy/#guidellm.scheduler.strategy.SynchronousStrategy.processes_limit","title":"<code>processes_limit</code>  <code>property</code>","text":"<p>The limit on the number of worker processes for the scheduling strategy. It determines how many worker processes are created for the scheduling strategy and must be implemented by subclasses.</p> <p>Returns:</p> Type Description <code>int</code> <p>1 for the synchronous scheduling strategy to limit the worker processes to one.</p>"},{"location":"reference/guidellm/scheduler/strategy/#guidellm.scheduler.strategy.SynchronousStrategy.processing_mode","title":"<code>processing_mode</code>  <code>property</code>","text":"<p>The processing mode for the scheduling strategy, either 'sync' or 'async'. This property determines how the worker processes are setup: either to run synchronously with one request at a time or asynchronously.</p> <p>Returns:</p> Type Description <code>Literal['sync']</code> <p>'sync' for synchronous scheduling strategy for the single worker process.</p>"},{"location":"reference/guidellm/scheduler/strategy/#guidellm.scheduler.strategy.SynchronousStrategy.processing_requests_limit","title":"<code>processing_requests_limit</code>  <code>property</code>","text":"<p>The maximum number of processing requests for the scheduling strategy. It determines how many requests can be processed at one time for the scheduling strategy and must be implemented by subclasses.</p> <p>Returns:</p> Type Description <code>int</code> <p>1 for the synchronous scheduling strategy to limit the processing requests to one that is ready to be processed.</p>"},{"location":"reference/guidellm/scheduler/strategy/#guidellm.scheduler.strategy.SynchronousStrategy.queued_requests_limit","title":"<code>queued_requests_limit</code>  <code>property</code>","text":"<p>The maximum number of queued requests for the scheduling strategy. It determines how many requests can be queued at one time for the scheduling strategy and must be implemented by subclasses.</p> <p>Returns:</p> Type Description <code>int</code> <p>1 for the synchronous scheduling strategy to limit the queued requests to one that is ready to be processed.</p>"},{"location":"reference/guidellm/scheduler/strategy/#guidellm.scheduler.strategy.SynchronousStrategy.request_times","title":"<code>request_times()</code>","text":"<p>A generator that yields time.time() so requests are sent immediately,     while scheduling them synchronously.</p> <p>Returns:</p> Type Description <code>Generator[float, None, None]</code> <p>A generator that yields time.time() for immediate request scheduling.</p> Source code in <code>src/guidellm/scheduler/strategy.py</code> <pre><code>def request_times(self) -&gt; Generator[float, None, None]:\n    \"\"\"\n    A generator that yields time.time() so requests are sent immediately,\n        while scheduling them synchronously.\n\n    :return: A generator that yields time.time() for immediate request scheduling.\n    \"\"\"\n    while True:\n        yield time.time()\n</code></pre>"},{"location":"reference/guidellm/scheduler/strategy/#guidellm.scheduler.strategy.ThroughputStrategy","title":"<code>ThroughputStrategy</code>","text":"<p>               Bases: <code>SchedulingStrategy</code></p> <p>A class representing a throughput scheduling strategy. This strategy schedules as many requests asynchronously as possible, with the maximum rate possible. It inherits from the <code>SchedulingStrategy</code> base class and implements the <code>request_times</code> method to provide the specific behavior for throughput scheduling.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <p>The throughput StrategyType to schedule requests asynchronously.</p> required Source code in <code>src/guidellm/scheduler/strategy.py</code> <pre><code>class ThroughputStrategy(SchedulingStrategy):\n    \"\"\"\n    A class representing a throughput scheduling strategy.\n    This strategy schedules as many requests asynchronously as possible,\n    with the maximum rate possible.\n    It inherits from the `SchedulingStrategy` base class and\n    implements the `request_times` method to provide the specific\n    behavior for throughput scheduling.\n\n    :param type_: The throughput StrategyType to schedule requests asynchronously.\n    \"\"\"\n\n    type_: Literal[\"throughput\"] = \"throughput\"  # type: ignore[assignment]\n    max_concurrency: Optional[int] = Field(\n        default=None,\n        description=(\n            \"The maximum number of concurrent requests to schedule. \"\n            \"If set to None, the concurrency value from settings will be used. \"\n            \"This must be a positive integer greater than 0.\"\n        ),\n        gt=0,\n    )\n\n    @property\n    def processing_mode(self) -&gt; Literal[\"async\"]:\n        \"\"\"\n        The processing mode for the scheduling strategy, either 'sync' or 'async'.\n        This property determines how the worker processes are setup:\n        either to run synchronously with one request at a time or asynchronously.\n\n        :return: 'async' for asynchronous scheduling strategy\n            for the multiple worker processes handling requests.\n        \"\"\"\n        return \"async\"\n\n    @property\n    def queued_requests_limit(self) -&gt; int:\n        \"\"\"\n        The maximum number of queued requests for the scheduling strategy.\n        It determines how many requests can be queued at one time\n        for the scheduling strategy and must be implemented by subclasses.\n\n        :return: The processing requests limit to ensure that there are enough\n            requests even for the worst case scenario where the max concurrent\n            requests are pulled at once for processing.\n        \"\"\"\n        return self.processing_requests_limit\n\n    @property\n    def processing_requests_limit(self) -&gt; int:\n        \"\"\"\n        The maximum number of processing requests for the scheduling strategy.\n        It determines how many requests can be processed at one time\n        for the scheduling strategy and must be implemented by subclasses.\n\n        :return: {self.max_concurrency} for the throughput scheduling strategy to limit\n            the processing requests to the maximum concurrency.\n            If max_concurrency is None, then the default processing requests limit\n            will be used.\n        \"\"\"\n        return self.max_concurrency or super().processing_requests_limit\n\n    def request_times(self) -&gt; Generator[float, None, None]:\n        \"\"\"\n        A generator that yields the start time.time() so requests are sent\n        immediately, while scheduling as many asynchronously as possible.\n\n        :return: A generator that yields the start time.time()\n            for immediate request scheduling.\n        \"\"\"\n        start_time = time.time()\n\n        while True:\n            yield start_time\n</code></pre>"},{"location":"reference/guidellm/scheduler/strategy/#guidellm.scheduler.strategy.ThroughputStrategy.processing_mode","title":"<code>processing_mode</code>  <code>property</code>","text":"<p>The processing mode for the scheduling strategy, either 'sync' or 'async'. This property determines how the worker processes are setup: either to run synchronously with one request at a time or asynchronously.</p> <p>Returns:</p> Type Description <code>Literal['async']</code> <p>'async' for asynchronous scheduling strategy for the multiple worker processes handling requests.</p>"},{"location":"reference/guidellm/scheduler/strategy/#guidellm.scheduler.strategy.ThroughputStrategy.processing_requests_limit","title":"<code>processing_requests_limit</code>  <code>property</code>","text":"<p>The maximum number of processing requests for the scheduling strategy. It determines how many requests can be processed at one time for the scheduling strategy and must be implemented by subclasses.</p> <p>Returns:</p> Type Description <code>int</code> <p>{self.max_concurrency} for the throughput scheduling strategy to limit the processing requests to the maximum concurrency. If max_concurrency is None, then the default processing requests limit will be used.</p>"},{"location":"reference/guidellm/scheduler/strategy/#guidellm.scheduler.strategy.ThroughputStrategy.queued_requests_limit","title":"<code>queued_requests_limit</code>  <code>property</code>","text":"<p>The maximum number of queued requests for the scheduling strategy. It determines how many requests can be queued at one time for the scheduling strategy and must be implemented by subclasses.</p> <p>Returns:</p> Type Description <code>int</code> <p>The processing requests limit to ensure that there are enough requests even for the worst case scenario where the max concurrent requests are pulled at once for processing.</p>"},{"location":"reference/guidellm/scheduler/strategy/#guidellm.scheduler.strategy.ThroughputStrategy.request_times","title":"<code>request_times()</code>","text":"<p>A generator that yields the start time.time() so requests are sent immediately, while scheduling as many asynchronously as possible.</p> <p>Returns:</p> Type Description <code>Generator[float, None, None]</code> <p>A generator that yields the start time.time() for immediate request scheduling.</p> Source code in <code>src/guidellm/scheduler/strategy.py</code> <pre><code>def request_times(self) -&gt; Generator[float, None, None]:\n    \"\"\"\n    A generator that yields the start time.time() so requests are sent\n    immediately, while scheduling as many asynchronously as possible.\n\n    :return: A generator that yields the start time.time()\n        for immediate request scheduling.\n    \"\"\"\n    start_time = time.time()\n\n    while True:\n        yield start_time\n</code></pre>"},{"location":"reference/guidellm/scheduler/types/","title":"guidellm.scheduler.types","text":""},{"location":"reference/guidellm/scheduler/worker/","title":"guidellm.scheduler.worker","text":""},{"location":"reference/guidellm/scheduler/worker/#guidellm.scheduler.worker.GenerativeRequestsWorker","title":"<code>GenerativeRequestsWorker</code>","text":"<p>               Bases: <code>RequestsWorker[GenerationRequest, ResponseSummary]</code></p> <p>A class that handles the execution of requests using a backend. This class is responsible for sending requests to the backend, handling responses, and managing errors.</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <code>Backend</code> <p>The backend to use for handling requests. This should be an instance of Backend such as an OpenAIHTTPBackend.</p> required Source code in <code>src/guidellm/scheduler/worker.py</code> <pre><code>class GenerativeRequestsWorker(RequestsWorker[GenerationRequest, ResponseSummary]):\n    \"\"\"\n    A class that handles the execution of requests using a backend.\n    This class is responsible for sending requests to the backend,\n    handling responses, and managing errors.\n\n    :param backend: The backend to use for handling requests.\n        This should be an instance of Backend such as an OpenAIHTTPBackend.\n    \"\"\"\n\n    def __init__(self, backend: Backend):\n        self.backend = backend\n\n    @property\n    def description(self) -&gt; GenerativeRequestsWorkerDescription:\n        \"\"\"\n        Get the description of the worker.\n        :return: The description of the worker.\n        \"\"\"\n        return GenerativeRequestsWorkerDescription(\n            backend_type=self.backend.type_,\n            backend_target=self.backend.target,\n            backend_model=self.backend.model or \"None\",\n            backend_info=self.backend.info,\n        )\n\n    async def prepare_multiprocessing(self):\n        \"\"\"\n        Prepare the worker for multiprocessing.\n        This is useful for workers that have instance state that can not\n        be shared across processes and should be cleared out and re-initialized\n        for each new process.\n        \"\"\"\n        await self.backend.prepare_multiprocessing()\n\n    def process_loop_synchronous(\n        self,\n        requests_queue: multiprocessing.Queue,\n        results_queue: multiprocessing.Queue,\n        process_id: int,\n    ):\n        asyncio.run(self.backend.validate())\n        super().process_loop_synchronous(\n            requests_queue=requests_queue,\n            results_queue=results_queue,\n            process_id=process_id,\n        )\n\n    def process_loop_asynchronous(\n        self,\n        requests_queue: multiprocessing.Queue,\n        results_queue: multiprocessing.Queue,\n        max_concurrency: int,\n        process_id: int,\n    ):\n        asyncio.run(self.backend.validate())\n        super().process_loop_asynchronous(\n            requests_queue=requests_queue,\n            results_queue=results_queue,\n            max_concurrency=max_concurrency,\n            process_id=process_id,\n        )\n\n    async def resolve(\n        self,\n        request: GenerationRequest,\n        timeout_time: float,\n    ) -&gt; tuple[ResolveStatus, ResponseSummary]:\n        \"\"\"\n        Resolve a request by sending it to the backend and handling the response.\n        This method sends the request to the backend, waits for a response,\n        and handles any errors that may occur during the process.\n\n        :param request: The request to resolve.\n        :param timeout_time: The time to wait for a response before timing out.\n            If timeout_time is math.inf, the request will not timeout.\n        :return: A ResponseSummary object containing the response from the backend.\n            If an error occurs, the ResponseSummary will contain the error message.\n        \"\"\"\n        resolve_start_time = time.time()\n        response = None\n        error: Optional[str] = None\n        status = ResolveStatus(\n            requested=False,\n            completed=False,\n            errored=False,\n            canceled=False,\n            request_start=-1,\n            request_end=-1,\n        )\n\n        try:\n            if timeout_time &lt; time.time():\n                raise asyncio.TimeoutError(\n                    \"The timeout time has already passed.\"\n                )  # exit early\n\n            status.requested = True\n            request_func, request_kwargs = self._create_request_func_kwargs(request)\n\n            async def _runner():\n                # wrap function so we can enforce timeout and\n                # still return the latest state from the backend\n                async for resp in request_func(**request_kwargs):  # type: ignore[operator]\n                    nonlocal response\n                    response = resp\n\n            await asyncio.wait_for(\n                _runner(),\n                timeout=timeout_time - time.time() if timeout_time &lt; math.inf else None,\n            )\n\n            if not response:\n                raise ValueError(\n                    f\"No response received for request: {request} \"\n                    f\"and backend: {self.backend}\"\n                )\n            if not isinstance(response, ResponseSummary):\n                raise ValueError(\n                    f\"Received no ResponseSummary for request: {request} \"\n                    f\"and backend: {self.backend}, received: {response}\"\n                )\n\n            status.completed = True\n        except asyncio.TimeoutError:\n            error = \"TimeoutError: The request timed out before completing.\"\n            status.errored = True\n            status.canceled = True\n        except Exception as exc:  # noqa: BLE001\n            error = str(exc)\n            status.errored = True\n\n        return self._handle_response(\n            status=status,\n            request=request,\n            response=response,\n            error=error,\n            resolve_start_time=resolve_start_time,\n        )\n\n    def _create_request_func_kwargs(\n        self,\n        request: GenerationRequest,\n    ) -&gt; tuple[\n        AsyncGenerator[Union[StreamingTextResponse, ResponseSummary], None],\n        dict[str, Any],\n    ]:\n        request_func: AsyncGenerator[\n            Union[StreamingTextResponse, ResponseSummary], None\n        ]\n        request_kwargs: dict[str, Any]\n\n        if request.request_type == \"text_completions\":\n            request_func = self.backend.text_completions  # type: ignore[assignment]\n            request_kwargs = {\n                \"prompt\": request.content,\n                \"request_id\": request.request_id,\n                \"prompt_token_count\": request.stats.get(\"prompt_tokens\", None),\n                \"output_token_count\": request.constraints.get(\"output_tokens\", None),\n                **request.params,\n            }\n        elif request.request_type == \"chat_completions\":\n            request_func = self.backend.chat_completions  # type: ignore[assignment]\n            request_kwargs = {\n                \"content\": request.content,\n                \"request_id\": request.request_id,\n                \"prompt_token_count\": request.stats.get(\"prompt_tokens\", None),\n                \"output_token_count\": request.constraints.get(\"output_tokens\", None),\n                **request.params,\n            }\n        else:\n            raise ValueError(\n                f\"Invalid request type: {request.request_type} for {request}\"\n            )\n\n        return request_func, request_kwargs\n\n    def _handle_response(\n        self,\n        status: ResolveStatus,\n        request: GenerationRequest,\n        response: Any,\n        error: Optional[str],\n        resolve_start_time: float,\n    ) -&gt; tuple[ResolveStatus, ResponseSummary]:\n        if response is None or not isinstance(\n            response, (ResponseSummary, StreamingTextResponse)\n        ):\n            # nothing received or invalid response, fill in defaults for error\n            if response:\n                error = str(\n                    ValueError(\n                        f\"Invalid response: {type(response)} for request: {request}; \"\n                    )\n                ) + (error or \"\")\n\n            response = ResponseSummary(\n                value=\"\",\n                request_args=RequestArgs(\n                    target=self.backend.target,\n                    headers={},\n                    params={},\n                    payload={},\n                ),\n                start_time=resolve_start_time,\n                end_time=status.request_end,\n                first_iter_time=None,\n                last_iter_time=None,\n                request_id=request.request_id,\n                error=error or \"Unknown error\",\n            )\n        elif isinstance(response, StreamingTextResponse):\n            response = ResponseSummary(\n                value=response.value,\n                request_args=RequestArgs(\n                    target=self.backend.target,\n                    headers={},\n                    params={},\n                    payload={},\n                ),\n                start_time=response.start_time,\n                end_time=time.time(),\n                first_iter_time=response.first_iter_time,\n                last_iter_time=response.time if response.iter_count &gt; 0 else None,\n                request_prompt_tokens=request.stats.get(\"prompt_tokens\", None),\n                request_output_tokens=request.constraints.get(\"output_tokens\", None),\n                response_prompt_tokens=None,\n                response_output_tokens=response.iter_count,\n                request_id=request.request_id,\n                error=error or \"Unknown error\",\n            )\n\n        response.error = error\n        status.request_start = response.start_time\n        status.request_end = response.end_time\n\n        return status, response\n</code></pre>"},{"location":"reference/guidellm/scheduler/worker/#guidellm.scheduler.worker.GenerativeRequestsWorker.description","title":"<code>description</code>  <code>property</code>","text":"<p>Get the description of the worker.</p> <p>Returns:</p> Type Description <code>GenerativeRequestsWorkerDescription</code> <p>The description of the worker.</p>"},{"location":"reference/guidellm/scheduler/worker/#guidellm.scheduler.worker.GenerativeRequestsWorker.prepare_multiprocessing","title":"<code>prepare_multiprocessing()</code>  <code>async</code>","text":"<p>Prepare the worker for multiprocessing. This is useful for workers that have instance state that can not be shared across processes and should be cleared out and re-initialized for each new process.</p> Source code in <code>src/guidellm/scheduler/worker.py</code> <pre><code>async def prepare_multiprocessing(self):\n    \"\"\"\n    Prepare the worker for multiprocessing.\n    This is useful for workers that have instance state that can not\n    be shared across processes and should be cleared out and re-initialized\n    for each new process.\n    \"\"\"\n    await self.backend.prepare_multiprocessing()\n</code></pre>"},{"location":"reference/guidellm/scheduler/worker/#guidellm.scheduler.worker.GenerativeRequestsWorker.resolve","title":"<code>resolve(request, timeout_time)</code>  <code>async</code>","text":"<p>Resolve a request by sending it to the backend and handling the response. This method sends the request to the backend, waits for a response, and handles any errors that may occur during the process.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>GenerationRequest</code> <p>The request to resolve.</p> required <code>timeout_time</code> <code>float</code> <p>The time to wait for a response before timing out. If timeout_time is math.inf, the request will not timeout.</p> required <p>Returns:</p> Type Description <code>tuple[ResolveStatus, ResponseSummary]</code> <p>A ResponseSummary object containing the response from the backend. If an error occurs, the ResponseSummary will contain the error message.</p> Source code in <code>src/guidellm/scheduler/worker.py</code> <pre><code>async def resolve(\n    self,\n    request: GenerationRequest,\n    timeout_time: float,\n) -&gt; tuple[ResolveStatus, ResponseSummary]:\n    \"\"\"\n    Resolve a request by sending it to the backend and handling the response.\n    This method sends the request to the backend, waits for a response,\n    and handles any errors that may occur during the process.\n\n    :param request: The request to resolve.\n    :param timeout_time: The time to wait for a response before timing out.\n        If timeout_time is math.inf, the request will not timeout.\n    :return: A ResponseSummary object containing the response from the backend.\n        If an error occurs, the ResponseSummary will contain the error message.\n    \"\"\"\n    resolve_start_time = time.time()\n    response = None\n    error: Optional[str] = None\n    status = ResolveStatus(\n        requested=False,\n        completed=False,\n        errored=False,\n        canceled=False,\n        request_start=-1,\n        request_end=-1,\n    )\n\n    try:\n        if timeout_time &lt; time.time():\n            raise asyncio.TimeoutError(\n                \"The timeout time has already passed.\"\n            )  # exit early\n\n        status.requested = True\n        request_func, request_kwargs = self._create_request_func_kwargs(request)\n\n        async def _runner():\n            # wrap function so we can enforce timeout and\n            # still return the latest state from the backend\n            async for resp in request_func(**request_kwargs):  # type: ignore[operator]\n                nonlocal response\n                response = resp\n\n        await asyncio.wait_for(\n            _runner(),\n            timeout=timeout_time - time.time() if timeout_time &lt; math.inf else None,\n        )\n\n        if not response:\n            raise ValueError(\n                f\"No response received for request: {request} \"\n                f\"and backend: {self.backend}\"\n            )\n        if not isinstance(response, ResponseSummary):\n            raise ValueError(\n                f\"Received no ResponseSummary for request: {request} \"\n                f\"and backend: {self.backend}, received: {response}\"\n            )\n\n        status.completed = True\n    except asyncio.TimeoutError:\n        error = \"TimeoutError: The request timed out before completing.\"\n        status.errored = True\n        status.canceled = True\n    except Exception as exc:  # noqa: BLE001\n        error = str(exc)\n        status.errored = True\n\n    return self._handle_response(\n        status=status,\n        request=request,\n        response=response,\n        error=error,\n        resolve_start_time=resolve_start_time,\n    )\n</code></pre>"},{"location":"reference/guidellm/scheduler/worker/#guidellm.scheduler.worker.RequestsWorker","title":"<code>RequestsWorker</code>","text":"<p>               Bases: <code>ABC</code>, <code>Generic[RequestT, ResponseT]</code></p> <p>An abstract base class for a worker that processes requests. This class defines the interface for a worker that can resolve requests asynchronously or synchronously within the Scheduler class. Subclasses must implement the <code>resolve</code> method, which takes a request directly given from the load generator, along with the desired start_time for the request and a timeout_time. The <code>resolve</code> method should return the response from the backend.</p> Source code in <code>src/guidellm/scheduler/worker.py</code> <pre><code>class RequestsWorker(ABC, Generic[RequestT, ResponseT]):\n    \"\"\"\n    An abstract base class for a worker that processes requests.\n    This class defines the interface for a worker that can resolve requests\n    asynchronously or synchronously within the Scheduler class.\n    Subclasses must implement the `resolve` method,\n    which takes a request directly given from the load generator,\n    along with the desired start_time for the request and a timeout_time.\n    The `resolve` method should return the response from the backend.\n    \"\"\"\n\n    @property\n    @abstractmethod\n    def description(self) -&gt; WorkerDescription:\n        \"\"\"\n        An abstract property that must be implemented by subclasses.\n        This property should return a Serializable class representing the information\n        about the worker instance.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    async def prepare_multiprocessing(self):\n        \"\"\"\n        An abstract method that must be implemented by subclasses.\n        This is useful for workers that have instance state that can not\n        be shared across processes and should be cleared out and re-initialized\n        for each new process.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    async def resolve(\n        self,\n        request: RequestT,\n        timeout_time: float,\n    ) -&gt; tuple[ResolveStatus, ResponseT]:\n        \"\"\"\n        An abstract method that must be implemented by subclasses.\n        This method should handle the resolution of a request through asyncio,\n        including any necessary backend processing and response handling.\n\n        :param request: The request to be resolved generated by the load generator.\n        :param timeout_time: The timeout time for the request, if there is no timeout\n            given, then this will be math.inf.\n        :return: The response from the worker.\n        \"\"\"\n        ...\n\n    async def get_request(\n        self, requests_queue: multiprocessing.Queue\n    ) -&gt; Optional[WorkerProcessRequest[RequestT]]:\n        return await asyncio.to_thread(requests_queue.get)  # type: ignore[attr-defined]\n\n    async def send_result(\n        self,\n        results_queue: multiprocessing.Queue,\n        result: WorkerProcessResult[RequestT, ResponseT],\n    ):\n        await asyncio.to_thread(results_queue.put, result)  # type: ignore[attr-defined]\n\n    async def resolve_scheduler_request(\n        self,\n        request: Any,\n        queued_time: float,\n        dequeued_time: float,\n        start_time: float,\n        timeout_time: float,\n        results_queue: multiprocessing.Queue,\n        process_id: int,\n    ):\n        info = SchedulerRequestInfo(\n            targeted_start_time=start_time,\n            queued_time=queued_time,\n            dequeued_time=dequeued_time,\n            scheduled_time=time.time(),\n            process_id=process_id,\n        )\n        result: WorkerProcessResult[RequestT, ResponseT] = WorkerProcessResult(\n            type_=\"request_scheduled\",\n            request=request,\n            response=None,\n            info=info,\n        )\n        asyncio.create_task(self.send_result(results_queue, result))\n\n        if (wait_time := start_time - time.time()) &gt; 0:\n            await asyncio.sleep(wait_time)\n\n        info.worker_start = time.time()\n        result = WorkerProcessResult(\n            type_=\"request_start\",\n            request=request,\n            response=None,\n            info=info,\n        )\n        asyncio.create_task(self.send_result(results_queue, result))\n\n        status, response = await self.resolve(request, timeout_time)\n        info.worker_end = time.time()\n        info.requested = status.requested\n        info.completed = status.completed\n        info.errored = status.errored\n        info.canceled = status.canceled\n        info.request_start = status.request_start\n        info.request_end = status.request_end\n        result = WorkerProcessResult(\n            type_=\"request_complete\",\n            request=request,\n            response=response,\n            info=info,\n        )\n        asyncio.create_task(self.send_result(results_queue, result))\n\n    def process_loop_synchronous(\n        self,\n        requests_queue: multiprocessing.Queue,\n        results_queue: multiprocessing.Queue,\n        process_id: int,\n    ):\n        async def _process_runner():\n            while (\n                process_request := await self.get_request(requests_queue)\n            ) is not None:\n                dequeued_time = time.time()\n\n                await self.resolve_scheduler_request(\n                    request=process_request.request,\n                    queued_time=process_request.queued_time,\n                    dequeued_time=dequeued_time,\n                    start_time=process_request.start_time,\n                    timeout_time=process_request.timeout_time,\n                    results_queue=results_queue,\n                    process_id=process_id,\n                )\n\n        try:\n            asyncio.run(_process_runner())\n        except Exception as exc:  # noqa: BLE001\n            logger.error(\n                f\"Error in worker process {process_id}: {exc}\",\n                exc_info=True,\n                stack_info=True,\n            )\n\n    def process_loop_asynchronous(\n        self,\n        requests_queue: multiprocessing.Queue,\n        results_queue: multiprocessing.Queue,\n        max_concurrency: int,\n        process_id: int,\n    ):\n        async def _process_runner():\n            pending = asyncio.Semaphore(max_concurrency)\n\n            if pending.locked():\n                raise ValueError(\"Async worker called with max_concurrency &lt; 1\")\n\n            while (\n                process_request := await self.get_request(requests_queue)\n            ) is not None:\n                dequeued_time = time.time()\n\n                await pending.acquire()\n\n                def _task_done(_: asyncio.Task):\n                    nonlocal pending\n                    pending.release()\n\n                task = asyncio.create_task(\n                    self.resolve_scheduler_request(\n                        request=process_request.request,\n                        queued_time=process_request.queued_time,\n                        dequeued_time=dequeued_time,\n                        start_time=process_request.start_time,\n                        timeout_time=process_request.timeout_time,\n                        results_queue=results_queue,\n                        process_id=process_id,\n                    )\n                )\n                task.add_done_callback(_task_done)\n                await asyncio.sleep(0)  # enable start task immediately\n\n        try:\n            asyncio.run(_process_runner())\n        except Exception as exc:  # noqa: BLE001\n            logger.error(\n                f\"Error in worker process {process_id}: {exc}\",\n                exc_info=True,\n                stack_info=True,\n            )\n</code></pre>"},{"location":"reference/guidellm/scheduler/worker/#guidellm.scheduler.worker.RequestsWorker.description","title":"<code>description</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>An abstract property that must be implemented by subclasses. This property should return a Serializable class representing the information about the worker instance.</p>"},{"location":"reference/guidellm/scheduler/worker/#guidellm.scheduler.worker.RequestsWorker.prepare_multiprocessing","title":"<code>prepare_multiprocessing()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>An abstract method that must be implemented by subclasses. This is useful for workers that have instance state that can not be shared across processes and should be cleared out and re-initialized for each new process.</p> Source code in <code>src/guidellm/scheduler/worker.py</code> <pre><code>@abstractmethod\nasync def prepare_multiprocessing(self):\n    \"\"\"\n    An abstract method that must be implemented by subclasses.\n    This is useful for workers that have instance state that can not\n    be shared across processes and should be cleared out and re-initialized\n    for each new process.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/guidellm/scheduler/worker/#guidellm.scheduler.worker.RequestsWorker.resolve","title":"<code>resolve(request, timeout_time)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>An abstract method that must be implemented by subclasses. This method should handle the resolution of a request through asyncio, including any necessary backend processing and response handling.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>RequestT</code> <p>The request to be resolved generated by the load generator.</p> required <code>timeout_time</code> <code>float</code> <p>The timeout time for the request, if there is no timeout given, then this will be math.inf.</p> required <p>Returns:</p> Type Description <code>tuple[ResolveStatus, ResponseT]</code> <p>The response from the worker.</p> Source code in <code>src/guidellm/scheduler/worker.py</code> <pre><code>@abstractmethod\nasync def resolve(\n    self,\n    request: RequestT,\n    timeout_time: float,\n) -&gt; tuple[ResolveStatus, ResponseT]:\n    \"\"\"\n    An abstract method that must be implemented by subclasses.\n    This method should handle the resolution of a request through asyncio,\n    including any necessary backend processing and response handling.\n\n    :param request: The request to be resolved generated by the load generator.\n    :param timeout_time: The timeout time for the request, if there is no timeout\n        given, then this will be math.inf.\n    :return: The response from the worker.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/guidellm/utils/","title":"guidellm.utils","text":""},{"location":"reference/guidellm/utils/#guidellm.utils.filter_text","title":"<code>filter_text(text, filter_start=None, filter_end=None)</code>","text":"<p>Filter text by start and end strings or indices</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>the text to filter</p> required <code>filter_start</code> <code>Optional[Union[str, int]]</code> <p>the start string or index to filter from</p> <code>None</code> <code>filter_end</code> <code>Optional[Union[str, int]]</code> <p>the end string or index to filter to</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>the filtered text</p> Source code in <code>src/guidellm/utils/text.py</code> <pre><code>def filter_text(\n    text: str,\n    filter_start: Optional[Union[str, int]] = None,\n    filter_end: Optional[Union[str, int]] = None,\n) -&gt; str:\n    \"\"\"\n    Filter text by start and end strings or indices\n\n    :param text: the text to filter\n    :param filter_start: the start string or index to filter from\n    :param filter_end: the end string or index to filter to\n    :return: the filtered text\n    \"\"\"\n    filter_start_index = -1\n    filter_end_index = -1\n\n    if filter_start and isinstance(filter_start, str):\n        filter_start_index = text.index(filter_start)\n    elif filter_start:\n        if not isinstance(filter_start, int):\n            raise ValueError(f\"Invalid filter start index: {filter_start}\")\n        filter_start_index = filter_start\n\n    if filter_end and isinstance(filter_end, str):\n        filter_end_index = text.index(filter_end)\n    elif filter_end:\n        if not isinstance(filter_end, int):\n            raise ValueError(f\"Invalid filter end index: {filter_end}\")\n        filter_end_index = filter_end\n\n    if filter_start_index &gt; -1:\n        text = text[filter_start_index:]\n    if filter_end_index &gt; -1:\n        text = text[:filter_end_index]\n\n    return text\n</code></pre>"},{"location":"reference/guidellm/utils/#guidellm.utils.is_puncutation","title":"<code>is_puncutation(text)</code>","text":"<p>Check if the text is a punctuation</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>the text to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the text is a punctuation, False otherwise</p> Source code in <code>src/guidellm/utils/text.py</code> <pre><code>def is_puncutation(text: str) -&gt; bool:\n    \"\"\"\n    Check if the text is a punctuation\n\n    :param text: the text to check\n    :type text: str\n    :return: True if the text is a punctuation, False otherwise\n    :rtype: bool\n    \"\"\"\n    return len(text) == 1 and not text.isalnum() and not text.isspace()\n</code></pre>"},{"location":"reference/guidellm/utils/#guidellm.utils.load_text","title":"<code>load_text(data, encoding=None)</code>","text":"<p>Load an HTML file from a path or URL</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[str, Path]</code> <p>the path or URL to load the HTML file from</p> required <code>encoding</code> <code>Optional[str]</code> <p>the encoding to use when reading the file</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>the HTML content</p> Source code in <code>src/guidellm/utils/text.py</code> <pre><code>def load_text(data: Union[str, Path], encoding: Optional[str] = None) -&gt; str:\n    \"\"\"\n    Load an HTML file from a path or URL\n\n    :param data: the path or URL to load the HTML file from\n    :type data: Union[str, Path]\n    :param encoding: the encoding to use when reading the file\n    :type encoding: str\n    :return: the HTML content\n    :rtype: str\n    \"\"\"\n    logger.debug(\"Loading text: {}\", data)\n\n    if not data:\n        return \"\"\n\n    # check URLs\n    if isinstance(data, str) and data.strip().startswith((\"http\", \"ftp\")):\n        with httpx.Client(timeout=settings.request_timeout) as client:\n            response = client.get(data.strip())\n            response.raise_for_status()\n            return response.text\n\n    # check package data\n    if isinstance(data, str) and data.startswith(\"data:\"):\n        resource_path = files(package_data).joinpath(data[5:])\n        with (\n            as_file(resource_path) as resource_file,\n            gzip.open(resource_file, \"rt\", encoding=encoding) as file,\n        ):\n            return file.read()\n\n    # check gzipped files\n    if isinstance(data, str) and data.endswith(\".gz\"):\n        with gzip.open(data, \"rt\", encoding=encoding) as file:\n            return file.read()\n\n    # check if it's raw text by not being a path\n    if isinstance(data, str) and (\n        len(data) &gt; MAX_PATH_LENGTH or not Path(data).exists()\n    ):\n        return data\n\n    # assume local file\n    if not isinstance(data, Path):\n        data = Path(data)\n\n    if not data.exists() or not data.is_file():\n        raise FileNotFoundError(f\"File not found: {data}\")\n\n    return data.read_text(encoding=encoding)\n</code></pre>"},{"location":"reference/guidellm/utils/#guidellm.utils.split_text_list_by_length","title":"<code>split_text_list_by_length(text_list, max_characters, pad_horizontal=True, pad_vertical=True)</code>","text":"<p>Split a list of strings into a list of strings, each with a maximum length of max_characters</p> <p>Parameters:</p> Name Type Description Default <code>text_list</code> <code>list[Any]</code> <p>the list of strings to split</p> required <code>max_characters</code> <code>Union[int, list[int]]</code> <p>the maximum length of each string</p> required <code>pad_horizontal</code> <code>bool</code> <p>whether to pad the strings horizontally, defaults to True</p> <code>True</code> <code>pad_vertical</code> <code>bool</code> <p>whether to pad the strings vertically, defaults to True</p> <code>True</code> <p>Returns:</p> Type Description <code>list[list[str]]</code> <p>a list of strings</p> Source code in <code>src/guidellm/utils/text.py</code> <pre><code>def split_text_list_by_length(\n    text_list: list[Any],\n    max_characters: Union[int, list[int]],\n    pad_horizontal: bool = True,\n    pad_vertical: bool = True,\n) -&gt; list[list[str]]:\n    \"\"\"\n    Split a list of strings into a list of strings,\n    each with a maximum length of max_characters\n\n    :param text_list: the list of strings to split\n    :param max_characters: the maximum length of each string\n    :param pad_horizontal: whether to pad the strings horizontally, defaults to True\n    :param pad_vertical: whether to pad the strings vertically, defaults to True\n    :return: a list of strings\n    \"\"\"\n    if not isinstance(max_characters, list):\n        max_characters = [max_characters] * len(text_list)\n\n    if len(max_characters) != len(text_list):\n        raise ValueError(\n            f\"max_characters must be a list of the same length as text_list, \"\n            f\"but got {len(max_characters)} and {len(text_list)}\"\n        )\n\n    result: list[list[str]] = []\n    for index, text in enumerate(text_list):\n        lines = textwrap.wrap(text, max_characters[index])\n        result.append(lines)\n\n    if pad_vertical:\n        max_lines = max(len(lines) for lines in result)\n        for lines in result:\n            while len(lines) &lt; max_lines:\n                lines.append(\" \")\n\n    if pad_horizontal:\n        for index in range(len(result)):\n            lines = result[index]\n            max_chars = max_characters[index]\n            new_lines = []\n            for line in lines:\n                new_lines.append(line.rjust(max_chars))\n            result[index] = new_lines\n\n    return result\n</code></pre>"},{"location":"reference/guidellm/utils/colors/","title":"guidellm.utils.colors","text":""},{"location":"reference/guidellm/utils/hf_transformers/","title":"guidellm.utils.hf_transformers","text":""},{"location":"reference/guidellm/utils/random/","title":"guidellm.utils.random","text":""},{"location":"reference/guidellm/utils/text/","title":"guidellm.utils.text","text":""},{"location":"reference/guidellm/utils/text/#guidellm.utils.text.filter_text","title":"<code>filter_text(text, filter_start=None, filter_end=None)</code>","text":"<p>Filter text by start and end strings or indices</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>the text to filter</p> required <code>filter_start</code> <code>Optional[Union[str, int]]</code> <p>the start string or index to filter from</p> <code>None</code> <code>filter_end</code> <code>Optional[Union[str, int]]</code> <p>the end string or index to filter to</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>the filtered text</p> Source code in <code>src/guidellm/utils/text.py</code> <pre><code>def filter_text(\n    text: str,\n    filter_start: Optional[Union[str, int]] = None,\n    filter_end: Optional[Union[str, int]] = None,\n) -&gt; str:\n    \"\"\"\n    Filter text by start and end strings or indices\n\n    :param text: the text to filter\n    :param filter_start: the start string or index to filter from\n    :param filter_end: the end string or index to filter to\n    :return: the filtered text\n    \"\"\"\n    filter_start_index = -1\n    filter_end_index = -1\n\n    if filter_start and isinstance(filter_start, str):\n        filter_start_index = text.index(filter_start)\n    elif filter_start:\n        if not isinstance(filter_start, int):\n            raise ValueError(f\"Invalid filter start index: {filter_start}\")\n        filter_start_index = filter_start\n\n    if filter_end and isinstance(filter_end, str):\n        filter_end_index = text.index(filter_end)\n    elif filter_end:\n        if not isinstance(filter_end, int):\n            raise ValueError(f\"Invalid filter end index: {filter_end}\")\n        filter_end_index = filter_end\n\n    if filter_start_index &gt; -1:\n        text = text[filter_start_index:]\n    if filter_end_index &gt; -1:\n        text = text[:filter_end_index]\n\n    return text\n</code></pre>"},{"location":"reference/guidellm/utils/text/#guidellm.utils.text.is_puncutation","title":"<code>is_puncutation(text)</code>","text":"<p>Check if the text is a punctuation</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>the text to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the text is a punctuation, False otherwise</p> Source code in <code>src/guidellm/utils/text.py</code> <pre><code>def is_puncutation(text: str) -&gt; bool:\n    \"\"\"\n    Check if the text is a punctuation\n\n    :param text: the text to check\n    :type text: str\n    :return: True if the text is a punctuation, False otherwise\n    :rtype: bool\n    \"\"\"\n    return len(text) == 1 and not text.isalnum() and not text.isspace()\n</code></pre>"},{"location":"reference/guidellm/utils/text/#guidellm.utils.text.load_text","title":"<code>load_text(data, encoding=None)</code>","text":"<p>Load an HTML file from a path or URL</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[str, Path]</code> <p>the path or URL to load the HTML file from</p> required <code>encoding</code> <code>Optional[str]</code> <p>the encoding to use when reading the file</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>the HTML content</p> Source code in <code>src/guidellm/utils/text.py</code> <pre><code>def load_text(data: Union[str, Path], encoding: Optional[str] = None) -&gt; str:\n    \"\"\"\n    Load an HTML file from a path or URL\n\n    :param data: the path or URL to load the HTML file from\n    :type data: Union[str, Path]\n    :param encoding: the encoding to use when reading the file\n    :type encoding: str\n    :return: the HTML content\n    :rtype: str\n    \"\"\"\n    logger.debug(\"Loading text: {}\", data)\n\n    if not data:\n        return \"\"\n\n    # check URLs\n    if isinstance(data, str) and data.strip().startswith((\"http\", \"ftp\")):\n        with httpx.Client(timeout=settings.request_timeout) as client:\n            response = client.get(data.strip())\n            response.raise_for_status()\n            return response.text\n\n    # check package data\n    if isinstance(data, str) and data.startswith(\"data:\"):\n        resource_path = files(package_data).joinpath(data[5:])\n        with (\n            as_file(resource_path) as resource_file,\n            gzip.open(resource_file, \"rt\", encoding=encoding) as file,\n        ):\n            return file.read()\n\n    # check gzipped files\n    if isinstance(data, str) and data.endswith(\".gz\"):\n        with gzip.open(data, \"rt\", encoding=encoding) as file:\n            return file.read()\n\n    # check if it's raw text by not being a path\n    if isinstance(data, str) and (\n        len(data) &gt; MAX_PATH_LENGTH or not Path(data).exists()\n    ):\n        return data\n\n    # assume local file\n    if not isinstance(data, Path):\n        data = Path(data)\n\n    if not data.exists() or not data.is_file():\n        raise FileNotFoundError(f\"File not found: {data}\")\n\n    return data.read_text(encoding=encoding)\n</code></pre>"},{"location":"reference/guidellm/utils/text/#guidellm.utils.text.split_text_list_by_length","title":"<code>split_text_list_by_length(text_list, max_characters, pad_horizontal=True, pad_vertical=True)</code>","text":"<p>Split a list of strings into a list of strings, each with a maximum length of max_characters</p> <p>Parameters:</p> Name Type Description Default <code>text_list</code> <code>list[Any]</code> <p>the list of strings to split</p> required <code>max_characters</code> <code>Union[int, list[int]]</code> <p>the maximum length of each string</p> required <code>pad_horizontal</code> <code>bool</code> <p>whether to pad the strings horizontally, defaults to True</p> <code>True</code> <code>pad_vertical</code> <code>bool</code> <p>whether to pad the strings vertically, defaults to True</p> <code>True</code> <p>Returns:</p> Type Description <code>list[list[str]]</code> <p>a list of strings</p> Source code in <code>src/guidellm/utils/text.py</code> <pre><code>def split_text_list_by_length(\n    text_list: list[Any],\n    max_characters: Union[int, list[int]],\n    pad_horizontal: bool = True,\n    pad_vertical: bool = True,\n) -&gt; list[list[str]]:\n    \"\"\"\n    Split a list of strings into a list of strings,\n    each with a maximum length of max_characters\n\n    :param text_list: the list of strings to split\n    :param max_characters: the maximum length of each string\n    :param pad_horizontal: whether to pad the strings horizontally, defaults to True\n    :param pad_vertical: whether to pad the strings vertically, defaults to True\n    :return: a list of strings\n    \"\"\"\n    if not isinstance(max_characters, list):\n        max_characters = [max_characters] * len(text_list)\n\n    if len(max_characters) != len(text_list):\n        raise ValueError(\n            f\"max_characters must be a list of the same length as text_list, \"\n            f\"but got {len(max_characters)} and {len(text_list)}\"\n        )\n\n    result: list[list[str]] = []\n    for index, text in enumerate(text_list):\n        lines = textwrap.wrap(text, max_characters[index])\n        result.append(lines)\n\n    if pad_vertical:\n        max_lines = max(len(lines) for lines in result)\n        for lines in result:\n            while len(lines) &lt; max_lines:\n                lines.append(\" \")\n\n    if pad_horizontal:\n        for index in range(len(result)):\n            lines = result[index]\n            max_chars = max_characters[index]\n            new_lines = []\n            for line in lines:\n                new_lines.append(line.rjust(max_chars))\n            result[index] = new_lines\n\n    return result\n</code></pre>"}]}